% Encoding: UTF-8

@String{C_ACCS      = {International Conference on Advanced Computing and Communication Systems}}
@String{C_ACMMM     = {ACM Conference on Multimedia Systems}}
@String{C_AFGR      = {{IEEE} International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}}
@String{C_ASSP      = {{IEEE} International Conference on Acoustics, Speech and Signal Processing}}
@String{C_AVSS      = {{IEEE} International Conference on Advanced Video and Signal Based Surveillance (AVSS)}}
@String{C_BIDS      = {International Conference on Biometrics, Identity and Security (BIDS)}}
@String{C_BIOSIG    = {International Conference of the Biometrics Special Interest Group (BIOSIG)}}
@String{C_BMVC      = {British Machine Vision Conference (BMVC)}}
@String{C_BTAS      = {{IEEE} International Conference on Biometrics: Theory Applications and Systems (BTAS)}}
@String{C_BTHI      = {Biometric Technology for Human Identification (BTHI)}}
@String{C_CAIP      = {International Conference on Computer Analysis of Images and Patterns (CAIP)}}
@String{C_COMPSAC   = {{IEEE} Annual Computer Software and Applications Conference (COMPSAC)}}
@String{C_CRV       = {International Conference on Computer and Robot Vision (CRV)}}
@String{C_CVPR      = {{IEEE/CVF} International Conference on Computer Vision and Pattern Recognition (CVPR)}}
@String{C_CVPRW     = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}}
@String{C_DSP       = {International Conference on Digital Signal Processing (DSP)}}
@String{C_ECCV      = {European Conference on Computer Vision (ECCV)}}
@String{C_EUSIPCO   = {European Signal Processing Conference (EUSIPCO)}}
@String{C_FedCSIS   = {Federated Conference on Computer Science and Information Systems (FedCSIS)}}
@String{C_IASP      = {International Conference on Image Analysis and Signal Processing (IASP)}}
@String{C_ICANN     = {International Conference on Artificial Neural Networks and Machine Learning (ICANN)}}
@String{C_ICARCV    = {International Conference on Control Automation Robotics and Vision (ICARCV)}}
@String{C_ICASSP    = {{IEEE} International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}}
@String{C_ICB       = {{IEEE} International Conference on Biometrics (ICB)}}
@String{C_ICCCA     = {International Conference on Computing, Communication and Applications (ICCA)}}
@String{C_ICCSP     = {International Conference on Communication and Signal Processing (ICCSP)}}
@String{C_ICCST     = {{IEEE} International Carnahan Conference on Security Technology (ICCST)}}
@String{C_ICCST_ACM = {{ACM} International Conference on Computer Systems and Technologies (ICCST)}}
@String{C_ICCV      = {{IEEE} International Conference on Computer Vision (ICCV)}}
@String{C_ICIAP     = {International Conference on Image Analysis and Processing (ICIAP)}}
@String{C_ICICT     = {International Conference on Inventive Computation Technologies (ICICT)}}
@String{C_ICIP      = {International {IEEE} International Conference on Image Processing}}
@String{C_ICLR      = {International Conference on Learning Representations (ICLR)}}
@String{C_ICMC      = {International Conference on Multiple Classifier Systems (ICMC)}}
@String{C_ICMCS     = {International Conference on Multimedia Computing and Systems (ICMCS)}}
@String{C_ICML      = {International Conference on Machine Learning (ICML)}}
@String{C_ICMLC     = {International Conference on Machine Learning and Cybernetics (ICMLC)}}
@String{C_ICMMAR    = {{IEEE} International Conference on Methods and Models in Automation and Robotics (MMAR)}}
@String{C_ICPAIR    = {International Conference on Pattern Analysis and Intelligent Robotics (ICPAIR)}}
@String{C_ICPR      = {International Conference on Pattern Recognition (ICPR)}}
@String{C_ICT       = {World Congress on Information and Communication Technologies}}
@String{C_IJCB      = {{IEEE} International Joint Conference on Biometrics (IJCB)}}
@String{C_IJCNN     = {{IEEE} International Joint Conference on Neural Networks (IJCNN)}}
@String{C_ISBA      = {{IEEE} International Conference on Identity, Security and Behavior Analysis (ISBA)}}
@String{C_ISCE      = {{IEEE} International Symp. on Consumer Electronics (ISCE)}}
@String{C_IWBF      = {International Conference on Biometrics and Forensics (IWBF)}}
@String{C_MIPRO     = {International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}}
@String{C_NCVPRIPG  = {National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}}
@String{C_NRSC      = {National Radio Science Conference (NRSC)}}
@String{C_PDGC      = {International Conference on Parallel, Distributed and Grid Computing (PDGC)}}
@String{C_SCE       = {{IEEE} International Symp. on Consumer Electronics}}
@String{C_SIBGRAPI  = {Conference on Graphics, Patterns and Images (SIBGRAPI)}}
@String{C_SICE      = {SICE Annual Conference}}
@String{C_SIN       = {International Conference on Security of Information and Networks}}
@String{C_SITIS     = {International Conference on Signal-Image Technology Internet-Based Systems (SITIS)}}
@String{C_SPMB      = {{IEEE} Signal Processing in Medicine and Biology Symposium (SPMB)}}
@String{C_TSP       = {International Conference on Telecommunications and Signal Processing (TSP)}}
@String{C_UIST      = {Annual ACM Symp. on User Interface Software and Technology}}
@String{C_UMT       = {International Conference on Ultra Modern Telecommunications \& Workshops}}
@String{C_VISAPP    = {{IEEE} International Conference on Computer Vision Theory and Applications (VISAPP)}}
@String{C_WACV      = {{IEEE} Winter Conference on Applications of Computer Vision (WACV)}}
@String{C_WACVW	    = {{IEEE/CVF} Winter Conference on Applications of Computer Vision Workshops (WACVW)}}
@String{J_ACC       = {{IEEE} Access}}
@String{J_ACMCS     = {ACM Comput. Surv.}}
@String{J_AES       = {{IEEE} Trans. Aerosp. Electron. Syst.}}
@String{J_BTT       = {Biometric Technology Today}}
@String{J_C         = {J. Chemometrics}}
@String{J_CA        = {International Journal of Computer Applications}}
@String{J_CBM       = {Computers in Biology and Medicine}}
@String{J_CE        = {{IEEE} Trans. Cons. Elect.}}
@String{J_CEM       = {{IEEE} Consumer Electronics Magazine}}
@String{J_CGA       = {{IEEE} Computer Graphics and Applications}}
@String{J_COMPUTER  = {{IEEE} Computer}}
@String{J_CSDA      = {Computational Statistics \& Data Analysis}}
@String{J_CVIU      = {Computer Vision and Image Understanding}}
@String{J_FGCS      = {Future Generation Computer Systems}}
@String{J_FLM       = {Journal of Forensic and Legal Medicine}}
@String{J_FS        = {Journal of Forensic Sciences}}
@String{J_FSI       = {Forensic Science International}}
@String{J_FSM       = {Journal of Forensic Science and Medicine}}
@String{J_FT        = {Forensic Toxicology}}
@String{J_IETB      = {IET Biometrics}}
@String{J_IJAC      = {International Journal of Automation and Computing}}
@String{J_IJCV      = {International Journal of Computer Vision}}
@String{J_IJLM      = {International Journal of Legal Medicine}}
@String{J_IS        = {Information Sciences}}
@String{J_IVC       = {Image and Vision Computing}}
@String{J_JFO       = {Journal of Forensic Sciences}}
@String{J_JIVP      = {EURASIP Journal on Image and Video Processing}}
@String{J_JTIT      = {Journal of Telecommunications and Information Technology}}
@String{J_LM        = {Legal Medicine}}
@String{J_LNCS      = {Lecture Notes in Computer Science}}
@String{J_ML        = {Machine Learning}}
@String{J_NYT       = {New York Time}}
@String{J_PIEEE     = {Proceedings of the {IEEE}}}
@String{J_PLOS      = {PLoS Computational Biology}}
@String{J_PM        = {Philosophical Magazine}}
@String{J_PR        = {Pattern Recognition}}
@String{J_PRL       = {Pattern Recognition Letters}}
@String{J_SEE       = {Science and Engineering Ethics}}
@String{J_SMC       = {{IEEE} International Conference on Systems, Man and Cybernetics}}
@String{J_SPM       = {{IEEE} Signal Processing Magazine}}
@String{J_SR        = {Scientific Reports}}
@String{J_TBCS      = {{IEEE} Trans. Biomed. Circuits Syst.}}
@String{J_TCSVT     = {{IEEE} Trans. on Circuits and Systems for Video Tech.}}
@String{J_TIFS      = {{IEEE} Transactions on Information Forensics and Security}}
@String{J_TIP       = {{IEEE} Transactions on Image Processing}}
@String{J_TIST      = {ACM Trans. Intell. Sys. and Tech.}}
@String{J_TPAMI     = {{IEEE} Trans. Pattern Anal. Mach. Intell.}}
@String{J_TS        = {Telecommunication Systems}}
@String{J_TSMC      = {{IEEE} Trans. Syst. Man and Cybern.}}
@String{J_VP        = {Veterinary Pathology}}
@String{J_WIRECS    = {WIREs Comp. Stat.}}
@String{J_WMIP      = {International Journal of Wavelets, Multi-resolution and Information Processing}}
@String{W_ACCV      = {ACCV International Workshops}}
@String{W_AIAT      = {{IEEE} Workshop on Automatic Identification Advanced Technologies}}
@String{W_BF        = {{IAPR/IEEE} International Workshop on Biometrics and Forensics}}
@String{W_BIOMS     = {{IEEE} Workshop on Biometric Measurements and Systems for Security and Medical Applications}}
@String{W_IFS       = {{IEEE} International Workshop on Information Forensics and Security}}
@String{W_WACV      = {{IEEE} Workshop on Applications of Computer Vision (WACV)}}

@InProceedings{Huang_WACV_2013,
  author    = {Xinyu Huang and Changpeng Ti and Qi-zhen Hou and Alade Tokuta and Ruigang Yang},
  booktitle = W_WACV,
  title     = {An experimental study of pupil constriction for liveness detection},
  year      = {2013},
  address   = {Tampa, FL, USA},
  month     = {January},
  pages     = {252-258},
  publisher = {IEEE},
  abstract  = {As iris recognition systems have been deployed in many security areas, liveness detection that can distinguish between real iris patterns and fake ones becomes an important module. Most existing algorithms focus on the appearance difference between real and fake iris (for example, printed patterns, cosmetic contact lenses etc.) which is a very difficult problem. Instead of studying image properties of fake irises, we show that pupil constriction, the fundamental characteristic of real and live irises, can be very robust for liveness detection. In this experimental study, we first build an iris acquisition system that can acquire two eye images under two different illumination conditions in a less intrusive environment. Second, in order to model the process of pupil constriction, we propose a feature descriptor that consists of similarity measurement between iris patches and ratio of iris and pupil diameters. Third, the performance of liveness prediction is evaluated based on the training of a Support Vector Machine (SVM) classifier. The high success prediction rate shows that the classifier is effective without knowing any prior knowledge of fake irises.},
  comment   = {iris:PAD:dynamic:active},
  doi       = {10.1109/WACV.2013.6475026},
  issn      = {1550-5790},
  keywords  = {iris recognition;pattern classification;support vector machines;SVM classifier;appearance difference;fake iris patterns;feature descriptor;illumination conditions;iris acquisition system;iris recognition systems;liveness detection;pupil constriction;real iris patterns;similarity measurement;support vector machine classifier;Cameras;Iris;Iris recognition;Lenses;Lighting;Mutual information;Robustness},
}

@Article{Scheirer_TPAMI_2014,
  author  = {Scheirer, Walter J. and Anthony, Samuel E. and Nakayama, Ken and Cox, David D.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Perceptual Annotation: Measuring Human Vision to Improve Computer Vision},
  year    = {2014},
  number  = {8},
  pages   = {1679-1686},
  volume  = {36},
  doi     = {10.1109/TPAMI.2013.2297711},
}

@Article{Lake_BBS_2016,
  author    = {Brenden M. Lake and Tomer D. Ullman and Joshua B. Tenenbaum and Samuel J. Gershman},
  journal   = {Behavioral and Brain Sciences},
  title     = {Building machines that learn and think like people},
  year      = {2016},
  month     = nov,
  volume    = {40},
  doi       = {10.1017/s0140525x16001837},
  publisher = {Cambridge University Press ({CUP})},
  url       = {https://doi.org/10.1017/s0140525x16001837},
}

@InProceedings{Trokielewicz_SPIE_2014,
  author       = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  booktitle    = {Photonics Applications in Astronomy, Communications, Industry, and High-Energy Physics Experiments 2014},
  title        = {{Cataract influence on iris recognition performance}},
  year         = {2014},
  editor       = {Ryszard S. Romaniuk},
  organization = {International Society for Optics and Photonics},
  pages        = {556 -- 569},
  publisher    = {SPIE},
  volume       = {9290},
  abstract     = {This paper presents the experimental study revealing weaker performance of the automatic iris recognition methods for cataract-affected eyes when compared to healthy eyes. There is little research on the topic, mostly incorporating scarce databases that are often deficient in images representing more than one illness. We built our own database, acquiring 1288 eye images of 37 patients of the Medical University of Warsaw. Those images represent several common ocular diseases, such as cataract, along with less ordinary conditions, such as iris pattern alterations derived from illness or eye trauma. Images were captured in near-infrared light (used in biometrics) and for selected cases also in visible light (used in ophthalmological diagnosis). Since cataract is a disorder that is most populated by samples in the database, in this paper we focus solely on this illness. To assess the extent of the performance deterioration we use three iris recognition methodologies (commercial and academic solutions) to calculate genuine match scores for healthy eyes and those influenced by cataract. Results show a significant degradation in iris recognition reliability manifesting by worsening the genuine scores in all three matchers used in this study (12% of genuine score increase for an academic matcher, up to 175% of genuine score increase obtained for an example commercial matcher). This increase in genuine scores affected the final false non-match rate in two matchers. To our best knowledge this is the only study of such kind that employs more than one iris matcher, and analyzes the iris image segmentation as a potential source of decreased reliability},
  keywords     = {biometrics, iris recognition, ophthalmic disease, cataract},
  url          = {https://doi.org/10.1117/12.2076040},
}


@InProceedings{Xu_ICML_2015,
  author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Bach, Francis and Blei, David},
  month     = {07--09 Jul},
  pages     = {2048--2057},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  pdf       = {http://proceedings.mlr.press/v37/xuc15.pdf},
  url       = {http://proceedings.mlr.press/v37/xuc15.html},
}

@InProceedings{Wang_CVPR_2017,
  author    = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Residual Attention Network for Image Classification},
  year      = {2017},
  pages     = {6450-6458},
  doi       = {10.1109/CVPR.2017.683},
}

@Article{Berga_ELCVIA_2020,
  author   = {David Berga},
  journal  = {{ELCVIA Electronic Letters on Computer Vision and Image Analysis}},
  title    = {Understanding Eye Movements: Psychophysics and a Model of Primary Visual Cortex},
  year     = {2020},
  issn     = {1577-5097},
  number   = {2},
  pages    = {13--15},
  volume   = {18},
  abstract = {Humans move their eyes in order to learn visual representations of the world. These eye movements depend on distinct factors, either by the scene that we perceive or by our own decisions. To select what is relevant to attend is part of our survival mechanisms and the way we build reality, as we constantly react both consciously and unconsciously to all the stimuli that is projected into our eyes. In this thesis we try to explain (1) how we move our eyes, (2) how to build machines that understand visual information and deploy eye movements, and (3) how to make these machines understand tasks in order to decide for eye movements.(1) We provided the analysis of eye movement behavior elicited by low-level feature distinctiveness with a dataset of 230 synthetically-generated image patterns. A total of 15 types of stimuli has been generated (e.g. orientation, brightness, color, size, etc.), with 7 feature contrasts for each feature category. Eye-tracking data was collected from 34 participants during the viewing of the dataset, using Free-Viewing and Visual Search task instructions. Results showed that saliency is predominantly and distinctively influenced by: 1. feature type, 2. feature contrast, 3. temporality of fixations, 4. task difficulty and 5. center bias. From such dataset (SID4VAM), we have computed a benchmark of saliency models by testing performance using psychophysical patterns. Model performance has been evaluated considering model inspiration and consistency with human psychophysics. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation.(2) Computations in the primary visual cortex (area V1 or striate cortex) have long been hypothesized to be responsible, among several visual processing mechanisms, of bottom-up visual attention (also named saliency). In order to validate this hypothesis, images from eye tracking datasets have been processed with a biologically plausible model of V1 (named Neurodynamic Saliency Wavelet Model or NSWAM). Following Li's neurodynamic model, we define V1's lateral connections with a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation and scale. Early subcortical processes (i.e. retinal and thalamic) are functionally simulated. The resulting saliency maps are generated from the model output, representing the neuronal activity of V1 projections towards brain areas involved in eye movement control. We want to pinpoint that our unified computational architecture is able to reproduce several visual processes (i.e.  brightness, chromatic induction and visual discomfort) without applying any type of training or optimization and keeping the same parametrization. The model has been extended (NSWAM-CM) with an implementation of the cortical magnification function to define the retinotopical projections towards V1, processing neuronal activity for each distinct view during scene observation. Novel computational definitions of top-down inhibition (in terms of inhibition of return and selection mechanisms), are also proposed to predict attention in Free-Viewing and Visual Search conditions. Results show that our model outpeforms other biologically-inpired models of saliency prediction as well as to predict visual saccade sequences, specifically for nature and synthetic images. We also show how temporal and spatial characteristics of inhibition of return can improve prediction of saccades, as well as how distinct search strategies (in terms of feature-selective or category-specific inhibition) predict attention at distinct image contexts.(3) Although previous scanpath models have been able to efficiently predict saccades during Free-Viewing, it is well known that stimulus and task instructions can strongly affect eye movement patterns. In particular, task priming has been shown to be crucial to the deployment of eye movements, involving interactions between brain areas related to goal-directed behavior, working and long-term memory in combination with stimulus-driven eye movement neuronal correlates. In our latest study we proposed an extension of the Selective Tuning Attentive Reference Fixation Controller Model based on task demands (STAR-FCT), describing novel computational definitions of Long-Term Memory, Visual Task Executive and Task Working Memory. With these modules we are able to use textual instructions in order to guide the model to attend to specific categories of objects and/or places in the scene. We have designed our memory model by processing a visual hierarchy of low- and high-level features. The relationship between the executive task instructions and the memory representations has been specified using a tree of semantic similarities between the learned features and the object category labels. Results reveal that by using this model, the resulting object localization maps and predicted saccades have a higher probability to fall inside the salient regions depending on the distinct task instructions compared to saliency.},
  keywords = {saliency; eye movements; attention; visual cortex; horizontal connections; visual search; free-viewing; psychophysics; firing rate; neural networks},
  url      = {https://elcvia.cvc.uab.es/article/view/v18-n2-Berga},
}

@InProceedings{Bansal_CHI_2021,
  author    = {Gagan Bansal and Tongshuang (Sherry) Wu and Joyce Zhou and Raymond Fok and Besmira Nushi and Ece Kamar and Marco T{\'u}lio Ribeiro and Daniel S. Weld},
  booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI'21)},
  title     = {{Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance}},
  year      = {2021},
}

@Article{Jacobs_Nature_2021,
  author    = {Maia Jacobs and Melanie F. Pradier and Thomas H. McCoy and Roy H. Perlis and Finale Doshi-Velez and Krzysztof Z. Gajos},
  journal   = {Translational Psychiatry},
  title     = {How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection},
  year      = {2021},
  month     = feb,
  number    = {1},
  volume    = {11},
  doi       = {10.1038/s41398-021-01224-x},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1038/s41398-021-01224-x},
}

@InProceedings{Trokielewicz_BTAS_2015,
  author    = {Trokielewicz, Mateusz and Czajka, Adam and Maciejewicz, Piotr},
  booktitle = {2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
  title     = {Assessment of iris recognition reliability for eyes affected by ocular pathologies},
  year      = {2015},
  pages     = {1-6},
  doi       = {10.1109/BTAS.2015.7358747},
}

@InProceedings{Kathikeyan_ICCCA_2012,
  author    = {T. Kathikeyan and B. Sabarigiri},
  booktitle = C_ICCCA,
  title     = {{Countermeasures against iris spoofing and liveness detection using electroencephalogram (EEG)}},
  year      = {2012},
  month     = {Feb},
  pages     = {1-5},
  abstract  = {The proposed multi-modality human identification system, which fuses the IRIS and Electroencephalogram (EEG). These approaches are the most popular human identification system when they are considered separately. They can be affected by various direct attacks and spoofing. Hence we combine them to obtain a highly secured human identification system. This integrated system using IRIS and Electroencephalogram biometric with Liveness detection is the first of its kind in the international scenario. Experimental results show that the highest identification performance is obtained and neuro image coding is proposed. These ideas and directions helps to develop new tools, techniques, and methodologies in biometrics, lie detection, crime detection, to detect the record of specific terrorist act or any incident stored in the brain and to intensify brain related methodologies, brain imaging studies and healthcare solutions in future.},
  doi       = {10.1109/ICCCA.2012.6179228},
  issn      = {2325-6001},
  keywords  = {electroencephalography;health care;image coding;image fusion;medical image processing;neurophysiology;object detection;IRIS biometric;IRIS spoofing;brain imaging;brain related methodology;crime detection;electroencephalogram biometric;health care solution;lie detection;liveness detection;multimodality human identification system;neuro image coding;Biological neural networks;Biomedical imaging;Brain modeling;Electroencephalography;Humans;Image color analysis;Iris recognition;Direct attacks;Electroencephalogram (EEG);IRIS Patterns;Liveness Detection;Neuro image coding;Spoofing},
}

@InProceedings{Rathgeb_ICPR_2010,
  author    = {Christian Rathgeb and Andreas Uhl},
  booktitle = C_ICPR,
  title     = {Attacking Iris Recognition: An Efficient Hill-Climbing Technique},
  year      = {2010},
  address   = {Istanbul, Turkey},
  month     = {August},
  pages     = {1217-1220},
  publisher = {IEEE},
  abstract  = {In this paper we propose a modified hill-climbing attack to iris biometric systems. Applying our technique we are able to effectively gain access to iris biometric systems at very low effort. Furthermore, we demonstrate that reconstructing approximations of original iris images is highly non-trivial.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICPR.2010.303},
  issn      = {1051-4651},
  keywords  = {iris recognition;search problems;hill-climbing attack;hill-climbing technique;iris biometric systems;iris recognition;Approximation algorithms;Approximation methods;Feature extraction;Image reconstruction;Iris;Iris recognition;Pixel;Biometrics;Hill-Climbing Attack;Iris Recognition},
}

@Article{Bowyer_COMPUTER_2014,
  author    = {Kevin W. Bowyer and James S. Doyle},
  journal   = J_COMPUTER,
  title     = {Cosmetic Contact Lenses and Iris Recognition Spoofing},
  year      = {2014},
  issn      = {0018-9162},
  month     = {May},
  number    = {5},
  pages     = {96-98},
  volume    = {47},
  abstract  = {Automatically detecting novel types of cosmetic contact lenses in iris images is a very difficult pattern-recognition problem, but experimental datasets that enable researchers to study this problem have recently become available.},
  comment   = {iris:PAD},
  doi       = {10.1109/MC.2014.118},
  keywords  = {contact lenses;iris recognition;object detection;automatic cosmetic contact lense detection;iris images;iris recognition spoofing;pattern-recognition problem;Image color analysis;Iris recognition;Lenses;Lighting;Pigments;Sensors;bioinformatics;biometrics;cosmetic contact lenses;identity sciences;iris recognition;near-infrared illumination;pattern recognition;security;spoofing},
  publisher = {IEEE},
}


@Misc{IriCoreIlluminationWavelength,
  author = {{U-JIN LED}},
  title  = {{ULI-81036A-30IRP IR Lamp Specification}},
  year   = {accessed: July 3, 2020},
  url    = {http://ujin.ezion.co.kr/down/ULI-81036A-30IRP.pdf},
}

@InProceedings{Trokielewicz_BTAS_2018,
  author    = {M. {Trokielewicz} and A. {Czajka} and P. {Maciejewicz}},
  booktitle = {2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
  title     = {Presentation Attack Detection for Cadaver Iris},
  year      = {2018},
  address   = {Redondo Beach, CA, USA},
  month     = {Oct},
  pages     = {1-10},
  publisher = {IEEE},
  abstract  = {This paper presents a deep-learning-based method for iris presentation attack detection (PAD) when iris images are obtained from deceased people. Post-mortem iris recognition, despite being a potentially useful method that could aid forensic identification, can also pose challenges when used inappropriately, i.e. utilizing a dead organ of a person in an unauthorized way. Our approach is based on the VGG-16 architecture fine-tuned with a database of 574 post-mortem, near-infrared iris images from the WarsawBioBase-PostMortem-Iris-v1 database, complemented by a dataset of 256 images of live irises, collected within the scope of this study. Experiments described in this paper show that our approach is able to correctly classify iris images as either representing a live or a dead eye in almost 99% of the trials, averaged over 20 subject-disjoint, train/test splits. We also show that the post-mortem iris detection accuracy increases as time since death elapses, and that we are able to construct a classification system with APCER=0%@BPCER≈1% (Attack Presentation and Bona Fide Presentation Classification Error Rates, respectively) when only post-mortem samples collected at least 16 hours post-mortem are considered. Since acquisitions of ante- and post-mortem samples differ significantly, we applied countermeasures to minimize bias in our classification methodology caused by image properties that are not related to the PAD. This included using the same iris sensor in collection of ante- and post-mortem samples, and analysis of class activation maps to ensure that discriminant iris regions utilized by our classifier are related to properties of the eye, and not to those of the acquisition protocol. This paper offers the first known to us PAD method in a postmortem setting, together with an explanation of the decisions made by the convolutional neural network. Along with the paper we offer source codes, weights of the trained network, and a dataset of live iris images to facilitate reproducibility and further research.},
  doi       = {10.1109/BTAS.2018.8698542},
  issn      = {2474-9680},
  url       = {https://arxiv.org/abs/1807.04058},
}

@InProceedings{Trokielewicz_BTAS_2019,
  author    = {M. Trokielewicz and A. Czajka and P. Maciejewicz},
  booktitle = {2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS), September 23-26, 2019, Tampa, FL, USA},
  title     = {{Perception of Image Features in Post-Mortem Iris Recognition: Humans vs Machines}},
  year      = {2019},
  address   = {Tampa, FL, USA},
  pages     = {1--9},
  publisher = {IEEE},
  url       = {https://arxiv.org/abs/1807.04049},
}

@Article{Prieto2015,
  author    = {Prieto-Bonete, Gemma and Perez-Carceles, Maria D. and Luna, Aurelio},
  journal   = {Legal Medicine},
  title     = {Morphological and histological changes in eye lens: Possible application for estimating postmortem interval},
  year      = {2017},
  issn      = {1344-6223},
  number    = {6},
  pages     = {437-442},
  volume    = {17},
  doi       = {10.1016/j.legalmed.2015.09.002},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.legalmed.2015.09.002},
}

@InCollection{Czajka_Handbook_2016,
  author    = {Czajka, Adam},
  booktitle = {Handbook of Iris Recognition},
  publisher = {Springer London},
  title     = {Iris Liveness Detection by Modeling Dynamic Pupil Features},
  year      = {2016},
  address   = {London},
  editor    = {Bowyer, Kevin W. and Burge, Mark J.},
  isbn      = {978-1-4471-6784-6},
  pages     = {439--467},
  abstract  = {The objective of this chapter is to present how to employ pupil dynamics in eye liveness detection. A thorough review of current liveness detection methods is provided at the beginning of the chapter to make the scientific background and position this method within current state-of-the-art methodology. Pupil dynamics may serve as a component of a wider presentation attack detection in iris recognition systems, making them more secure. Due to a lack of public databases that would support this research, we have built our own iris capture device to register pupil size changes under visible light stimuli, and registered 204 observations for 26 subjects (52 different irides), each containing 750 iris images taken every 40 ms. Each measurement registers the spontaneous pupil oscillations and its reaction after a sudden increase and a sudden decrease of the intensity of visible light. The Kohn and Clynes pupil dynamics model is used to describe these changes; hence, we convert each observation into a point in a feature space defined by model parameters. To answer the question whether the eye is alive (that is, if it reacts to light changes as a human eye) or the presentation is suspicious (that is, if it reacts oddly or no reaction is observed), we use linear and nonlinear support vector machines to classify natural reaction and spontaneous oscillations, simultaneously investigating the goodness of fit to reject bad modeling. Our experiments show that this approach can achieve a perfect performance for the data we have collected; all normal reactions are correctly differentiated from spontaneous oscillations. We investigated three variants of modeling to find the simplest, yet still powerful configuration of the method, namely (1) observing the pupil reaction to both the positive and negative changes in the light intensity, (2) using only the pupil reaction to positive surge of the light intensity, and (3) employing only the pupil reaction when the light is suddenly turned off. Further investigation related to the shortest observation time required to model the pupil reaction led to the final conclusion that time periods not exceeding 3 s are adequate to offer a perfect performance (on this dataset).},
  comment   = {iris:PAD:dynamic:active},
  doi       = {10.1007/978-1-4471-6784-6_19},
  url       = {http://dx.doi.org/10.1007/978-1-4471-6784-6_19},
}

@Article{Freytsis_Frontiers_2021,
  author   = {Freytsis, Maria and Barclay, Iain and Radha, Swapna Krishnakumar and Czajka, Adam and Siwo, Geoffery H. and Taylor, Ian and Bucher, Sherri},
  journal  = {Frontiers in Blockchain},
  title    = {Development of a Mobile, Self-Sovereign Identity Approach for Facility Birth Registration in Kenya},
  year     = {2021},
  issn     = {2624-7852},
  pages    = {2},
  volume   = {4},
  abstract = {Birth registration is a critical element of newborn care. Increasing the coverage of birth registration is an essential part of the strategy to improve newborn survival globally, and is central to achieving greater health, social, and economic equity as defined under the United Nations Sustainable Development Goals. Parts of Eastern and Southern Africa have some of the lowest birth registration rates in the world. Mobile technologies have been used successfully with mothers and health workers in Africa to increase coverage of essential newborn care, including birth registration. However, mounting concerns about data ownership and data protection in the digital age are driving the search for scalable, user-centered, privacy protecting identity solutions. There is increasing interest in understanding if a self-sovereign identity (SSI) approach can help lower the barriers to birth registration by empowering families with a smartphone based process while providing high levels of data privacy and security in populations where birth registration rates are low. The process of birth registration and the barriers experienced by stakeholders are highly contextual. There is currently a gap in the literature with regard to modeling birth registration using SSI technology. This paper describes the development of a smartphone-based prototype system that allows interaction between families and health workers to carry out the initial steps of birth registration and linkage of mothers-baby pairs in an urban Kenyan setting using verifiable credentials, decentralized identifiers, and the emerging standards for their implementation in identity systems. The goal of the project was to develop a high fidelity prototype that could be used to obtain end-user feedback related to the feasibility and acceptability of an SSI approach in a particular Kenyan healthcare context. This paper will focus on how this technology was adapted for the specific context and implications for future research.},
  doi      = {10.3389/fbloc.2021.631341},
  url      = {https://www.frontiersin.org/article/10.3389/fbloc.2021.631341},
}

@Article{Czajka_BTT_2021,
  author   = {Adam Czajka},
  journal  = {Biometric Technology Today},
  title    = {Is that eye dead or alive? Detecting new iris biometrics attacks},
  year     = {2021},
  issn     = {0969-4765},
  number   = {5},
  pages    = {9-12},
  volume   = {2021},
  abstract = {Iris recognition has been revolutionising biometric-based human identification for almost three decades, due to the unprecedented information richness of iris patterns translating into low false matches – which has now been observed across a population exceeding 1 billion users worldwide. We are also able to effectively detect those presentation attack types known to our algorithms during the design phase. But the question remains: are we prepared to detect those unknown specimens, strikingly similar to images of authentic irises, that can be generated by modern artificial intelligence tools or – more gruesomely – captured from decedents?},
  doi      = {https://doi.org/10.1016/S0969-4765(21)00060-6},
  url      = {https://www.sciencedirect.com/science/article/pii/S0969476521000606},
}

@Article{Czajka_TIFS_2015,
  author    = {Adam Czajka},
  journal   = J_TIFS,
  title     = {Pupil Dynamics for Iris Liveness Detection},
  year      = {2015},
  issn      = {1556-6013},
  month     = {April},
  number    = {4},
  pages     = {726-735},
  volume    = {10},
  abstract  = {The primary objective of this paper is to propose a complete methodology for eye liveness detection based on pupil dynamics. This method may serve as a component of presentation attack detection in iris recognition systems, making them more secure. Due to a lack of public databases that would support this paper, we have built our own iris capture device to register pupil size changes under visible light stimuli, and registered 204 observations for 26 subjects (52 different irides), each containing 750 iris images taken every 40 ms. Each measurement registers the spontaneous pupil oscillations and its reaction after a sudden increase of the intensity of visible light. The Kohn and Clynes pupil dynamics model is used to describe these changes; hence we convert each observation into a feature space defined by model parameters. To answer the question whether the eye is alive (that is, if it reacts to light changes as a human eye) or the presentation is suspicious (that is, if it reacts oddly or no reaction is observed), we use linear and nonlinear support vector machines to classify natural reaction and spontaneous oscillations, simultaneously investigating the goodness of fit to reject bad modeling. Our experiments show that this approach can achieve a perfect performance for the data we have collected. All normal reactions are correctly differentiated from spontaneous oscillations. We investigated the shortest observation time required to model the pupil reaction, and found that time periods not exceeding 3 s are adequate to offer a perfect performance.},
  comment   = {iris:PAD:dynamic:active},
  doi       = {10.1109/TIFS.2015.2398815},
  keywords  = {computer crime;feature extraction;image classification;iris recognition;support vector machines;eye liveness detection;feature space;iris capture device;iris images;iris liveness detection;iris recognition systems;model parameters;natural reaction classification;nonlinear support vector machines;presentation attack detection;pupil dynamics;pupil oscillations;pupil size changes;spontaneous oscillations;visible light intensity;visible light stimuli;Cameras;Databases;Iris recognition;Lenses;Motion pictures;Oscillators;Liveness detection;biometrics;iris recognition;presentation attack detection;pupil dynamics},
  publisher = {IEEE},
}

@InProceedings{Czajka_ICMMAR_2013,
  author    = {Adam Czajka},
  booktitle = C_ICMMAR,
  title     = {Database of iris printouts and its application: Development of liveness detection method for iris recognition},
  year      = {2013},
  address   = {Mi\c{e}dzyzdroje, Poland},
  month     = {August},
  pages     = {28-33},
  publisher = {IEEE},
  abstract  = {Liveness detection (often referred to as presentation attack detection) is the ability to detect artificial objects presented to a biometric device with an intention to subvert the recognition system. This paper presents the database of iris printout images with a controlled quality, and its fundamental application, namely development of liveness detection method for iris recognition. The database gathers images of only those printouts that were accepted by an example commercial camera, i.e. the iris template calculated for an artefact was matched to the corresponding iris reference of the living eye. This means that the quality of the employed imitations is not accidental and precisely controlled. The database consists of 729 printout images for 243 different eyes, and 1274 images of the authentic eyes, corresponding to imitations. It may thus serve as a good benchmark for at least two challenges: a) assessment of the liveness detection algorithms, and b) assessment of the eagerness of matching real and fake samples by iris recognition methods. To our best knowledge, the iris printout database of such properties is the first worldwide published as of today. In its second part, the paper presents an example application of this database, i.e. the development of liveness detection method based on iris image frequency analysis. We discuss how to select frequency windows and regions of interest to make the method sensitive to “alien frequencies” resulting from the printing process. The proposed method shows a very promising results, since it may be configured to achieve no false alarms when the rate of accepting the iris printouts is approximately 5% (i.e. 95% of presentation attack trials are correctly identified). This favorable compares to the results of commercial equipment used in the database development, as this device accepted all the printouts used. The method employs the same image as used in iris recognition process, hence no investments into the capture devices is required, and may be applied also to other carriers for printed iris patterns, e.g. contact lens.},
  comment   = {iris:PAD:static:pasive},
  doi       = {10.1109/MMAR.2013.6669876},
  keywords  = {image matching;iris recognition;object detection;artificial object detection;authentic eyes;biometric device;database development;eagerness assessment;fake sample matching;frequency windows;image matching;iris image frequency analysis;iris printout database;iris printout images;iris recognition;liveness detection method;presentation attack detection;presentation attack trials;printing process;real sample matching;Cameras;Databases;Image resolution;Image segmentation;Iris;Iris recognition;Printers},
}

@InProceedings{Doyle_BTAS_2013,
  author    = {James S. Doyle and Kevin W. Bowyer and Patrick J. Flynn},
  booktitle = C_BTAS,
  title     = {Variation in accuracy of textured contact lens detection based on sensor and lens pattern},
  year      = {2013},
  address   = {Arlington, VA, USA},
  month     = {September},
  pages     = {1-7},
  publisher = {IEEE},
  abstract  = {Automatic detection of textured contact lenses in images acquired for iris recognition has been studied by several researchers. However, to date, the experimental results in this area have all been based on the same manufacturer of contact lenses being represented in both the training data and the test data and only one previous work has considered images from more than one iris sensor. Experimental results in this work show that accuracy of textured lens detection can drop dramatically when tested on a manufacturer of lenses not seen in the training data, or when the iris sensor in use varies between the training and test data. These results suggest that the development of a fully general approach to textured lens detection is a problem that still requires attention.},
  doi       = {10.1109/BTAS.2013.6712745},
  keywords  = {image sensors;image texture;iris recognition;lenses;automatic detection;iris sensor;lens pattern;sensor pattern;textured contact lens detection;textured lens detection;Accuracy;Feature extraction;Iris;Iris recognition;Lenses;Training},
}

@Article{Galbally_TIP_2014,
  author    = {Javier Galbally and Sebastien Marcel and Julian Fierrez},
  journal   = J_TIP,
  title     = {Image Quality Assessment for Fake Biometric Detection: Application to Iris, Fingerprint, and Face Recognition},
  year      = {2014},
  issn      = {1057-7149},
  month     = {February},
  number    = {2},
  pages     = {710-724},
  volume    = {23},
  abstract  = {To ensure the actual presence of a real legitimate trait in contrast to a fake self-manufactured synthetic or reconstructed sample is a significant problem in biometric authentication, which requires the development of new and efficient protection measures. In this paper, we present a novel software-based fake detection method that can be used in multiple biometric systems to detect different types of fraudulent access attempts. The objective of the proposed system is to enhance the security of biometric recognition frameworks, by adding liveness assessment in a fast, user-friendly, and non-intrusive manner, through the use of image quality assessment. The proposed approach presents a very low degree of complexity, which makes it suitable for real-time applications, using 25 general image quality features extracted from one image (i.e., the same acquired for authentication purposes) to distinguish between legitimate and impostor samples. The experimental results, obtained on publicly available data sets of fingerprint, iris, and 2D face, show that the proposed method is highly competitive compared with other state-of-the-art approaches and that the analysis of the general image quality of real biometric samples reveals highly valuable information that may be very efficiently used to discriminate them from fake traits.},
  comment   = {face:PAD, fingerprint:PAD, iris:PAD},
  doi       = {10.1109/TIP.2013.2292332},
  keywords  = {face recognition;feature extraction;fingerprint identification;iris recognition;biometric authentication;biometric recognition framework;face recognition;fake biometric detection;feature extraction;fingerprint recognition;fraudulent access attempts;image quality assessment;iris recognition;liveness assessment;protection measures;software based fake detection method;Biomedical imaging;Face;Feature extraction;Image quality;Iris recognition;Security;Image quality assessment;attacks;biometrics;countermeasures;security;0},
  publisher = {IEEE},
}

@InProceedings{Galbally_ICB_2012,
  author    = {Javier Galbally and Jaime Ortiz-Lopez and Julian Fierrez and Javier Ortega-Garcia},
  booktitle = {2012 5th IAPR International Conference on Biometrics (ICB)},
  title     = {Iris liveness detection based on quality related features},
  year      = {2012},
  address   = {New Delhi, India},
  month     = {March},
  pages     = {271-276},
  publisher = {IEEE},
  abstract  = {A new liveness detection scheme for iris based on quality related measures is presented. The novel anti-spoofing technique is tested on a database comprising over 1,600 real and fake (high quality printed images) iris samples proving to have a very high potential as an effective protection scheme against direct attacks. Furthermore, the liveness detection method presented has the added advantage over previously studied techniques of needing just one iris image (the same used for verification) to decide whether it comes from a real or fake eye.},
  comment   = {iris:PAD:static:passive},
  doi       = {10.1109/ICB.2012.6199819},
  issn      = {2376-4201},
  keywords  = {iris recognition;visual databases;antispoofing technique;direct attacks;fake iris samples;high quality printed images;iris database;iris liveness detection;protection scheme;quality related features;real iris samples;Biomedical imaging;Databases;Feature extraction;Frequency conversion;Frequency measurement;Iris;Iris recognition},
}

@Article{Gragnaniello_TIFS_2015,
  author    = {Diego Gragnaniello and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  journal   = J_TIFS,
  title     = {An Investigation of Local Descriptors for Biometric Spoofing Detection},
  year      = {2015},
  issn      = {1556-6013},
  month     = apr,
  number    = {4},
  pages     = {849-863},
  volume    = {10},
  abstract  = {Biometric authentication systems are quite vulnerable to sophisticated spoofing attacks. To keep a good level of security, reliable spoofing detection tools are necessary, preferably implemented as software modules. The research in this field is very active, with local descriptors, based on the analysis of microtextural features, gaining more and more popularity, because of their excellent performance and flexibility. This paper aims at assessing the potential of these descriptors for the liveness detection task in authentication systems based on various biometric traits: fingerprint, iris, and face. Besides compact descriptors based on the independent quantization of features, already considered for some liveness detection tasks, we will study promising descriptors based on the joint quantization of rich local features. The experimental analysis, conducted on publicly available data sets and in fully reproducible modality, confirms the potential of these tools for biometric applications, and points out possible lines of development toward further improvements.},
  comment   = {iris:PAD, face:PAD, fingerprint:PAD},
  doi       = {10.1109/TIFS.2015.2404294},
  publisher = {IEEE},
}

@InProceedings{Gupta_ICPR_2014,
  author    = {Priyanshu Gupta and Shipra Behera and Mayank Vatsa and Richa Singh},
  booktitle = C_ICPR,
  title     = {On Iris Spoofing Using Print Attack},
  year      = {2014},
  address   = {Stockholm, Sweden},
  month     = {August},
  pages     = {1681-1686},
  publisher = {IEEE},
  abstract  = {Human iris contains rich textural information which serves as the key information for biometric identifications. It is very unique and one of the most accurate biometric modalities. However, spoofing techniques can be used to obfuscate or impersonate identities and increase the risk of false acceptance or false rejection. This paper revisits iris recognition with spoofing attacks and analyzes their effect on the recognition performance. Specifically, print attack with contact lens variations is used as the spoofing mechanism. It is observed that print attack and contact lens, individually and in conjunction, can significantly change the inter-personal and intra-personal distributions and thereby increase the possibility to deceive the iris recognition systems. The paper also presents the IIITD iris spoofing database, which contains over 4800 iris images pertaining to over 100 individuals with variations due to contact lens, sensor, and print attack. Finally, the paper also shows that cost effective descriptor approaches may help in counter-measuring spooking attacks.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICPR.2014.296},
  issn      = {1051-4651},
}

@InProceedings{Kanematsu_SICE_2007,
  author    = {Masashi Kanematsu and Hironobu Takano and Kiyomi Nakamura},
  booktitle = C_SICE,
  title     = {Highly reliable liveness detection method for iris recognition},
  year      = {2007},
  address   = {Takamatsu, Japan},
  month     = {September},
  pages     = {361-364},
  publisher = {IEEE},
  abstract  = {The importance of personal authentication is increasing with the development of the information society. The accuracy of personal authentication by identifying the iris is higher than that by using other biometric traits such as faces or fingerprints. However, the iris authentication system is vulnerable to deception by a fake iris even though the recognition accuracy is high. In this study, we developed a liveness detection method by using a variation in the brightness of an iris pattern induced by a pupillary reflex. The live and artificial irises were classified by a decision threshold of 7% brightness variation rate.},
  comment   = {iris:PAD},
  doi       = {10.1109/SICE.2007.4421008},
  keywords  = {biometrics (access control);image recognition;message authentication;decision threshold;image classification;iris recognition;liveness detection method;personal authentication;Authentication;Bioinformatics;Biometrics;Brightness;Face detection;Fingerprint recognition;Iris recognition;Optical reflection;Timing;Waveguide discontinuities;Brightness variation of iris;Iris recognition;Liveness detection},
}

@InCollection{Lee_ICB_2005,
  author    = {Lee, Eui Chul and Park, Kang Ryoung and Kim, Jaihie},
  booktitle = J_LNCS,
  publisher = {Springer Berlin Heidelberg},
  title     = {Fake Iris Detection by Using Purkinje Image},
  year      = {2005},
  address   = {Berlin, Heidelberg},
  editor    = {Zhang, David and Jain, Anil K.},
  isbn      = {978-3-540-31621-3},
  pages     = {397--403},
  series    = {International Conference on Biometrics: Advances in Biometrics},
  abstract  = {Fake iris detection is to detect and defeat a fake (forgery) iris image input. To solve the problems of previous researches on fake iris detection, we propose the new method of detecting fake iris attack based on the Purkinje image. Especially, we calculated the theoretical positions and distances between the Purkinje images based on the human eye model and the performance of fake detection algorithm could be much enhanced by such information. Experimental results showed that the FAR (False Acceptance Rate for accepting fake iris as live one) was 0.33% and FRR(False Rejection Rate of rejecting live iris as fake one) was 0.33%.},
  comment   = {irid:PASD:static:passive},
  doi       = {10.1007/11608288_53},
  url       = {http://dx.doi.org/10.1007/11608288_53},
}

@InCollection{Lovish_CAIP_2015,
  author    = {Lovish and Aditya Nigam and Balender Kumar and Phalguni Gupta},
  booktitle = C_CAIP,
  publisher = {Springer},
  title     = {Robust Contact Lens Detection Using Local Phase Quantization and Binary Gabor Pattern},
  year      = {2015},
  address   = {Valletta, Malta},
  editor    = {G. Azzopardi and N. Petkov},
  isbn      = {978-3-319-23192-1},
  pages     = {702-714},
  abstract  = {Due to its resistance to circumvention, iris has been used as a prime biometric trait in border crossings and identity related civil projects. However, sensor level spoofing attacks such as the use of printed iris, plastic eyeballs and contact lens pose a challenge by helping intruders to sidestep security in iris based biometric systems. Attacks through contact lenses are most challenging to detect as they obfuscate the iris partially and part of original iris remains visible through them. In this paper, we present a contact lens dataset containing 12823 images acquired from 50 subjects. Each subject has images pertaining to no lens, soft lens and cosmetic lens class. Verification results with three different techniques on three datasets suggest an average degradation of 3.10% in EER when subject is wearing soft lens and 17.34% when subject is wearing cosmetic lens. Further we propose a cosmetic lens detection approach based on Local Phase Quantization(LPQ) and Binary Gabor Pattern(BGP). Experiments conducted on publicly available IIITD Vista, IIITD Cogent, ND_2010 and self-collected dataset indicate that our method outperforms previous lens detection techniques in terms of Correct Classification Rate and false Acceptance Rate. The results suggest that a comprehensive texture descriptor having blur tolerance of LPQ and robustness of BGP is suitable for cosmetic lens detection.},
  comment   = {iris:PAD},
}

@Article{Menotti_TIFS_2015,
  author    = {David Menotti and Giovani Chiachia and Allan Pinto and William Robson Schwartz and Helio Pedrini and Alexandre Xavier Falcao and Anderson Rocha},
  journal   = J_TIFS,
  title     = {Deep Representations for Iris, Face, and Fingerprint Spoofing Detection},
  year      = {2015},
  issn      = {1556-6013},
  month     = {April},
  number    = {4},
  pages     = {864-879},
  volume    = {10},
  abstract  = {Biometrics systems have significantly improved person identification and authentication, playing an important role in personal, national, and global security. However, these systems might be deceived (or spoofed) and, despite the recent advances in spoofing detection, current solutions often rely on domain knowledge, specific biometric reading systems, and attack types. We assume a very limited knowledge about biometric spoofing at the sensor to derive outstanding spoofing detection systems for iris, face, and fingerprint modalities based on two deep learning approaches. The first approach consists of learning suitable convolutional network architectures for each domain, whereas the second approach focuses on learning the weights of the network via back propagation. We consider nine biometric spoofing benchmarks - each one containing real and fake samples of a given biometric modality and attack type - and learn deep representations for each benchmark by combining and contrasting the two learning approaches. This strategy not only provides better comprehension of how these approaches interplay, but also creates systems that exceed the best known results in eight out of the nine benchmarks. The results strongly indicate that spoofing detection systems based on convolutional networks can be robust to attacks already known and possibly adapted, with little effort, to image-based attacks that are yet to come.},
  comment   = {iris:PAD, face:PAD, fingerprints:PAD},
  doi       = {10.1109/TIFS.2015.2398817},
  publisher = {IEEE},
}

@InProceedings{Pacut_ICCST_2006,
  author    = {Andrzej Pacut and Adam Czajka},
  booktitle = C_ICCST,
  title     = {Aliveness Detection for Iris Biometrics},
  year      = {2006},
  address   = {Lexington, KY, USA},
  month     = {October},
  pages     = {122-129},
  publisher = {IEEE},
  abstract  = {Various experiments show an alarming lack of anti-spoofing mechanisms in devices already protecting many sensitive areas all over the world, proving that aliveness detection methods must be quickly included in commercial equipment. To introduce and systemize the topic, the paper begins with a survey of possible types of eye forgery, together with possible countermeasures. The authors introduce three solutions of eye aliveness detection, based on analyses of image frequency spectrum, controlled light reflection from the cornea, and pupil dynamics. A body of various fake (printed) eye images was used to test the developed methodologies, including different printers and printout carriers. The proposed methodology was embedded into the NASK iris recognition system and showed its large potential. For a local database of pairs of alive and printed eyes, all methods proposed in the paper revealed zero false acceptance rate of fakes FAR-F. The false rejection rate of genuines FRR-G reached 2.8% for the first proposed solution, and showed null value for the next two proposed methods. This very favorable compares to the commercial equipment tested: two popular iris cameras accepted 73% and 15% of the prepared fake irises.},
  comment   = {iris:PAD},
  doi       = {10.1109/CCST.2006.313440},
  issn      = {1071-6572},
  keywords  = {biometrics (access control);eye;image recognition;aliveness detection;anti-spoofing mechanisms;biometrics;eye forgery;false acceptance rate of fakes;false rejection rate of genuines;image frequency spectrum;iris recognition;light reflection;pupil dynamics;Biometrics;Cornea;Forgery;Frequency;Image analysis;Iris;Lighting control;Optical reflection;Protection;Testing;aliveness detection;biometrics;iris recognition},
}

@Article{Raghavendra_TIFS_2015,
  author    = {Ramachandra Raghavendra and Christoph Busch},
  journal   = J_TIFS,
  title     = {Robust Scheme for Iris Presentation Attack Detection Using Multiscale Binarized Statistical Image Features},
  year      = {2015},
  issn      = {1556-6013},
  month     = {April},
  number    = {4},
  pages     = {703-715},
  volume    = {10},
  abstract  = {Vulnerability of iris recognition systems remains a challenge due to diverse presentation attacks that fail to assure the reliability when adopting these systems in real-life scenarios. In this paper, we present an in-depth analysis of presentation attacks on iris recognition systems especially focusing on the photo print attacks and the electronic display (or screen) attack. To this extent, we introduce a new relatively large scale visible spectrum iris artefact database comprised of 3300 iris normal and artefact samples that are captured by simulating five different attacks on iris recognition system. We also propose a novel presentation attack detection (PAD) scheme based on multiscale binarized statistical image features and linear support vector machines. Extensive experiments are carried out on four different publicly available iris artefact databases that have revealed the outstanding performance of the proposed PAD scheme when benchmarked with various well-established state-of-the-art schemes.},
  comment   = {iris:PAD},
  doi       = {10.1109/TIFS.2015.2400393},
  publisher = {IEEE},
}

@Article{Raja_TIFS_2015,
  author    = {Kiran B. Raja and Ramachandra Raghavendra and Christoph Busch},
  journal   = J_TIFS,
  title     = {Video Presentation Attack Detection in Visible Spectrum Iris Recognition Using Magnified Phase Information},
  year      = {2015},
  issn      = {1556-6013},
  month     = {October},
  number    = {10},
  pages     = {2048-2056},
  volume    = {10},
  abstract  = {The gaining popularity of the visible spectrum iris recognition has sparked the interest in adopting it for various access control applications. Along with the popularity of visible spectrum iris recognition comes the threat of identity spoofing, presentation, or direct attack. This paper presents a novel scheme for detecting video presentation attacks in visible spectrum iris recognition system by magnifying the phase information in the eye region of the subject. The proposed scheme employs modified Eulerian video magnification (EVM) to enhance the subtle phase information in eye region and novel decision module to classify it as artefact(spoof attack) or normal presentation. The proposed decision module is based on estimating the change of phase information obtained from EVM, specially tailored to detect presentation attacks on video-based iris recognition systems in visible spectrum. The proposed scheme is extensively evaluated on the newly constructed database consisting of 62 unique iris video acquired using two smartphones-iPhone 5S and Nokia Lumia 1020. We also construct the artefact database with 62 iris acquired by replaying normal presentation iris video on iPad with retina display. Extensive evaluation of proposed presentation attack detection (PAD) scheme on the newly constructed database has shown an outstanding performance of average classification error rate = 0\% supporting the robustness of the proposed PAD scheme.},
  comment   = {iris:PAD},
  doi       = {10.1109/TIFS.2015.2440188},
  publisher = {IEEE},
}

@Article{Baker_CVIU_2010,
  author    = {Sarah E.Baker and Amanda Hentz and Kevin W.Bowyer and Patrick J.Flynn},
  journal   = J_CVIU,
  title     = {Degradation of Iris Recognition Performance Due to Non-Cosmetic Prescription Contact Lenses},
  year      = {2010},
  month     = {September},
  number    = {9},
  pages     = {1030-1044},
  volume    = {114},
  abstract  = {Many iris recognition systems operate under the assumption that non-cosmetic contact lenses have no or minimal effect on iris biometrics performance and convenience. In this paper we show results of a study of 12,003 images from 87 contact-lens-wearing subjects and 9697 images from 124 non-contact-lens-wearing subjects. We visually classified the contact lens images into four categories according to the type of lens effects observed in the image. Our results show different degradations in performance for different types of contact lenses. Lenses that produce larger artifacts on the iris yield more degraded performance. This is the first study to document degraded iris biometrics performance with non-cosmetic contact lenses.},
  comment   = {iris:PAD},
  doi       = {https://doi.org/10.1016/j.cviu.2010.06.002},
  publisher = {Elsevier},
}

@InCollection{Ruiz-Albacete_LNCS_2008,
  author    = {Virginia Ruiz-Albacete and Pedro Tome-Gonzalez and Fernando Alonso-Fernandez and Javier Galbally and Julian Fierrez and Javier Ortega-Garcia},
  booktitle = J_LNCS,
  publisher = {Springer},
  title     = {Direct Attacks Using Fake Images in Iris Verification},
  year      = {2008},
  address   = {Roskilde, Denmark},
  pages     = {181-190},
  series    = {Lecture Notes in Computer Science},
  volume    = {5372},
  abstract  = {In this contribution, the vulnerabilities of iris-based recognition systems to direct attacks are studied. A database of fake iris images has been created from real iris of the BioSec baseline database. Iris images are printed using a commercial printer and then, presented at the iris sensor. We use for our experiments a publicly available iris recognition system, which some modifications to improve the iris segmentation step. Based on results achieved on different operational scenarios, we show that the system is vulnerable to direct attacks, pointing out the importance of having countermeasures against this type of fraudulent actions.},
  comment   = {iris:PAD},
}


@InProceedings{Yambay_IJCB_2014,
  author    = {David Yambay and James S. Doyle and Kevin W. Bowyer and Adam Czajka and Stephanie Schuckers},
  booktitle = C_IJCB,
  title     = {LivDet-iris 2013 - Iris Liveness Detection Competition 2013},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {The use of an artificial replica of a biometric characteristic in an attempt to circumvent a system is an example of a biometric presentation attack. Liveness detection is one of the proposed countermeasures, and has been widely implemented in fingerprint and iris recognition systems in recent years to reduce the consequences of spoof attacks. The goal for the Liveness Detection (LivDet) competitions is to compare software-based iris liveness detection methodologies using a standardized testing protocol and large quantities of spoof and live images. Three submissions were received for the competition Part 1; Biometric Recognition Group de Universidad Autonoma de Madrid, University of Naples Federico II, and Faculdade de Engenharia de Universidade do Porto. The best results from across all three datasets was from Federico with a rate of falsely rejected live samples of 28.6% and the rate of falsely accepted fake samples of 5.7%.},
  comment   = {iris:PAD},
  doi       = {10.1109/BTAS.2014.6996283},
  keywords  = {iris recognition;Biometric Recognition Group de Universidad Autonoma de Madrid;Faculdade de Engenharia de Universidade do Porto;Iris Liveness Detection Competition 2013;LivDet-iris 2013;University of Naples Federico II;biometric characteristic;biometric presentation attack;fingerprint recognition system;iris recognition system;software-based iris liveness detection methodologies;spoof attacks;standardized testing protocol;Cameras;Educational institutions;Iris;Iris recognition;Lenses;Testing;Training},
}

@InProceedings{Sequeira_IJCNN_2014,
  author    = {Ana F. Sequeira and Juliano Murari and Jaime S. Cardoso},
  booktitle = C_IJCNN,
  title     = {Iris liveness detection methods in the mobile biometrics scenario},
  year      = {2014},
  address   = {Beijing, China},
  month     = {July},
  pages     = {3002-3008},
  publisher = {IEEE},
  abstract  = {Biometrie systems based on iris are vulnerable to direct attacks consisting on the presentation of a fake iris to the sensor (a printed or a contact lenses iris image, among others). The mobile biometrics scenario stresses the importance of assessing the security issues. The application of countermeasures against this type of attacking scheme is the problem addressed in the present paper. Widening a previous work, several state-of-the-art iris liveness detection methods were implemented and adapted to a less-constrained scenario. The proposed method combines a feature selection step prior to the use of state-of-the-art classifiers to perform the classification based upon the "best features". Five well known existing databases for iris liveness purposes (Biosec, Clarkson, NotreDame and Warsaw) and a recently published database, MobBIOfake, with real and fake images captured in the mobile scenario were tested. The results obtained suggest that the automated segmentation step does not degrade significantly the results.},
  comment   = {iris:PAD},
  doi       = {10.1109/IJCNN.2014.6889816},
  issn      = {2161-4393},
  keywords  = {feature extraction;image classification;image segmentation;iris recognition;mobile computing;object detection;security of data;Biosec database;Clarkson database;MobBIOfake;NotreDame database;Warsaw database;attacking scheme;automated segmentation step;feature selection step;iris liveness detection methods;mobile biometrics scenario;security issues;Databases;Feature extraction;Image segmentation;Iris;Iris recognition;Lenses},
}

@InProceedings{Sequeira_VISAPPa_2014,
  author    = {Ana F. Sequeira and Juliano Murari and Jaime S. Cardoso},
  booktitle = C_VISAPP,
  title     = {Iris liveness detection methods in mobile applications},
  year      = {2014},
  address   = {Lisbon, Portugal},
  month     = {January},
  pages     = {22-33},
  publisher = {IEEE},
  volume    = {3},
  abstract  = {Biometric systems are vulnerable to different kinds of attacks. Particularly, the systems based on iris are vulnerable to direct attacks consisting on the presentation of a fake iris to the sensor trying to access the system as it was from a legitimate user. The analysis of some countermeasures against this type of attacking scheme is the problem addressed in the present paper. Several state-of-the-art methods were implemented and included in a feature selection framework so as to determine the best cardinality and the best subset that conducts to the highest classification rate. Three different classifiers were used: Discriminant analysis, K nearest neighbours and Support Vector Machines. The implemented methods were tested in existing databases for iris liveness purposes (Biosec and Clarkson) and in a new fake database which was constructed for evaluation of iris liveness detection methods in the mobile scenario. The results suggest that this new database is more challenging than the others. Therefore, improvements are required in this line of research to achieve good performance in real world mobile applications.},
  comment   = {iris:PAD},
  keywords  = {Databases;Feature extraction;Iris;Iris recognition;Lenses;Biometrics;Fake Database;Handheld Device;Iris;Liveness Detection},
}

@InProceedings{Sequeira_IJCB_2014,
  author    = {Ana F. Sequeira and H\'{e}lder P. Oliveira and Jo{\~{a}}o C. Monteiro and Jo{\~{a}}o P. Monteiro and Jaime S. Cardoso},
  booktitle = C_IJCB,
  title     = {{MobILive} 2014 - Mobile Iris Liveness Detection Competition},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {September},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Biometric systems based on iris are vulnerable to several attacks, particularly direct attacks consisting on the presentation of a fake iris to the sensor. The development of iris liveness detection techniques is crucial for the deployment of iris biometric applications in daily life specially in the mobile biometric field. The 1st Mobile Iris Liveness Detection Competition (MobILive) was organized in the context of IJCB2014 in order to record recent advances in iris liveness detection. The goal for (MobILive) was to contribute to the state of the art of this particular subject. This competition covered the most common and simple spoofing attack in which printed images from an authorized user are presented to the sensor by a non-authorized user in order to obtain access. The benchmark dataset was the MobBIOfake database which is composed by a set of 800 iris images and its corresponding fake copies (obtained from printed images of the original ones captured with the same handheld device and in similar conditions). In this paper we present a brief description of the methods and the results achieved by the six participants in the competition.},
  comment   = {iris:PAD},
  doi       = {10.1109/BTAS.2014.6996290},
}

@InProceedings{Wei_CPR_2008,
  author    = {Zhuoshi Wei and Xianchao Qiu and Zhenan Sun and Tieniu Tan},
  booktitle = C_ICPR,
  title     = {Counterfeit iris detection based on texture analysis},
  year      = {2008},
  address   = {Tampa, FL, USA},
  month     = {Dec},
  pages     = {1-4},
  publisher = {IEEE},
  abstract  = {This paper addresses the issue of counterfeit iris detection, which is a liveness detection problem in biometrics. Fake iris mentioned here refers to iris wearing color contact lens with textures printed onto them. We propose three measures to detect fake iris: measuring iris edge sharpness, applying Iris-Texton feature for characterizing the visual primitives of iris textures and using selected features based on co-occurrence matrix (CM). Extensive testing is carried out on two datasets containing different types of contact lens with totally 640 fake iris images, which demonstrates that Iris-Texton and CM features are effective and robust in anticounterfeit iris. Detailed comparisons with two state-of-the-art methods are also presented, showing that the proposed iris edge sharpness measure acquires a comparable performance with these two methods, while Iris-Texton and CM features outperform the state-of-the-art.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICPR.2008.4761673},
  issn      = {1051-4651},
  keywords  = {biometrics (access control);edge detection;feature extraction;image texture;matrix algebra;visual databases;biometrics;color contact lens;cooccurrence matrix;counterfeit iris detection;iris edge sharpness;iris-texton feature;liveness detection problem;state- of-the-art methods;texture analysis;visual primitives;Biometrics;Control systems;Counterfeiting;Gabor filters;Histograms;Image edge detection;Image segmentation;Iris recognition;Lenses;Robustness},
}

@Article{Yadav_TIFS_2014,
  author    = {Daksha Yadav and Naman Kohli and James S. Doyle and Richa Singh and Mayank Vatsa and Kevin W. Bowyer},
  journal   = J_TIFS,
  title     = {Unraveling the Effect of Textured Contact Lenses on Iris Recognition},
  year      = {2014},
  issn      = {1556-6013},
  month     = {May},
  number    = {5},
  pages     = {851-862},
  volume    = {9},
  abstract  = {The presence of a contact lens, particularly a textured cosmetic lens, poses a challenge to iris recognition as it obfuscates the natural iris patterns. The main contribution of this paper is to present an in-depth analysis of the effect of contact lenses on iris recognition. Two databases, namely, the IIIT-D Iris Contact Lens database and the ND-Contact Lens database, are prepared to analyze the variations caused due to contact lenses. We also present a novel lens detection algorithm that can be used to reduce the effect of contact lenses. The proposed approach outperforms other lens detection algorithms on the two databases and shows improved iris recognition performance.},
  comment   = {iris:PAD},
  doi       = {10.1109/TIFS.2014.2313025},
  keywords  = {image texture;iris recognition;visual databases;IIIT-D iris contact lens database;ND contact lens database;iris recognition;textured contact lenses;textured cosmetic lens;Databases;Image color analysis;Iris;Iris recognition;Lenses;Probes;Training;Iris recognition;contact lens;lens detection},
  publisher = {IEEE},
}

@InProceedings{Silva_SIBGRAPI_2015,
  author       = {Pedro Silva and Eduardo Luz and Rafael Baeta and Helio Pedrini and Alexandre Xavier Falcao and David Menotti},
  booktitle    = C_SIBGRAPI,
  title        = {An Approach to Iris Contact Lens Detection based on Deep Image Representations},
  year         = {2015},
  address      = {Salvador, Brazil},
  month        = {August},
  organization = {IEEE},
  pages        = {157--164},
  publisher    = {IEEE},
  abstract     = {Spoofing detection is a challenging task in biometric systems, when differentiating illegitimate users from genuine ones. Although iris scans are far more inclusive than fingerprints, and also more precise for person authentication, iris recognition systems are vulnerable to spoofing via textured cosmetic contact lenses. Iris spoofing detection is also referred to as liveness detection (binary classification of fake and real images). In this work, we focus on a three-class detection problem: images with textured (colored) contact lenses, soft contact lenses, and no lenses. Our approach uses a convolutional network to build a deep image representation and an additional fully-connected single layer with soft max regression for classification. Experiments are conducted in comparison with a state-of-the-art approach (SOTA) on two public iris image databases for contact lens detection: 2013 Notre Dame and IIIT-Delhi. Our approach can achieve a 30% performance gain over SOTA on the former database (from 80% to 86%) and comparable results on the latter. Since IIIT-Delhi does not provide segmented iris images and, differently from SOTA, our approach does not segment the iris yet, we conclude that these are very promising results.},
  doi          = {10.1109/SIBGRAPI.2015.16},
  issn         = {1530-1834},
  keywords     = {authorisation;image representation;image segmentation;image texture;iris recognition;regression analysis;IIIT-Delhi;Notre Dame;SOTA;biometric systems;deep image representations;fully-connected single layer;illegitimate users;iris contact lens detection;iris recognition systems;iris scans;iris spoofing detection;liveness detection;person authentication;segmented iris images;soft contact lenses;softmax regression;state-of-the-art approach;textured cosmetic contact lenses;Databases;Image representation;Iris;Iris recognition;Lenses;Network topology;Training;Contact Lens Detection;Convolutional Networks;Deep Learning;Iris Biometrics},
}

@Misc{ISO_30107-3_2017,
  author  = {{ISO/IEC FDIS 30107-3:2017}},
  title   = {{Information technology -- Biometric presentation attack detection -- Part 3: Testing and reporting}},
  year    = {2017},
  comment = {standards:PAD:ISO},
}

@Misc{IEC_60825_1,
  author = {{IEC 60825-1}},
  title  = {{Safety of Laser Products. Part 1: Equipment Classification, Requirements and User's Guide}},
  year   = {1993 (amended in 1997 and 2001)},
}

@InProceedings{Bolme_BTAS_2016,
  author    = {David S. Bolme and Ryan A. Tokola and Chris B. Boehnen and Tiffany B. Saul and Kelly A. Sauerwein and Dawnie Wolfe Steadman},
  booktitle = C_BTAS,
  title     = {Impact of environmental factors on biometric matching during human decomposition},
  year      = {2016},
  address   = {Niagara Falls, NY, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {Automatic recognition systems are valuable tools for identifying unknown deceased individuals. Immediately after death, fingerprint and face biometric samples are easy to collect using standard sensors and can be easily matched to antemortem biometric samples. Even though early postmortem fingerprints and facial images have been used for identification purposes for decades, there are no studies that track these biometrics through the later stages of decomposition to determine the length of time they remain viable. This paper discusses a multimodal dataset of finger-prints, faces, and irises from twelve donated human subjects that decomposed outdoors under natural conditions. Results include predictive models relating time and temperature, measured as Accumulated Degree Days (ADD), and season (winter, spring, summer), to the probability of automatic verification using a commercial algorithm.},
  comment   = {iris:post-mortem, face:post-mortem, fingerprints:post-mortem},
  doi       = {10.1109/BTAS.2016.7791177},
  keywords  = {environmental factors;face recognition;fingerprint identification;image matching;iris recognition;accumulated degree days;antemortem biometric samples;automatic recognition systems;biometric matching;environmental factors;face biometric samples;facial images;fingerprint samples;human decomposition;postmortem fingerprints;Biological tissues;Biomedical imaging;Face;Insects;Iris recognition;Sensors;Springs},
}

@InProceedings{Saripalle_BTAS_2015,
  author    = {S. K. Saripalle and A. McLaughlin and R. Krishna and A. Ross and R. Derakhshani},
  booktitle = C_BTAS,
  title     = {{Post-mortem Iris Biometric Analysis in Sus scrofa domesticus}},
  year      = {2015},
  address   = {Arlington, VA, USA},
  month     = {September},
  pages     = {1-5},
  publisher = {IEEE},
  abstract  = {Although biometric utility of ante-mortem human iris tissue has been long established, post-mortem study of human iris tissue for its biometric utility has only been speculated. Given obstacles in measuring and analyzing biometric capability of post-mortem human iris tissue, an investigation into the feasibility of using post-mortem Sus scrofa domesticus iris tissue as a biometric is undertaken. The contributions of our work are two-fold: first, our method discusses a feasible alternative to human iris for study of post-mortem iris biometric analysis. Second, we report the performance of iris biometrics over a period of time after death. Previous studies have only reported qualitative changes in iris after death while for the first time we measure the biometric capacity of post-mortem iris tissue.},
  comment   = {iris:post-mortem},
  doi       = {10.1109/BTAS.2015.7358789},
  keywords  = {eye;iris recognition;ante-mortem human iris tissue;biometric capability;biometric utility;post-mortem Sus scrofa domesticus iris tissue;post-mortem human iris tissue;post-mortem iris biometric analysis;Biomedical imaging;Image recognition;Image segmentation},
}

@MastersThesis{Sansola_MastersThesis_2015,
  author  = {Alora Sansola},
  school  = {Boston University},
  title   = {Postmortem iris recognition and its application in human identification},
  year    = {2015},
  address = {Boston, MA, USA},
}

@Misc{ISO_19794_6_2011,
  author  = {{ISO/IEC 19794-6:2011}},
  title   = {{Information technology -- Biometric data interchange formats -- Part 6: Iris image data}},
  year    = {2011},
  comment = {standards:ISO},
}

@Misc{ISO_30107-1_2016,
  author  = {{ISO/IEC 30107-1:2016}},
  title   = {{Information technology -- Biometric presentation attack detection -- Part 1: Framework}},
  year    = {2016},
  comment = {standards:PAD:ISO},
}

@Article{DeMarsico_IVC_2014,
  author    = {Maria De Marsico and Chiara Galdi and Michele Nappi and Daniel Riccio},
  journal   = J_IVC,
  title     = {FIRME: Face and Iris Recognition for Mobile Engagement},
  year      = {2014},
  issn      = {0262-8856},
  number    = {12},
  pages     = {1161 - 1172},
  volume    = {32},
  abstract  = {Abstract Mobile devices, namely phones and tablets, have long gone “smart”. Their growing use is both a cause and an effect of their technological advancement. Among the others, their increasing ability to store and exchange sensitive information, has caused interest in exploiting their vulnerabilities, and the opposite need to protect users and their data through secure protocols for access and identification on mobile platforms. Face and iris recognition are especially attractive, since they are sufficiently reliable, and just require the webcam normally equipping the involved devices. On the contrary, the alternative use of fingerprints requires a dedicated sensor. Moreover, some kinds of biometrics lend themselves to uses that go beyond security. Ambient intelligence services bound to the recognition of a user, as well as social applications, such as automatic photo tagging on social networks, can especially exploit face recognition. This paper describes \{FIRME\} (Face and Iris Recognition for Mobile Engagement) as a biometric application based on a multimodal recognition of face and iris, which is designed to be embedded in mobile devices. Both design and implementation of \{FIRME\} rely on a modular architecture, whose workflow includes separate and replaceable packages. The starting one handles image acquisition. From this point, different branches perform detection, segmentation, feature extraction, and matching for face and iris separately. As for face, an antispoofing step is also performed after segmentation. Finally, results from the two branches are fused. In order to address also security-critical applications, \{FIRME\} can perform continuous reidentification and best sample selection. To further address the possible limited resources of mobile devices, all algorithms are optimized to be low-demanding and computation-light.},
  comment   = {iris:PAD, face:PAD},
  doi       = {https://doi.org/10.1016/j.imavis.2013.12.014},
  keywords  = {Android, Face authentication, Iris authentication, Identity management, Mobile computing, Open source, Pervasive computing, Security, Spoofing detection},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S0262885614000055},
}

@Article{Park_OptEng_2007,
  author    = {Jong Hyun Park and Moon Gi Kang},
  journal   = {Optical Engineering},
  title     = {Multispectral iris authentication system against counterfeit attack using gradient-based image fusion},
  year      = {2007},
  number    = {11},
  pages     = {117003-117003-14},
  volume    = {46},
  abstract  = {A new iris recognition scheme using multispectral iris images aimed for preventing the counterfeit attack is proposed. In the proposed system, multispectral infrared iris images are taken in order to utilize the spectral features of real iris. Rather than additionally deciding whether the enrolled iris is fake or not, the multispectral images are fused into a grayscale image to contain the complementary information among them by a gradient-based image fusion algorithm, and the iris region of the fused image is applied directly to the recognition procedure. Through the fusion process, the images which do not show multispectral variations result in a scrambled image that does not contain the exact features of the original iris. Because of the failure in the fusion process, the fused image of a fake iris does not match the original iris features in the database. Thus, they are simply rejected in the recognition step. Experimental results show that the proposed scheme successfully localizes the iris position of real irises and prevents possible counterfeit attacks while maintaining the performance of the authentication system.},
  comment   = {iris:PAD},
  doi       = {10.1117/1.2802367},
  isbn      = {0091-3286},
  publisher = {SPIE},
  url       = {http://dx.doi.org/10.1117/1.2802367},
}

@Article{Komogortsev_TIFS_2015,
  author    = {Oleg V. Komogortsev and Alexey Karpov and Corey D. Holland},
  journal   = J_TIFS,
  title     = {Attack of Mechanical Replicas: Liveness Detection With Eye Movements},
  year      = {2015},
  issn      = {1556-6013},
  month     = {April},
  number    = {4},
  pages     = {716-725},
  volume    = {10},
  abstract  = {This paper investigates liveness detection techniques in the area of eye movement biometrics. We investigate a specific scenario, in which an impostor constructs an artificial replica of the human eye. Two attack scenarios are considered: 1) the impostor does not have access to the biometric templates representing authentic users, and instead utilizes average anatomical values from the relevant literature and 2) the impostor gains access to the complete biometric database, and is able to employ exact anatomical values for each individual. In this paper, liveness detection is performed at the feature and match score levels for several existing forms of eye movement biometric, based on different aspects of the human visual system. The ability of each technique to differentiate between live and artificial recordings is measured by its corresponding false spoof acceptance rate, false live rejection rate, and classification rate. The results suggest that eye movement biometrics are highly resistant to circumvention by artificial recordings when liveness detection is performed at the feature level. Unfortunately, not all techniques provide feature vectors that are suitable for liveness detection at the feature level. At the match score level, the accuracy of liveness detection depends highly on the biometric techniques employed.},
  comment   = {iris:PAD:dynamic:active},
  doi       = {10.1109/TIFS.2015.2405345},
  keywords  = {eye;gaze tracking;image classification;image matching;iris recognition;artificial recording;artificial replica;biometric database;classification rate;eye movement biometrics;false live rejection rate;false spoof acceptance rate;feature vectors;human eye;human visual system;impostor;live recording;liveness detection;match score level;mechanical replicas;Accuracy;Biological system modeling;Feature extraction;Iris recognition;Mathematical model;Vectors;Biometrics;attack vectors;eye movements;liveness detection;pattern analysis;security and protection;spoofs},
  publisher = {IEEE},
}

@Misc{Czajka_patent_2011,
  author   = {Adam Czajka and Andrzej Pacut and Marcin Chochowski},
  title    = {Method of eye aliveness testing and device for eye aliveness testing, {U}nited {S}tates {P}atent, {US} 8,061,842},
  year     = {2011},
  abstract = {In compliance with the method, the measurement of the characteristic dimensions of the hypothetical pupil are taken on the basis of a sequence of images. The eye is stimulated with the light featuring a pre-defined intensity profile. For each image in this sequence, the characteristic dimensions of the hypothetical pupil are calculated by means of image processing methods. For a sequence of images, the system determines the function ƒ which defines the changes in the characteristic dimensions of the hypothetical pupil within the measurement period, and on the basis of the said changes as well as on the selected mathematical model, the aliveness parameters O of the eye are determined by means of estimation methods. The calculated aliveness parameters are compared with the statistical template by way of classification process.},
  comment  = {iris:PAD},
}

@InProceedings{Galbally_IWBF_2016,
  author    = {Javier Galbally and Marta Gomez-Barrero},
  booktitle = W_BF,
  title     = {A review of iris anti-spoofing},
  year      = {2016},
  address   = {Limassol, Cyprus},
  month     = {March},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {To ensure the actual presence of a real legitimate trait in contrast to a fake self-manufactured physical synthetic sample, is a significant problem in biometric authentication, which requires the development of new and efficient protection measures. The whole biometric community, including researchers, developers, standardizing bodies and vendors, has thrown itself into the very challenging task of proposing and developing efficient protection methods against this threat, known as spoofing. The goal of this paper is to provide a comprehensive and structured overview on the work that has been carried out over the last decade in the field of iris anti-spoofing. In brief, the paper has been thought as a tool to provide biometric researchers an overall picture of the current panorama in the mentioned area following a systematic approach.},
  doi       = {10.1109/IWBF.2016.7449676},
  keywords  = {iris recognition;biometric authentication;iris antispoofing;legitimate trait;protection measures;protection methods;Face;Feature extraction;Iris recognition;Lenses;Lighting;Three-dimensional displays;Iris recognition;anti-spoofing;vulnerabilities},
}

@Article{Sun_PAMI_2014,
  author    = {Zhenan Sun and Hui Zhang and Tieniu Tan and Jianyu Wang},
  journal   = J_TPAMI,
  title     = {Iris Image Classification Based on Hierarchical Visual Codebook},
  year      = {2014},
  issn      = {0162-8828},
  month     = {June},
  number    = {6},
  pages     = {1120-1133},
  volume    = {36},
  abstract  = {Iris recognition as a reliable method for personal identification has been well-studied with the objective to assign the class label of each iris image to a unique subject. In contrast, iris image classification aims to classify an iris image to an application specific category, e.g., iris liveness detection (classification of genuine and fake iris images), race classification (e.g., classification of iris images of Asian and non-Asian subjects), coarse-to-fine iris identification (classification of all iris images in the central database into multiple categories). This paper proposes a general framework for iris image classification based on texture analysis. A novel texture pattern representation method called Hierarchical Visual Codebook (HVC) is proposed to encode the texture primitives of iris images. The proposed HVC method is an integration of two existing Bag-of-Words models, namely Vocabulary Tree (VT), and Locality-constrained Linear Coding (LLC). The HVC adopts a coarse-to-fine visual coding strategy and takes advantages of both VT and LLC for accurate and sparse representation of iris texture. Extensive experimental results demonstrate that the proposed iris image classification method achieves state-of-the-art performance for iris liveness detection, race classification, and coarse-to-fine iris identification. A comprehensive fake iris image database simulating four types of iris spoof attacks is developed as the benchmark for research of iris liveness detection.},
  doi       = {10.1109/TPAMI.2013.234},
  keywords  = {image classification;image coding;image representation;image texture;iris recognition;linear codes;object detection;trees (mathematics);HVC method;LLC;VT;application specific category;bag-of-words models;coarse-to-fine iris identification;coarse-to-fine visual coding strategy;hierarchical visual codebook;image texture analysis;iris image classification;iris liveness detection;iris recognition;locality-constrained linear coding;personal identification;race classification;sparse iris texture representation;texture pattern representation;vocabulary tree;Biomedical imaging;Encoding;Feature extraction;Iris;Iris recognition;Visualization;Vocabulary;Coarse-to-fine iris identification;Ethnic iris classification;Hierarchical Visual Codebook (HVC);Iris image classification;Iris liveness detection;coarse-to-fine iris identification;iris liveness detection;race classification;0},
  publisher = {IEEE},
}

@InProceedings{Doyle_ICB_2013,
  author    = {James S. Doyle and Patrick J. Flynn and Kevin W. Bowyer},
  booktitle = {2013 International Conference on Biometrics (ICB)},
  title     = {Automated classification of contact lens type in iris images},
  year      = {2013},
  address   = {Madrid, Spain},
  month     = {June},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Textured cosmetic lenses have long been known to present a problem for iris recognition. It was once believed that clear, soft contact lenses did not impact iris recognition accuracy. However, it has recently been shown that persons wearing clear, soft contact lenses experience an increased false non-match rate relative to persons not wearing contact lenses. Iris recognition systems need the ability to automatically determine if a person is (a) wearing no contact lens, (b) wearing a clear prescription lens, or (c), wearing a textured cosmetic lens. This work presents results of the first attempt that we are aware of to solve this three-class classification problem. Results show that it is possible to identify with high accuracy (96.5%) the images in which a textured cosmetic contact lens is present, but that correctly distinguishing between no lenses and soft lenses is a challenging problem.},
  doi       = {10.1109/ICB.2013.6612954},
  issn      = {2376-4201},
  keywords  = {contact lenses;image classification;image texture;iris recognition;automated classification;iris images;iris recognition systems;prescription lens;soft contact lenses;textured cosmetic contact lens;Accuracy;Feature extraction;Iris;Iris recognition;Lenses;Training},
}

@InProceedings{Raghavendra_WACV_2017,
  author    = {Ramachandra Raghavendra and Kiran B. Raja and Christoph Busch},
  booktitle = C_WACV,
  title     = {ContlensNet: Robust Iris Contact Lens Detection Using Deep Convolutional Neural Networks},
  year      = {2017},
  address   = {Santa Rosa, CA, USA},
  month     = {March},
  pages     = {1160-1167},
  publisher = {IEEE},
  abstract  = {Contact lens detection in the eye is a significant task to improve the reliability of iris recognition systems. A contact lens overlays the iris region and prevents the iris sensor from capturing the normal iris region. In this paper, we present a novel scheme for detection to detecting a contact lens using Deep Convolutional Neural Network (CNN). The proposed CNN architecture ContlensNet is structured to have fifteen layers and configured for the three-class detection problem with the following classes: images with textured (or colored) contact lens, soft (or transparent) contact lens, and no contact lens. The proposed ContlensNet is trained using numerous iris image patches and the problem of overfitting the network is addressed by using the dropout regularization method. Extensive experiments are carried out on two publicly available large-scale databases, namely: IIIT-Delhi Contact lens iris database (IIITD) and Notre Dame cosmetic contact lens database 2013 (ND) that are comprised of contact lens iris samples captured using four different sensors. The obtained results have demonstrated the improved performance of the proposed scheme with an average performance improvement of more than 10% in Correct Classification Rate (CCR%) when compared with eight different state-of-the-art contact lens detection systems.},
  comment   = {iris:PAD},
  doi       = {10.1109/WACV.2017.134},
  keywords  = {Convolution;Databases;Image segmentation;Iris;Iris recognition;Lenses;Neural networks},
}

@InProceedings{He_BTAS_2016,
  author    = {Lingxiao He and Haiqing Li and Fei Liu and Nianfeng Liu and Zhenan Sun and Zhaofeng He},
  booktitle = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
  title     = {Multi-patch convolution neural network for iris liveness detection},
  year      = {2016},
  address   = {Niagara Falls, NY, USA},
  month     = {September},
  pages     = {1-7},
  publisher = {IEEE},
  abstract  = {Attacking iris systems with fake iris patterns has become the largest security risk of iris recognition systems. Therefore iris liveness detection which discriminate genuine or fake iris images is of significant importance to iris recognition systems. However, the state-of-the-art algorithms mainly rely on hand-crafted texture features which can only identify fake iris images with single pattern. This paper proposes a Multi-patch Convolution Neural Network (MCNN) that is capable of handling different types of fake iris images. MCNN directly learns the mapping function between raw pixels of the input iris patch and the labels. The outputs of each patch are fed into a decision layer which determines the final decision. Our proposed algorithm automatically learns the features to detect hybrid pattern of fake iris images rather than handcraft. The decision layer helps to improve the robustness and accuracy for iris liveness detection. Experimental results demonstrate an extremely higher accuracy of iris liveness detection than other state-of-the-art algorithms. The proposed MCNN remarkably achieve the best results with nearly 100% accuracy on ND-Contact and CAISA-Iris-Fake datasets.},
  comment   = {iris:PAD},
  doi       = {10.1109/BTAS.2016.7791186},
  keywords  = {iris recognition;neural nets;decision layer;fake iris images;iris liveness detection;iris recognition systems;mapping function;multipatch convolution neural network;Algorithm design and analysis;Convolution;Feature extraction;Iris;Iris recognition;Lenses;Neural networks},
}

@Article{Kuehlkamp_TIFS_2019,
  author  = {Kuehlkamp, Andrey and Pinto, Allan and Rocha, Anderson and Bowyer, Kevin W. and Czajka, Adam},
  journal = {IEEE Transactions on Information Forensics and Security},
  title   = {Ensemble of Multi-View Learning Classifiers for Cross-Domain Iris Presentation Attack Detection},
  year    = {2019},
  number  = {6},
  pages   = {1419-1431},
  volume  = {14},
  doi     = {10.1109/TIFS.2018.2878542},
}




@InProceedings{Raghavendra_ESI_2014,
  author    = {Ramachandra Raghavendra and Kiran B. Raja and Christoph Busch},
  booktitle = {Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing},
  title     = {Ensemble of Statistically Independent Filters for Robust Contact Lens Detection in Iris Images},
  year      = {2014},
  address   = {New York, NY, USA},
  pages     = {24:1--24:7},
  publisher = {ACM},
  series    = {ICVGIP '14},
  abstract  = {Contact lenses are known to degrade the performance of the iris recognition system. Thus, accurate detection of a contact lens is of paramount importance not only in improving the reliability but also the security of an iris recognition system. In this paper we present a novel scheme for detecting contact lenses in iris images. The proposed scheme is based on an ensemble of statistically independent filters whose response on the iris image is processed and further classified using a linear Support Vector Machine (SVM) to detect the contact lens. Extensive experiments are carried out on two publicly available large scale databases, namely: IIIT-Delhi Contact lens iris database (IIITD) and Notre Dame cosmetic contact lens database 2012 (ND) that are comprised of contact lens iris samples captured using four different sensors. The rigorous experiments conducted in this work show the outstanding performance of the proposed scheme, especially in detecting a textured contact lens with a Correct Classification Rate (CCR) of 100%. We also present a comprehensive benchmark of the proposed scheme with six different well established state-of-the-art schemes available for the contact lens detection.},
  acmid     = {2683507},
  articleno = {24},
  doi       = {10.1145/2683483.2683507},
  isbn      = {978-1-4503-3061-9},
  keywords  = {Biometrics, Contact lens, Iris recognition, Lens detection},
  location  = {Bangalore, India},
  numpages  = {7},
  url       = {http://doi.acm.org/10.1145/2683483.2683507},
}

@Article{Doyle_IEEEAccess_2015,
  author    = {James S. Doyle and Kevin W. Bowyer},
  journal   = J_ACC,
  title     = {Robust Detection of Textured Contact Lenses in Iris Recognition Using BSIF},
  year      = {2015},
  issn      = {2169-3536},
  pages     = {1672-1683},
  volume    = {3},
  abstract  = {This paper considers three issues that arise in creating an algorithm for the robust detection of textured contact lenses in iris recognition images. The first issue is whether the accurate segmentation of the iris region is required in order to achieve the accurate detection of textured contact lenses. Our experimental results suggest that accurate iris segmentation is not required. The second issue is whether an algorithm trained on the images acquired from one sensor will well generalize to the images acquired from a different sensor. Our results suggest that using a novel iris sensor can significantly degrade the correct classification rate of a detection algorithm trained with the images from a different sensor. The third issue is how well a detector generalizes to a brand of textured contact lenses, not seen in the training data. This paper shows that a novel textured lens type may have a significant impact on the performance of textured lens detection.},
  comment   = {iris:PAD},
  doi       = {10.1109/ACCESS.2015.2477470},
  keywords  = {image classification;image segmentation;image texture;iris recognition;object detection;BSIF;classification rate;iris recognition;iris region segmentation;iris sensor;robust textured contact lenses detection;Classification algorithms;Contact lenses;Detection algorithms;Detectors;Eyes;Image processing;Image segmentation;Iris recognition;Lenses;Biometrics;image classification;image processing;image texture analysis;machine learning},
  publisher = {IEEE},
}

@Online{MASEK_SOFTWARE_URL,
  abstract     = {Introduction: I have made the MATLAB source code for my system publicly available for research and testing purposes. For details of the underlying scientific principles please download my dissertation, available on the main project page. Limitations: The system basically inputs an eye image, and outputs a binary biometric template. Also included is code for calculating the Hamming distance between templates, which is required for matching of templates. I have not included code for storage of templates into a database, and code for determining match performance, although I may make this code available at a later stage. A database of eye images is available from the Chinese Academy of Sciences. Abstract: A biometric system provides automatic identification of an individual based on a unique feature or characteristic possessed by the individual. Iris recognition is regarded as the most reliable and accurate biometric identification system available. Most commercial iris recognition systems use patented algorithms developed by Daugman, and these algorithms are able to produce perfect recognition rates. However, published results have usually been produced under favourable conditions, and there have been no independent trials of the technology. My work has involved developing an ‘open-source’ iris recognition system in order to verify both the uniqueness of the human iris and also its performance as a biometric. For determining the recognition performance of the system two databases of digitised greyscale eye images were used. The iris recognition system consists of an automatic segmentation system that is based on the Hough transform, and is able to localise the circular iris and pupil region, occluding eyelids and eyelashes, and reflections. The extracted iris region was then normalised into a rectangular block with constant dimensions to account for imaging inconsistencies. Finally, the phase data from 1D Log-Gabor filters was extracted and quantised to four levels to encode the unique pattern of the iris into a bit-wise biometric template. The Hamming distance was employed for classification of iris templates, and two templates were found to match if a test of statistical independence was failed. The system performed with perfect recognition on a set of 75 eye images; however, tests on another set of 624 images resulted in false accept and false reject rates of 0.005% and 0.238% respectively. Therefore, iris recognition is shown to be a reliable and accurate biometric technology.},
  author       = {Libor Masek and Peter Kovesi},
  lastaccessed = {January 3, 2018},
  title        = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
  url          = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
  year         = {2003},
}

@Misc{Thalheim_CT_2002,
  author       = {Lisa Thalheim and Jan Krissler and Peter-Michael Ziegler},
  howpublished = {on-line},
  title        = {{Biometric Access Protection Devices and their Programs Put to the Test, Available online in c't Magazine, No. 11/2002, p. 114}},
  year         = {2002},
  abstract     = {Memorizing passwords is out. Laying your finger on a sensor or peering into a webcam can suffice to gain you immediate access to a system. There is the danger, however, that this new ease might be bought at the expense of security. How well do biometric access controls prevent unauthorized access? We have tested eleven products for you. According to estimates of the IBIA, the international organization of biometric devices and programs suppliers, worldwide turnover of biometric security devices and programs this year will for the first time exceed the 500 million euro limit. Though the growth is primarily being driven by large-scale orders by industrial customers and administrative bodies, nevertheless the number of products on the market designed for in-home and in-house PC use is rising. The range of biometric security access tools for PCs meanwhile extends from mice and keyboards with integrated fingerprint scanners to webcam solutions whose software is able to recognize the facial features of registered persons to scanners that make use of the distinct iris patters of humans for identifying individuals. When the PC is booted the security software that goes with the tool writes itself into the log-on routine expanding the latter to include biometric authentication. In many instances the screen saver is integrated into the routine thus allowing for biometric authentication after breaks from work while the PC is still running. Sophisticated solutions, moreover, permit biometry based security protection of specific programs and/or documents.},
}

@InProceedings{Komogortsev_ICB_2013,
  author    = {Oleg V. Komogortsev and Alex Karpov},
  booktitle = C_ICB,
  title     = {Liveness detection via oculomotor plant characteristics: Attack of mechanical replicas},
  year      = {2013},
  address   = {Madrid, Spain},
  month     = {June},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {A novel approach that performs liveness detection for biometric modalities that use eye movement signal for person identification is proposed and evaluated. Liveness detection is done via estimation and analysis of the internal non-visible anatomical structure of the human eye termed Oculomotor Plant Characteristics (OPC). At this stage of its development the OPC approach targets prevention of spoof attacks that are generated by the accurate mechanical replicas of the human eye. We generalize and test two classes of such eye replicas via their mathematical representations. Specifically, we investigate following classes of replicas: a) those that are built using default OPC values specified by the research literature, and b) those that are built from the OPC specific to an individual. The results that involved processing live data from 32 individuals over four recording sessions and their eye replicas indicate relatively high theoretical resistance of the OPC liveness detection method to the mechanical attack that impersonates an authentic user.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICB.2013.6612984},
  issn      = {2376-4201},
  keywords  = {authorisation;eye;face recognition;fingerprint identification;iris recognition;object detection;OPC liveness detection method;biometric modalities;eye movement signal;eye replicas;face domain;fingerprint domain;human eye termed oculomotor plant characteristics;internal nonvisible anatomical structure analysis;internal nonvisible anatomical structure estimation;iris domain;mathematical representations;mechanical replica attack;oculomotor plant characteristics;person identification;recording sessions;spoof attack prevention;Brain modeling;Detectors;Fingerprint recognition;Iris recognition;Mathematical model;Muscles;Tracking},
}

@InCollection{HeXiaofu_ICB_2009,
  author    = {He, Xiaofu and Lu, Yue and Shi, Pengfei},
  booktitle = C_ICB,
  publisher = {Springer Berlin Heidelberg},
  title     = {A New Fake Iris Detection Method},
  year      = {2009},
  address   = {Berlin, Heidelberg},
  editor    = {Tistarelli, Massimo and Nixon, Mark S.},
  isbn      = {978-3-642-01793-3},
  pages     = {1132--1139},
  abstract  = {Recent research works have revealed that it is not difficult to spoof an automated iris recognition system using fake iris such as contact lens and paper print etc. Therefore, it is very important to detect fake iris as much as possible. In this paper, we propose a new fake iris detection method based on wavelet packet transform. First, wavelet packet decomposition is used to extract the feature values which provide unique information for discriminating fake irises from real ones. Second, to enhance the detecting accuracy of fake iris, Support vector machine (SVM) is used to characterize the distribution boundary based on extracted wavelet packet features, for it has good classification performance in high dimensional space and it is originally developed for two-class problems. The experimental results indicate the proposed method is to be a very promising technique for making iris recognition systems more robust against fake iris spoofing attempts.},
  doi       = {10.1007/978-3-642-01793-3_114},
  url       = {http://dx.doi.org/10.1007/978-3-642-01793-3_114},
}

@Article{Lee_IMA_2010,
  author    = {Eui Chul Lee and Kang Ryoung Park},
  journal   = {International Journal of Imaging Systems and Technology},
  title     = {Fake iris detection based on 3D structure of iris pattern},
  year      = {2010},
  issn      = {1098-1098},
  number    = {2},
  pages     = {162--166},
  volume    = {20},
  abstract  = {A new fake iris detection method based on 3D feature of iris pattern is proposed. In pervious researches, they did not consider 3D structure of iris pattern, but only used 2D features of iris image. However, in our method, by using four near infra-red (NIR) illuminators attached on the left and right sides of iris camera, we could obtain the iris image in which the 3D structure of iris pattern could be shown distinctively. Based on that, we could determine the live or fake iris by wavelet analysis of the 3D feature of iris pattern. Experimental result showed that the Equal Error Rate (EER) of determining the live or fake iris was 0.33%.},
  doi       = {10.1002/ima.20227},
  keywords  = {fake iris detection, 3D feature of iris pattern, wavelet analysis},
  publisher = {John Wiley \& Sons, Inc. New York, NY, USA},
  url       = {http://dx.doi.org/10.1002/ima.20227},
}

@InProceedings{Hughes_HICSS_2013,
  author    = {Ken Hughes and Kevin W. Bowyer},
  booktitle = {2013 46th Hawaii International Conference on System Sciences},
  title     = {Detection of Contact-Lens-Based Iris Biometric Spoofs Using Stereo Imaging},
  year      = {2013},
  address   = {Wailea, Maui, HI, USA},
  month     = {January},
  pages     = {1763-1772},
  publisher = {IEEE},
  abstract  = {Cosmetic contact lenses can be used to spoof an iris biometric system, either to evade being matched to a watch list or in principle even to masquerade as a selected other person. Existing approaches to detecting whether or not a person is wearing cosmetic contact lenses either are limited to detecting lenses created by a particular manufacturing technology, assume knowledge of the particular pattern printed in/on the lens, or require a sequence of images. We present proof-of-concept results for a method of detecting cosmetic contact lenses that is general, in the sense that it assumes nothing about the manufacturing technique or texture pattern of the lens, and that requires only a "snapshot" instance of imaging. The "snapshot" is a stereo pair of images, from which the shape of the surface of the iris texture region is estimated. In the absence of contacts or the presence of clear contacts, the iris region presents a coarse planar surface. In the presence of cosmetic contacts, the iris region presents a convex surface. Thus the problem of determining if a person is wearing a cosmetic contact lens is transformed into the problem of classifying the estimated surface shape for the iris region. This is the first approach to analyze iris biometric images in the context of 3D shape.},
  comment   = {iris:PAD},
  doi       = {10.1109/HICSS.2013.172},
  issn      = {1530-1605},
  keywords  = {Cameras;Cornea;Image segmentation;Iris;Iris recognition;Lenses;Shape;biometrics;contact lenses;identify theft;iris recognition;spoof detection},
}

@InCollection{HeZhaofeng_ICB_2009,
  author    = {He, Zhaofeng and Sun, Zhenan and Tan, Tieniu and Wei, Zhuoshi},
  booktitle = C_ICB,
  publisher = {Springer Berlin Heidelberg},
  title     = {Efficient Iris Spoof Detection via Boosted Local Binary Patterns},
  year      = {2009},
  address   = {Berlin, Heidelberg},
  editor    = {Tistarelli, Massimo and Nixon, Mark S.},
  isbn      = {978-3-642-01793-3},
  pages     = {1080--1090},
  abstract  = {Recently, spoof detection has become an important and challenging topic in iris recognition. Based on the textural differences between the counterfeit iris images and the live iris images, we propose an efficient method to tackle this problem. Firstly, the normalized iris image is divided into sub-regions according to the properties of iris textures. Local binary patterns (LBP) are then adopted for texture representation of each sub-region. Finally, Adaboost learning is performed to select the most discriminative LBP features for spoof detection. In particular, a kernel density estimation scheme is proposed to complement the insufficiency of counterfeit iris images during Adaboost training. The comparison experiments indicate that the proposed method outperforms state-of-the-art methods in both accuracy and speed.},
  comment   = {iris:PAD},
  doi       = {10.1007/978-3-642-01793-3_109},
  url       = {http://dx.doi.org/10.1007/978-3-642-01793-3_109},
}

@InProceedings{He_CCPR_2008,
  author    = {Xiaofu He and Yue Lu and Pengfei Shi},
  booktitle = {Chinese Conference on Pattern Recognition},
  title     = {A Fake Iris Detection Method Based on FFT and Quality Assessment},
  year      = {2008},
  address   = {Beijing, China},
  month     = {Oct},
  pages     = {1-4},
  publisher = {IEEE},
  abstract  = {In recent years, iris recognition is becoming a very active topic in both research and practical applications. However, fake iris is a potential threat there are potential threats for iris-based systems. This paper presents a novel fake iris detection method based on the analysis of 2-D Fourier spectra together with iris image quality assessment. First, image quality assessment method is used to exclude the defocused, motion blurred fake iris. Then statistical properties of Fourier spectra for fake iris are used for clear fake iris detection. Experimental results show that the proposed method can detect photo iris and printed iris effectively.},
  doi       = {10.1109/CCPR.2008.68},
  keywords  = {biometrics (access control);fast Fourier transforms;image recognition;statistical analysis;2D Fourier spectra;fake iris detection;fast Fourier transforms;image quality assessment;iris recognition;statistical properties;Biometrics;Counterfeiting;Frequency;Image analysis;Image quality;Iris recognition;Lenses;Optical reflection;Pattern recognition;Quality assessment},
}

@InProceedings{Zhang_CPR_2010,
  author    = {Hui Zhang and Zhenan Sun and Tieniu Tan},
  booktitle = C_ICPR,
  title     = {Contact Lens Detection Based on Weighted LBP},
  year      = {2010},
  address   = {Istanbul, Turkey},
  month     = {Aug},
  pages     = {4279-4282},
  publisher = {IEEE},
  abstract  = {Spoof detection is a critical function for iris recognition because it reduces the risk of iris recognition systems being forged. Despite various counterfeit artifacts, cosmetic contact lens is one of the most common and difficult to detect. In this paper, we proposed a novel fake iris detection algorithm based on improved LBP and statistical features. Firstly, a simplified SIFT descriptor is extracted at each pixel of the image. Secondly, the SIFT descriptor is used to rank the LBP encoding sequence. Then, statistical features are extracted from the weighted LBP map. Lastly, SVM classifier is employed to classify the genuine and counterfeit iris images. Extensive experiments are conducted on a database containing more than 5000 fake iris images by wearing 70 kinds of contact lens, and captured by four iris devices. Experimental results show that the proposed method achieves state-of-the-art performance in contact lens spoof detection.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICPR.2010.1040},
  issn      = {1051-4651},
  keywords  = {image coding;image sequences;iris recognition;object detection;statistical analysis;transforms;LBP encoding sequence;contact lens spoof detection;fake iris detection algorithm;iris recognition;statistical features;weighted LBP;Databases;Feature extraction;Iris;Iris recognition;Lenses;Pixel;Support vector machines;cosmetic contact lens;fake iris;spoof detection;weighted LBP},
}

@Article{Villalobos_ESA_2013,
  author    = {Fabiola M. Villalobos-Castaldi and Ernesto Suaste-G\'{o}mez},
  journal   = {Expert Systems with Applications},
  title     = {A new spontaneous pupillary oscillation-based verification system},
  year      = {2013},
  issn      = {0957-4174},
  number    = {13},
  pages     = {5352 - 5362},
  volume    = {40},
  abstract  = {Abstract A novel pupillary-based verification system is introduced, along with the early identity authentication results and analysis, based on the spatio-temporal features computed from the spontaneous pupillary oscillations. The authors demonstrate that this biometric trait has the capability to provide enough discriminative information to authenticate the identity of a subject. A new methodology to compute the spatio-temporal biometric template recordings of the pupil area changes, in a video-oculography sequence under constant luminance level, is also introduced in this paper. According to the authors’ knowledge, there is no evidence that other attempts were made, addressing this methodology to distinguish individuals based on the spatio-temporal representations, computed from the normal dilation-contraction behavior of the pupil. In this work, liveness will be detected by using the information obtained from the spontaneous pupillary oscillation mechanism. Preliminary experiments were conducted by using a particular own collected database, resulting in a (Equal Error Rate) in the order of 0.2338%.},
  comment   = {iris:PAD},
  doi       = {https://doi.org/10.1016/j.eswa.2013.03.042},
  keywords  = {Spatio-temporal biometric template, Video-based verification system, Spontaneous pupillary oscillations, Pupillary hippus, Biometric authentication},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S0957417413002297},
}

@Article{Cohn_BRMIC_2003,
  author    = {Jeffrey F. Cohn and Jing Xiao and Tsuyoshi Moriyama and Zara Ambadar and Takeo Kanade},
  journal   = {Behavior Research Methods, Instruments, {\&} Computers},
  title     = {Automatic recognition of eye blinking in spontaneously occurring behavior},
  year      = {2003},
  issn      = {1532-5970},
  number    = {3},
  pages     = {420--428},
  volume    = {35},
  abstract  = {Previous researchin automatic facial expression recognition has been limited to recognition of gross expression categories (e.g., joy or anger) in posed facial behavior under well-controlled conditions (e.g., frontal pose and minimal out-of-plane head motion). We have developed a system that detects a discrete and important facial action (e.g., eye blinking) in spontaneously occurring facial behavior that has been measured with a nonfrontal pose, moderate out-of-plane head motion, and occlusion. The system recovers three-dimensional motion parameters, stabilizes facial regions, extracts motion and appearance information, and recognizes discrete facial actions in spontaneous facial behavior. We tested the system in video data from a two-person interview. The 10 subjects were ethnically diverse, action units occurred during speech, and out-of-plane motion and occlusion from head motion and glasses were common. The video data were originally collected to answer substantive questions in psychology and represent a substantial challenge to automated action unit recognition. In analysis of blinks, the system achieved 98{\%} accuracy.},
  comment   = {iris:PAD},
  doi       = {10.3758/BF03195519},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.3758/BF03195519},
}

@InProceedings{Puhan_ISCE_2011,
  author    = {Niladri Bihari Puhan and Sudha Natarajan and A. Suhas Hegde},
  booktitle = C_ISCE,
  title     = {A new iris liveness detection method against contact lens spoofing},
  year      = {2011},
  address   = {Singapore},
  month     = {June},
  pages     = {71-74},
  publisher = {IEEE},
  abstract  = {Liveness detection is an important and challenging issue in iris recognition system security against spoofing. In this paper, a new iris liveness detection method is proposed against semi-transparent contact lens based spoofing. The proposed method works through iris texture dissimilarity between two iris regions due to pupillary light reflex. The texture dissimilarity is computed in the iris region that can be superimposed by a contact lens and it exists from the expanded pupil to outer iris boundary. The normalized Hamming distance computed on binary texture features is used to quantify textural dissimilarity between localized iris regions. Simulation results show that live iris textures produce high values of Hamming distance with the decreasing trend from inner to outer iris boundary.},
  comment   = {iris:PAD},
  doi       = {10.1109/ISCE.2011.5973786},
  issn      = {0747-668X},
  keywords  = {Hamming codes;contact lenses;image texture;iris recognition;vision defects;Hamming distance;binary texture features;iris liveness detection method;iris recognition system security;iris texture dissimilarity;pupillary light reflex;semitransparent contact lens spoofing;Feature extraction;Hamming distance;Image segmentation;Iris;Iris recognition;Lenses;Pixel},
}

@InProceedings{Lee_BS_2006,
  author    = {Sung Joo Lee and Kang Ryoung Park and Jaihie Kim},
  booktitle = {Biometrics Symposium: Special Session on Research at the Biometric Consortium Conference},
  title     = {Robust Fake Iris Detection Based on Variation of the Reflectance Ratio Between the Iris and the Sclera},
  year      = {2006},
  address   = {Baltimore, MD, USA},
  month     = {September},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {In this paper, we propose a new fake iris detection method based on the changes in the reflectance ratio between the iris and the sclera. The proposed method has four advantages over previous works. First, it is possible to detect fake iris images with high accuracy. Second, our method does not cause inconvenience to users since it can detect fake iris images at a very fast speed. Third, it is possible to show the theoretical background of using the variation of the reflectance ratio between the iris and the sclera. To compare fake iris images with live ones, three types of fake iris images were produced: a printed iris, an artificial eye, and a fake contact lens. In the experiments, we prove that the proposed fake iris detection method achieves high performance when distinguishing between live and fake iris.},
  comment   = {iris:PAD},
  doi       = {10.1109/BCC.2006.4341624},
  keywords  = {Authentication;Biometrics;Cameras;Fingerprint recognition;Fingers;Infrared imaging;Iris recognition;Reflectivity;Robustness;Waveguide discontinuities},
}

@InProceedings{Connell_ASSP_2013,
  author    = {Jonathan Connell and Nalini Ratha and James Gentile and Ruud Bolle},
  booktitle = C_ASSP,
  title     = {Fake iris detection using structured light},
  year      = {2013},
  address   = {Vancouver, BC, Canada},
  month     = {May},
  pages     = {8692-8696},
  publisher = {IEEE},
  abstract  = {Iris recognition has gained popularity due to factors such as its perceived high accuracy, significant usability advantages attributed to its non-contact acquisition method, and the availability of low cost sensors due to improvements in technology. However, non-contact biometrics authentication systems are vulnerable to different types of attacks than contact-type biometrics, such as fingerprints, for which there are a number of simple techniques to guard against attacks. In particular, the fashion industry has developed designer contact lenses with patterns that range from a simple change in eye color to the imposition of stars or other festive decorations. As these lenses are readily available and can be personalized at a very affordable price, their use in thwarting or spoofing iris-based authentication systems becomes plausible. Given the high security nature of many of these systems, there is a urgent need for a some countermeasure to this type of attack. In this paper, we describe a novel method to detect the presence of fake iris patterns, such as designer contact lenses, during the image acquisition stage to further enhance the basic security value of iris biometrics. Exploiting the anatomy and geometry of the human eye, we present a structured light projection method to detect the presence of artificial items obscuring the real iris. The detection principle has been verified using an inexpensive experimental setup consisting of a miniature projector and an offset camera. We also describe a novel algorithm to process the acquired images to find patterned contact lenses, and measure its performance using data collected with our apparatus. We argue that the addition of the proposed system and algorithm to existing iris biometrics based authentication systems will significantly improve their security.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICASSP.2013.6639363},
  issn      = {1520-6149},
  keywords  = {iris recognition;designer contact lens;fake iris detection;fake iris pattern;image acquisition;iris based authentication system;iris biometrics;iris recognition;noncontact biometrics authentication system;structured light projection method;Authentication;Iris;Iris recognition;Lenses;Pattern recognition;designer contact lenses;iris recognition;spoofing iris acquisition;structured light projection},
}

@InProceedings{Rigas_IJCB_2014,
  author    = {Ioannis Rigas and Oleg V. Komogortsev},
  booktitle = C_IJCB,
  title     = {Gaze estimation as a framework for iris liveness detection},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {This work investigates the possibility of detecting iris print-attacks via the analysis of a number of gaze-related features acquired in a process of eye tracking. Gaze estimation algorithms employ models based on the physical structure and function of the eye, providing thus a number of salient features that can be potentially employed for the detection of spoofing print-attacks. In our study, a combined dataset was assembled for the investigation of these features, consisting of eye movement recordings and the corresponding iris images collected from 100 subjects. The collected iris images were utilized in direct implementation of iris print-attacks against an eye tracking device. We developed a methodology for the detection of spoof indicative artifacts in the recorded signals, and fed the extracted features from the live and spoof eye signals into a two-class SVM classifier. The obtained results indicate a best correct classification rate (CCR) of 95.7%. Furthermore, we demonstrate the moderate decrease in liveness detection rates during subsampling of the eye movement signal to frequencies as low as 15 Hz. This result indicates the usefulness of running gaze estimation algorithms on existing iris recognition devices where such sampling frequency rate is common.},
  comment   = {iris:PAD},
  doi       = {10.1109/BTAS.2014.6996282},
  keywords  = {feature extraction;image classification;iris recognition;security of data;support vector machines;eye movement recordings;eye tracking device;feature extraction;gaze estimation algorithms;gaze-related features;iris images;iris liveness detection;iris print-attacks;iris recognition devices;spoof eye signals;spoof indicative artifacts;spoofing print-attacks;two-class SVM classifier;Estimation;Feature extraction;Iris recognition;Optical imaging;Reflection;Visualization},
}

@Article{Thavalengal_TCE_2016,
  author    = {Shejin Thavalengal and Tudor Nedelcu and Petronel Bigioi and Peter Corcoran},
  journal   = J_CE,
  title     = {Iris liveness detection for next generation smartphones},
  year      = {2016},
  issn      = {0098-3063},
  month     = {May},
  number    = {2},
  pages     = {95-102},
  volume    = {62},
  abstract  = {This paper presents a novel liveness detection method that exploits the acquisition workflow for iris biometrics on smartphones using a hybrid visible (RGB)/near infra-red (NIR) sensor. These devices are able to capture both RGB and NIR images of the eye and iris region in synchronization. This multi-spectral information is mapped into a discrete feature space. An intermediate classifier which uses a distance metric close to Jenson-Shannon divergence is employed to classify the incoming image. Further, a fast, multi-frame pupil localization technique using one-dimensional processing of the eye region is proposed and evaluated. This is used to analyze the pupil characteristics of the images classified as 'live' in the previous stage. It is shown that such an analysis could detect presentation attacks, even with a 3-D face model made of materials that has properties similar to human skin and the ocular region1.},
  doi       = {10.1109/TCE.2016.7514667},
  keywords  = {eye;face recognition;image capture;image classification;infrared detectors;iris recognition;smart phones;visible spectra;3D face model;Jenson-Shannon divergence;NIR image capture;NIR sensor;RGB image capture;RGB sensor;acquisition workflow;discrete feature space;distance metric;eye region;hybrid visible sensor;image classification;intermediate classifier;iris biometrics;iris liveness detection method;multiframe pupil localization technique;multispectral information mapping;near infrared sensor;next generation smartphone;one-dimensional processing;Authentication;Cameras;Hardware;Iris recognition;Smart phones;Smartphone;consumer biometrics;iris recognition;liveness},
  publisher = {IEEE},
}

@Article{Akhtar_SECURITY_2015,
  author    = {Zahid Akhtar and Christian Micheloni and Gian Luca Foresti},
  journal   = {IEEE Security Privacy},
  title     = {Biometric Liveness Detection: Challenges and Research Opportunities},
  year      = {2015},
  issn      = {1540-7993},
  month     = {Sept},
  number    = {5},
  pages     = {63-72},
  volume    = {13},
  abstract  = {In a spoofing attack, an impostor masquerades as a legitimate user by replicating that user's biometrics. Although methods exist to determine whether a live person or biometric artifact is in front of a biometric sensor, spoofing attacks remain a problem.},
  doi       = {10.1109/MSP.2015.116},
  keywords  = {biometrics (access control);biometric liveness detection;biometric sensor;spoofing attack;user biometrics;Computer security;Databases;Face recognition;Feature extraction;Fingerprint recognition;Iris recognition;biometrics;liveness detection;presentation attack;presentation attack detection (PAD);spoofing attacks},
  publisher = {IEEE},
}

@Article{Czajka_TIFS_2017,
  author    = {Adam Czajka and Kevin W. Bowyer and Michael Krumdick and Rosaura G. VidalMata},
  journal   = J_TIFS,
  title     = {Recognition of image-orientation-based iris spoofing},
  year      = {2017},
  issn      = {1556-6013},
  number    = {99},
  pages     = {1-1},
  volume    = {PP},
  abstract  = {This paper presents a solution to automatically recognize the correct left / right and upright / upside-down orientation of iris images. This solution can be used to counter spoofing attacks directed to generate fake identities by rotating an iris image or the iris sensor during the acquisition. Two approaches are compared on the same data, using the same evaluation protocol: a) feature engineering, using hand-crafted features classified by a Support Vector Machine (SVM), and b) feature learning, using data-driven features learned and classified by a Convolutional Neural Network (CNN). A dataset of 20,750 iris images, acquired for 103 subjects using four sensors, was used for development. An additional subject-disjoint dataset of 1,939 images, from 32 additional subjects, was used for testing purposes. Both same-sensor and cross-sensor tests were carried out to investigate how the classification approaches generalize to unknown hardware. The SVM-based approach achieved an average correct classification rate above 95% (89%) for recognition of left / right (upright / upside-down) orientation when tested on subject- and camera-disjoint data, and 99% (97%) if the images were acquired by the same sensor. The CNN-based approach performed better for same-sensor experiments, and presented slightly worse generalization capabilities to unknown sensors when compared to the SVM. We are not aware of any other papers on automatic recognition of upright / upside-down orientation of iris images, or studying both hand-crafted and data-driven features in same-sensor and cross-sensor subjectdisjoint experiments. The datasets used in this work, along with random splits of the data used in cross-validation, are being made available.},
  doi       = {10.1109/TIFS.2017.2701332},
  keywords  = {Ducts;Head;Image recognition;Iris;Iris recognition;Neural networks;Support vector machines},
  publisher = {IEEE},
}

@InProceedings{Akhtar_AVSS_2014,
  author    = {Zahid Akhtar and Christian Micheloni and Claudio Piciarelli and Gian Luca Foresti},
  booktitle = C_AVSS,
  title     = {MoBio\_LivDet: Mobile biometric liveness detection},
  year      = {2014},
  address   = {Seoul, South Korea},
  month     = {Aug},
  pages     = {187-192},
  publisher = {IEEE},
  abstract  = {Biometric authentication is now being used ubiquitously as an alternative to passwords on mobile devices. However, current biometric systems are vulnerable to simple spoofing attacks. Several liveness detection methods have been proposed to determine whether there is a live person or an artificial replica in front of the biometric sensor. Yet, the problem is unsolved due to hardship in finding discriminative and computationally inexpensive features for spoofing attacks. Moreover, previous liveness detection approaches are not explicitly aimed for mobile biometric, thus principally unsuited for portable devices. Therefore, we build a software-based multi-biometric prototype that detects face, iris and fingerprint spoofing attacks on mobile devices. We present MoBio_LivDet (Mobile Biometric Liveness Detection), a novel approach that analyzes local features and global structures of the biometric images using a set of low-level feature descriptors and decision level fusion. The system allows user to balance the security level (robustness against spoofing) and convenience that they want. The proposed method is highly fast, simple, efficient, robust and does not require user-cooperation, thus making it extremely apt for mobile devices. Experimental analysis on publicly available face, iris and fingerprint data sets with real spoofing attacks show promising results.},
  doi       = {10.1109/AVSS.2014.6918666},
  keywords  = {face recognition;fingerprint identification;image fusion;iris recognition;mobile computing;MoBio_LivDet;artificial replica;biometric authentication;biometric images;biometric sensor;decision level fusion;face detection;fingerprint spoofing attacks;global structures;iris detection;live person;local features;low-level feature descriptors;mobile biometric liveness detection;mobile devices;software-based multibiometric prototype;Face;Feature extraction;Fingerprint recognition;Iris recognition;Mobile communication;Mobile handsets;Support vector machines},
}

@InProceedings{Alonso-Fernandez_MIPRO_2014,
  author    = {Fernando Alonso-Fernandez and Josef Big\"{u}n},
  booktitle = C_MIPRO,
  title     = {Exploiting periocular and RGB information in fake iris detection},
  year      = {2014},
  address   = {Opatija, Croatia},
  month     = {May},
  pages     = {1354-1359},
  publisher = {IEEE},
  abstract  = {Fake iris detection has been studied by several researchers. However, to date, the experimental setup has been limited to near-infrared (NIR) sensors, which provide grey-scale images. This work makes use of images captured in visible range with color (RGB) information. We employ Gray-Level CoOccurrence textural features and SVM classifiers for the task of fake iris detection. The best features are selected with the Sequential Forward Floating Selection (SFFS) algorithm. To the best of our knowledge, this is the first work evaluating spoofing attack using color iris images in visible range. Our results demonstrate that the use of features from the three color channels clearly outperform the accuracy obtained from the luminance (gray scale) image. Also, the R channel is found to be the best individual channel. Lastly, we analyze the effect of extracting features from selected (eye or periocular) regions only. The best performance is obtained when GLCM features are extracted from the whole image, highlighting that both the iris and the surrounding periocular region are relevant for fake iris detection. An added advantage is that no accurate iris segmentation is needed. This work is relevant due to the increasing prevalence of more relaxed scenarios where iris acquisition using NIR light is unfeasible (e.g. distant acquisition or mobile devices), which are putting high pressure in the development of algorithms capable of working with visible light.},
  doi       = {10.1109/MIPRO.2014.6859778},
  keywords  = {feature extraction;image classification;image colour analysis;image texture;iris recognition;support vector machines;GLCM feature extraction;RGB information;SFFS algorithm;SVM classifiers;color iris images;fake iris detection;gray-level co-occurrence textural features;luminance image;periocular region information;sequential forward floating selection;spoofing attack;three color channels;visible range imaging;Accuracy;Databases;Feature extraction;Image color analysis;Iris;Iris recognition;Support vector machines},
}

@InProceedings{Bryant_ASRC_2010,
  author    = {Bryant, Kelvin S. and Dozier, Gerry V.},
  booktitle = {Proceedings of the 48th Annual Southeast Regional Conference},
  title     = {A Two-phased Approach to Reducing the False Accept Rate of Spoofed Iris Codes},
  year      = {2010},
  address   = {New York, NY, USA},
  pages     = {28:1--28:4},
  publisher = {ACM},
  series    = {ACM SE'10},
  abstract  = {In this paper, we demonstrate how to reduce the chance of a spoofed iris code being falsely accepted by an iris recognition system. We simulate the system attack by taking one of the registered iris codes from a subject set and mutating it by several different rates and presenting the resultant iris codes to our system. Our approach uses the k-nearest neighbors from a training set to the known spoof to establish a critical distance. Presented iris codes from our mutant set that have a Hamming Ratio when compared to the spoof that is less than the critical distance are rejected. Those that are falsely accepted are totaled to produce a Spoof False Accept Rate (SP-FAR). The second phase of our approach uses traditional iris code recognition to reduce the SP-FAR by rejecting those spoofs that were mutated to a degree such that they will not match any of the other iris codes in the training set.},
  acmid     = {1900048},
  articleno = {28},
  doi       = {10.1145/1900008.1900048},
  isbn      = {978-1-4503-0064-3},
  keywords  = {Iris, biometrics, spoof},
  location  = {Oxford, Mississippi},
  numpages  = {4},
  url       = {http://doi.acm.org/10.1145/1900008.1900048},
}

@Article{Chen_PRL_2012,
  author    = {Rui Chen and Xirong Lin and Tianhuai Ding},
  journal   = J_PRL,
  title     = {Liveness detection for iris recognition using multispectral images},
  year      = {2012},
  issn      = {0167-8655},
  number    = {12},
  pages     = {1513 - 1519},
  volume    = {33},
  abstract  = {Liveness detection is a necessary step towards higher reliability of iris recognition. In this research, we propose a novel iris liveness detection method based on multi-features extracted from multispectral images. First, we analyze the specific multispectral characteristics of conjunctival vessels and iris textures. To ensure the effective utilization of these characteristics, iris images are simultaneously captured at near-infrared (860 nm) and blue (480 nm) wavelengths. Then we respectively define and measure relative number of conjunctival vessels (RNCV) and entropy ratio of iris textures (ERIT) using 860-nm and 480-nm images. Finally, the feature values of \{RNCV\} and \{ERIT\} are arranged to form a robust 2-D feature vector. The trained Support Vector Machine (SVM) is used to classify the feature vectors extracted from live and fake irises. Experimental results demonstrate that the proposed method can discriminate between live irises and various types of fake irises with high classification accuracy and low computational cost.},
  doi       = {https://doi.org/10.1016/j.patrec.2012.04.002},
  keywords  = {Liveness detection, Multispectral images, Conjunctival vessel detection, Wavelet packet decomposition},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167865512001262},
}

@Article{Corcoran_CEM_2016,
  author    = {Peter Corcoran and Claudia Costache},
  journal   = J_CEM,
  title     = {Biometric Technology and Smartphones: A consideration of the practicalities of a broad adoption of biometrics and the likely impacts},
  year      = {2016},
  issn      = {2162-2248},
  month     = {April},
  number    = {2},
  pages     = {70-78},
  volume    = {5},
  abstract  = {The Widespread Global Adoption of Smartphones across all demographics and the rapid commoditization of the technology to the point at which an entry-level device can be sold profitably for less than US$100 suggest that we are moving rapidly to a time at which almost everyone will own a smartphone. Or, perhaps more accurately, these devices will own us! They are compelling devices, combining a capability to act as a personal messaging hub, providing mobile access to web services, a sophisticated entertainment device for playing music and videos, and, most recently, a personal broadcasting engine created using new web technologies [1], should you require such capabilities. The ability of a smartphone to augment our daily lives is already effecting substantial changes in social behavior. For many years, it was considered quite rude to leave your cell phone active in meetings; today, it is quite acceptable to tap away at this gadget in your hand. Indeed, it now seems to be considered impolite to interrupt someone who is engaged in such arguably antisocial tapping.},
  doi       = {10.1109/MCE.2016.2521937},
  keywords  = {Web services;biometrics (access control);entertainment;mobile computing;smart phones;social sciences;Web services;Web technologies;antisocial tapping;biometric technology;cell phone;entertainment device;mobile access;personal broadcasting engine;personal messaging hub;smartphones;social behavior;Authentication;Biometrics;Cameras;Fingerprint recognition;Iris recognition;Smart phones},
  publisher = {IEEE},
}

@InProceedings{Yano_ICPR_2012,
  author    = {V. Yano and A. Zimmer and L. L. Ling},
  booktitle = C_ICPR,
  title     = {Multimodal biometric authentication based on iris pattern and pupil light reflex},
  year      = {2012},
  month     = {Nov},
  pages     = {2857-2860},
  abstract  = {Biometrics-based authentication is a method of personal identification that has some advantages over the password and object-based ones, mainly for the user, who doesn't need to carry or memorize anything. However, this kind of identification is also subject to problems. Besides the technology-related possibilities of fraud, such as system invasion, database corruption or algorithm injection, some of the common used bio-metric features can be faked. Furthermore, most cases of false rejection are related to the quality of the acquired sample. This paper proposes a multimodal bio-metric authentication method which incorporates the use of dynamic features of the human reflex and the iris pattern recognition for a better performance. A prototype system has been implemented and tested with 59 volunteers. Experimental results presented an EER of 2.44%.},
  issn      = {1051-4651},
  keywords  = {authorisation;feature extraction;fraud;iris recognition;dynamic features;fake biometric features;false rejection;human reflex;iris pattern recognition;multimodal biometric authentication method;personal identification;pupil light reflex;technology-related fraud possibility;Authentication;Cameras;Error analysis;Feature extraction;Humans;Iris recognition;Vectors},
}

@Article{Dunstone_BTT_2011,
  author    = {Ted Dunstone and Geoff Poulton},
  journal   = J_BTT,
  title     = {Vulnerability assessment},
  year      = {2011},
  month     = {May},
  pages     = {5-7},
  volume    = {2011},
  abstract  = {In the popular portrayal of biometrics, from Minority Report to the recent Get Smart movie, a common theme is the defeat of biometric systems using faked (or spoofed) biometrics. While such fears make for good stories, the issues relat- ing to the vulnerability of biometric systems are all too real. It is these issues that the Biometrics Institute, Australia, is addressing by introducing a method- ology for the identification and assessment of vulnerabilities – the Biometrics Vulnerability Assessment methodology (BVA).},
  issue     = {5},
  publisher = {Elsevier},
}

@Article{Trokielewicz_IVC_2017,
  author   = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  journal  = {Image and Vision Computing},
  title    = {Implications of ocular pathologies for iris recognition reliability},
  year     = {2017},
  issn     = {0262-8856},
  pages    = {158-167},
  volume   = {58},
  abstract = {This paper presents an analysis of how iris recognition is influenced by eye disease and an appropriate dataset comprising 2996 images of irises taken from 230 distinct eyes (including 184 affected by more than 20 different eye conditions). The images were collected in near infrared and visible light during routine ophthalmological examination. The experimental study carried out utilizing four independent iris recognition algorithms (MIRLIN, VeriEye, OSIRIS and IriCore) renders four valuable results. First, the enrollment process is highly sensitive to those eye conditions that obstruct the iris or cause geometrical distortions. Second, even those conditions that do not produce visible changes to the structure of the iris may increase the dissimilarity between samples of the same eyes. Third, eye conditions affecting the geometry or the tissue structure of the iris or otherwise producing obstructions significantly decrease same-eye similarity and have a lower, yet still statistically significant, influence on impostor comparison scores. Fourth, for unhealthy eyes, the most prominent effect of disease on iris recognition is to cause segmentation errors. To our knowledge this paper describes the largest database of iris images for disease-affected eyes made publicly available to researchers and offers the most comprehensive study of what we can expect when iris recognition is employed for diseased eyes.},
  doi      = {https://doi.org/10.1016/j.imavis.2016.08.001},
  keywords = {Iris recognition, Ocular disease, Biometrics, Ophthalmology},
  url      = {https://www.sciencedirect.com/science/article/pii/S0262885616301251},
}

@Article{Rebera_SEE_2014,
  author    = {Andrew P. Rebera and Matteo E. Bonfanti and Silvia Venier},
  journal   = J_SEE,
  title     = {Societal and Ethical Implications of Anti-Spoofing Technologies in Biometrics},
  year      = {2014},
  issn      = {1471-5546},
  number    = {1},
  pages     = {155--169},
  volume    = {20},
  abstract  = {Biometric identification is thought to be less vulnerable to fraud and forgery than are traditional forms of identification. However biometric identification is not without vulnerabilities. In a `spoofing attack' an artificial replica of an individual's biometric trait is used to induce a system to falsely infer that individual's presence. Techniques such as liveness-detection and multi-modality, as well as the development of new and emerging modalities, are intended to secure biometric identification systems against such threats. Unlike biometrics in general, the societal and ethical issues raised by spoofing and anti-spoofing techniques have not received much attention. This paper examines these issues.},
  doi       = {10.1007/s11948-013-9440-9},
  publisher = {Springer},
  url       = {http://dx.doi.org/10.1007/s11948-013-9440-9},
}

@InProceedings{Komulainen_IJCB_2014,
  author    = {Jukka Komulainen and Abdenour Hadid and Matti Pietik\"{a}inen},
  booktitle = C_IJCB,
  title     = {Generalized textured contact lens detection by extracting BSIF description from Cartesian iris images},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {Sept},
  pages     = {1-7},
  publisher = {IEEE},
  abstract  = {Textured contact lenses cause severe problems for iris biometric systems because they can be used to alter the appearance of iris texture in order to deliberately increase the false positive and, especially, false negative match rates. Many texture analysis based techniques have been proposed for detecting the presence of cosmetic contact lenses. However, it has been shown recently that the generalization capability of the existing approaches is not sufficient because they have been developed for detecting specific lens texture patterns and evaluated only on those same lens types seen during development phase. This scenario does not apply in unpredictable practical applications because unseen lens patterns will be definitely experienced in operation. In this paper, we address this issue by studying the effect of different iris image preprocessing techniques and introducing a novel approach formore generalized cosmetic contact lens detection using binarized statistical image features (BSIF).Our extensive experimental analysis on benchmark datasets shows that the BSIF description extracted from preprocessed Cartesian iris texture images yields to promising generalization capabilities across unseen texture patterns and different iris sensors with mean equal error rate of 0.14%and 0.88%, respectively. The findings support the intuition that the textural differences between genuine iris texture and fake ones are best described by preserving the regular structure of different printing signatures without transforming the iris images into polar coordinate system.},
  doi       = {10.1109/BTAS.2014.6996237},
  keywords  = {contact lenses;feature extraction;generalisation (artificial intelligence);image matching;image sensors;image texture;iris recognition;statistical analysis;BSIF description extraction;Cartesian iris texture images;binarized statistical image features;cosmetic contact lenses;false negative match rates;generalized cosmetic contact lens detection;generalized textured contact lens detection;iris image preprocessing techniques;lens texture pattern detection;mean equal error rate;polar coordinate system;texture analysis based techniques;Feature extraction;Iris;Iris recognition;Lenses;Printing;Support vector machines;Training},
}

@InProceedings{Sequeira_TSP_2016,
  author    = {Ana F. Sequeira and Shejin Thavalengal and James Ferryman and Peter Corcoran and Jaime S. Cardoso},
  booktitle = C_TSP,
  title     = {A realistic evaluation of iris presentation attack detection},
  year      = {2016},
  address   = {Vienna, Austria},
  month     = {June},
  pages     = {660-664},
  publisher = {IEEE},
  abstract  = {Iris liveness detection methods have been developed to overcome the vulnerability of iris biometric systems to spoofing attacks. In the literature, it is typically assumed that a known attack modality will be perpetrated. Then liveness models are designed using labelled samples from both real/live and fake/spoof distributions, the latter derived from the assumed attack modality. In this work it is argued that a comprehensive modelling of the spoof samples is not possible in a real-world scenario where the attack modality cannot be known with a high degree of certainty. In fact making this assumption will render the liveness detection system more vulnerable to attacks that were not included in the original training. To provide a more realistic evaluation, this work proposes: a) testing the binary models with unknown spoof samples that were not present in the training step; b) the use of a single-class classification designing the classifier by modelling only the distribution of live samples. The results obtained support the assertion that many evaluation methods from the literature are misleading and may lead to optimistic estimates of the robustness of liveness detection in practical use cases.},
  doi       = {10.1109/TSP.2016.7760965},
  keywords  = {image classification;iris recognition;security of data;attack modality;binary models;iris biometric systems;iris liveness detection;iris presentation attack detection;single-class classification;spoof distributions;spoofing attacks;Biological system modeling;Databases;Iris recognition;Lenses;Testing;Training;Training data;biometrics;iris;one-class classification;presentation attack},
}

@InProceedings{Galbally_SWB_2007,
  author    = {Javier Galbally and Julian Fierrez and Javier Ortega-Garcia},
  booktitle = {Spanish Workshop on Biometrics (SWB)},
  title     = {Vulnerabilities in Biometric Systems: Attacks and Recent Advances in Liveness Detection},
  year      = {2007},
  address   = {Girona, Spain},
  pages     = {1-8},
  publisher = {Springer},
  abstract  = {A review of the state-of-the-art in direct and indirect at- tacks to fingerprint and iris automatic recognition security systems is presented. A summary of the novel liveness detection methods, which take advantage of different physiological properties to distinguish be- tween real and fake biometric traits, is also reported.},
}

@InCollection{He_ICB_2007,
  author    = {He, Xiaofu and An, Shujuan and Shi, Pengfei},
  booktitle = {Advances in Biometrics: International Conference, ICB 2007, Seoul, Korea, August 27-29, 2007. Proceedings},
  publisher = {Springer Berlin Heidelberg},
  title     = {Statistical Texture Analysis-Based Approach for Fake Iris Detection Using Support Vector Machines},
  year      = {2007},
  address   = {Berlin, Heidelberg},
  editor    = {Lee, Seong-Whan and Li, Stan Z.},
  isbn      = {978-3-540-74549-5},
  pages     = {540--546},
  abstract  = {This paper presents a novel statistical texture analysis based method for detecting fake iris. Four distinctive features based on gray level co-occurrence matrices (GLCM) and properties of statistical intensity values of image pixels are used. A support vector machine (SVM) is selected to characterize the distribution boundary, for it has good classification performance in high dimensional space. The proposed approach is privacy friendly and does not require additional hardware. The experimental results indicate the new approach to be a very promising technique for making iris recognition systems more robust against fake-iris-based spoofing attempts.},
  doi       = {10.1007/978-3-540-74549-5_57},
  url       = {http://dx.doi.org/10.1007/978-3-540-74549-5_57},
}

@InCollection{Park_AMDO_2006,
  author    = {Park, Kang Ryoung},
  booktitle = {Articulated Motion and Deformable Objects: 4th International Conference, AMDO 2006, Port d'Andratx, Mallorca, Spain, July 11-14, 2006. Proceedings},
  publisher = {Springer Berlin Heidelberg},
  title     = {Robust Fake Iris Detection},
  year      = {2006},
  address   = {Berlin, Heidelberg},
  editor    = {Perales, Francisco J. and Fisher, Robert B.},
  isbn      = {978-3-540-36032-2},
  pages     = {10--18},
  abstract  = {Among biometrics such as face, fingerprint, iris and voice recognition, iris recognition system has been in the limelight for high security applications. Until now, most researches have been studied for iris identification algorithm and iris camera system, etc. But, there has been little researched for fake iris (such as printed, photographed or artificial iris, etc) detection and its importance has been much emphasized, recently. To overcome the problems of previous fake iris detection researches, we propose the new method of checking the hippus movement (the dilation/contraction of pupil size) and the change of iris code in local iris area by visible light in this paper.},
  doi       = {10.1007/11789239_2},
  url       = {http://dx.doi.org/10.1007/11789239_2},
}

@InProceedings{Raja_BTAS_2015,
  author    = {Kiran B. Raja and Ramachandra Raghavendra and Christoph Busch},
  booktitle = C_BTAS,
  title     = {Presentation attack detection using Laplacian decomposed frequency response for visible spectrum and Near-Infra-Red iris systems},
  year      = {2015},
  address   = {Arlington, VA, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {Biometrics systems are being challenged at the sensor level using artefact presentation such as printed artefacts or electronic screen attacks. In this work, we propose a novel technique to detect the artefact iris images by decomposing the images into Laplacian pyramids of various scales and obtain frequency responses in different orientations. The obtained features are classified using a support vector machine with a polynomial kernel. Further, we extend the same technique with majority voting rule to provide the decision on artefact detection for video based iris recognition in the visible spectrum. The proposed technique is evaluated on the newly created visible spectrum iris video database and also Near-Infra-Red (NIR) images. The newly constructed visible spectrum iris video database is specifically tailored to study the vulnerability of presentation attacks on visible spectrum iris recognition using videos on a smartphone. The newly constructed database is referred as `Presentation Attack Video Iris Database' (PAVID) and consists of 152 unique iris patterns obtained from two different smartphone - iPhone 5S and Nokia Lumia 1020. The proposed technique has provided an Attack Classificiation Error Rate (ACER) of 0.64% on PAVID database and 1.37% on LiveDet iris dataset validating the robustness and applicability of the proposed presentation attack detection (PAD) algorithm in real life scenarios.},
  doi       = {10.1109/BTAS.2015.7358790},
  keywords  = {image classification;iris recognition;video databases;video signal processing;ACER;Laplacian decomposed frequency response;Laplacian pyramid;LiveDet iris dataset;NIR image;Nokia Lumia 1020;PAD algorithm;PAVID database;artefact detection;artefact iris image;artefact presentation;attack classificiation error rate;biometrics system;electronic screen attack;iPhone 5S;majority voting rule;near-infra-red image;near-infra-red iris system;polynomial kernel;presentation attack detection algorithm;presentation attack video iris database;printed artefact;smartphone;support vector machine;video based iris recognition;visible spectrum iris recognition;visible spectrum iris video database;Databases;Iris recognition;Kernel;Laplace equations;Support vector machines},
}

@Article{Rigas_PRL_2015,
  author   = {Ioannis Rigas and Oleg V. Komogortsev},
  journal  = J_PRL,
  title    = {Eye movement-driven defense against iris print-attacks},
  year     = {2015},
  issn     = {0167-8655},
  note     = {Special Issue on ``Soft Biometrics''},
  pages    = {316 - 326},
  volume   = {68, Part 2},
  abstract = {Abstract This paper proposes a methodology for the utilization of eye movement cues for the task of iris print-attack detection. We investigate the fundamental distortions arising in the eye movement signal during an iris print-attack, due to the structural and functional discrepancies between a paper-printed iris and a natural eye iris. The performed experiments involve the execution of practical print-attacks against an eye-tracking device, and the collection of the resulting eye movement signals. The developed methodology for the detection of print-attack signal distortions is evaluated on a large database collected from 200 subjects, which contains both the real (‘live’) eye movement signals and the print-attack (‘spoof’) eye movement signals. The suggested methodology provides a sufficiently high detection performance, with a maximum average classification rate (ACR) of 96.5% and a minimum equal error rate (EER) of 3.4%. Due to the hardware similarities between eye tracking and iris capturing systems, we hypothesize that the proposed methodology can be adopted into the existing iris recognition systems with minimal cost. To further support this hypothesis we experimentally investigate the robustness of our scheme by simulating conditions of reduced sampling resolution (temporal and spatial), and of limited duration of the eye movement signals.},
  doi      = {https://doi.org/10.1016/j.patrec.2015.06.011},
  keywords = {Eye movements, Anti-spoofing cues, Iris print-attack},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865515001737},
}

@Article{Hu_PRL_2016,
  author     = {Hu, Yang and Sirlantzis, Konstantinos and Howells, Gareth},
  journal    = J_PRL,
  title      = {Iris Liveness Detection Using Regional Features},
  year       = {2016},
  issn       = {0167-8655},
  month      = oct,
  number     = {P2},
  pages      = {242--250},
  volume     = {82},
  abstract   = {Regional features with both low level features and high level feature distribution.Intensity and local descriptors as low level features.Spatial pyramid model seeking feature distribution in regions with varying size.Relational measure expressing feature distribution in regions with varying shape.Experiments on both NIR and colour datasets. In this paper, we exploit regional features for iris liveness detection. Regional features are designed based on the relationship of the features in neighbouring regions. They essentially capture the feature distribution among neighbouring regions. We construct the regional features via two models: spatial pyramid and relational measure which seek the feature distributions in regions with varying size and shape respectively. The spatial pyramid model extracts features from coarse to fine grid regions, and, it models a local to global feature distribution. The local distribution captures the local feature variations while the global distribution includes the information that is more robust to translational transform. The relational measure is based on a feature-level convolution operation defined in this paper. By varying the shape of the convolution kernel, we are able to obtain the feature distribution in regions with different shapes. To combine the feature distribution information in regions with varying size and shape, we fuse the results based on the two models at the score level. Experimental results on benchmark datasets demonstrate that the proposed method achieves an improved performance compared to state-of-the-art features.},
  acmid      = {3030452},
  address    = {New York, NY, USA},
  doi        = {10.1016/j.patrec.2015.10.010},
  issue_date = {October 2016},
  keywords   = {Iris liveness detection, Local descriptors, Regional feature},
  numpages   = {9},
  publisher  = {Elsevier Science Inc.},
  url        = {https://doi.org/10.1016/j.patrec.2015.10.010},
}

@InProceedings{Kohli_BTAS_2016,
  author    = {Naman Kohli and Daksha Yadav and Mayank Vatsa and Richa Singh and Afzel Noore},
  booktitle = C_BTAS,
  title     = {Detecting medley of iris spoofing attacks using DESIST},
  year      = {2016},
  address   = {Niagara Falls, NY, USA},
  month     = {Sept},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Human iris is considered a reliable and accurate modality for biometric recognition due to its unique texture information. However, similar to other biometric modalities, iris recognition systems are also vulnerable to presentation attacks (commonly called spoofing) that attempt to conceal or impersonate identity. Examples of typical iris spoofing attacks are printed iris images, textured contact lenses, and synthetic creation of iris images. It is critical to note that majority of the algorithms proposed in the literature are trained to handle a specific type of spoofing attack. These algorithms usually perform very well on that particular attack. However, in real-world applications, an attacker may perform different spoofing attacks. In such a case, the problem becomes more challenging due to inherent variations in different attacks. In this paper, we focus on a medley of iris spoofing attacks and present a unified framework for detecting such attacks. We propose a novel structural and textural feature based iris spoofing detection framework (DESIST). Multi-order dense Zernike moments are calculated across the iris image which encode variations in structure of the iris image. Local Binary Pattern with Variance (LBPV) is utilized for representing textural changes in a spoofed iris image. The highest classification accuracy of 82.20% is observed by the proposed framework for detecting normal and spoofed iris images on a combined iris spoofing database.},
  doi       = {10.1109/BTAS.2016.7791168},
  keywords  = {Zernike polynomials;image texture;iris recognition;security of data;DESIST;biometric recognition;human iris;iris spoofing attack detection;local binary pattern with variance;multiorder dense Zernike moments;printed iris image;structural based iris spoofing detection;textural feature based iris spoofing detection;textured contact lens;Algorithm design and analysis;Databases;Feature extraction;Iris;Iris recognition;Lenses;Training},
}

@InCollection{Nixon_HoB_2008,
  author    = {Nixon, Kristin Adair and Aimale, Valerio and Rowe, Robert K.},
  booktitle = {Handbook of Biometrics},
  publisher = {Springer US},
  title     = {Spoof Detection Schemes},
  year      = {2008},
  address   = {Boston, MA},
  editor    = {Jain, Anil K. and Flynn, Patrick and Ross, Arun A.},
  isbn      = {978-0-387-71041-9},
  pages     = {403--423},
  doi       = {10.1007/978-0-387-71041-9_20},
  url       = {http://dx.doi.org/10.1007/978-0-387-71041-9_20},
}

@InProceedings{Raghavendra_IJCB_2014,
  author    = {Ramachandra Raghavendra and Christoph Busch},
  booktitle = C_IJCB,
  title     = {Presentation attack detection on visible spectrum iris recognition by exploring inherent characteristics of Light Field Camera},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {Presentation (or spoof) attacks on biometric system is a growing concern that received substantial attention from both academics and industry. In this paper, we present a novel way of addressing a Presentation Attack Detection (PAD) (or spoof detection) by exploiting the inherent characteristics of the Light Field Camera (LFC) for visible spectrum iris biometric system. The proposed PAD algorithm will capture the variation in the depth (or focus) between multiple depth images rendered by the LFC that in turn can be used to reveal the presentation attacks. To this extent, we introduce a new presentation attack database comprised of 52 subjects with 104 unique eye samples. The database is collected using LFC by simulating the attacks through visible spectrum iris biometric artefacts like printed photo and electronic display (using both Apple iPad (4th generation) and Samsung Galaxy Note 10.1 tablet). Extensive experiments carried out on this database reveal the efficacy of the proposed PAD algorithm with a lowest Average Classification Error Rate = 0.5% when confronted with diverse set of attacks on visible spectrum iris biometric system.},
  doi       = {10.1109/BTAS.2014.6996226},
  keywords  = {cameras;iris recognition;visible spectra;Apple iPad (4th generation);LFC;PAD algorithm;Samsung Galaxy Note 10.1 tablet;average classification error rate;depth images;electronic display;light field camera;presentation attack database;presentation attack detection;printed photo;spoof attack;spoof detection;visible spectrum iris biometric artefacts;visible spectrum iris biometric system;visible spectrum iris recognition;Cameras;Databases;Image resolution;Iris recognition;Protocols;Tablet computers},
}

@InProceedings{Sun_GRAPHICON_2008,
  author    = {Zhenan Sun and Wenbo Dong and Tieniu Tan},
  booktitle = {Graphicon, Moscow State University},
  title     = {Technology Roadmap for Smart Iris Recognition},
  year      = {2008},
  month     = {June},
  pages     = {1-7},
  abstract  = {Iris recognition has many desirable properties for reliable individual authentication but usability is its largest bottleneck to wide deployment. Thus smart interface and machine intelligent are the objective of next-generation iris recognition. This paper presents the technology roadmap for smart iris recognition (SIR). Firstly, the concept of SIR is introduced, including its definition, characteristics and performance target. Then the evolution process of iris acquisition and recognition algorithm is proposed respectively. With various strategies of human-machine interaction, iris acquisition systems are grouped into seven categories, i.e. Close-range IR, Active IR, IR at a distance, Active IR at a distance, Passive IR on move, Active IR on move, IR for Surveillance. Iris recognition algorithms advance to be more accurate, robust, efficient and secure. The achievements of state-of-the-art iris recognition methods especially the contributions of our research group are reviewed in the roadmap.},
}

@InCollection{Sun_HoBAS_2014,
  author    = {Sun, Zhenan and Tan, Tieniu},
  booktitle = {Handbook of Biometric Anti-Spoofing: Trusted Biometrics under Spoofing Attacks},
  publisher = {Springer London},
  title     = {Iris Anti-spoofing},
  year      = {2014},
  address   = {London},
  editor    = {Marcel, S{\'e}bastien and Nixon, Mark S. and Li, Stan Z.},
  isbn      = {978-1-4471-6524-8},
  pages     = {103--123},
  abstract  = {Iris images contain rich texture information for reliable personal identifi- cation. However, forged iris patterns may be used to spoof iris recognition systems. This paper proposes an iris anti-spoofing approach based on the texture discrimina- tion between genuine and fake iris images. Four texture analysis methods include gray level co-occurrence matrix, statistical distribution of iris texture primitives, local binary patterns (LBP) and weighted-LBP are used for iris liveness detection. And a fake iris image database is constructed for performance evaluation of iris liveness detection methods. Fake iris images are captured from artificial eyeballs, textured contact lens and iris patterns printed on a paper, or synthesised from textured contact lens patterns. Experimental results demonstrate the effectiveness of the proposed tex- ture analysis methods for iris liveness detection. And the learned statistical texture features based on weighted-LBP can achieve 99accuracy in classification of genuine and fake iris images.},
  doi       = {10.1007/978-1-4471-6524-8_6},
  url       = {http://dx.doi.org/10.1007/978-1-4471-6524-8_6},
}

@InProceedings{Wei_FRONTEX_2013,
  author    = {Hong Wei and Lulu Chen and James Ferryman},
  booktitle = {FRONTEX 2nd Global Conference on Future Developments of Automated Border Control},
  title     = {Biometrics in ABC: counter-spoofing research},
  year      = {2013},
  address   = {Warsaw, Poland},
  pages     = {1-4},
  publisher = {FRONTEX},
  abstract  = {Automated border control (ABC) is concerned with fast and secure processing for intelligence-led identification. The 
FastPass project aims to build a harmonised, modular reference system for future European ABC. When biometrics is taken on 
board as identity, spoofing attacks become a concern. This paper presents current research in algorithm development for 
counter-spoofing attacks in biometrics. Focussing on three biometric traits, face, fingerprint, and iris, it examines possible types 
of spoofing attacks, and reviews existing algorithms reported in relevant academic papers in the area of countering measures to 
biometric spoofing attacks. It indicates that the new developing trend is fusion of multiple biometrics against spoofing attacks.},
  doi       = {10.2819/20688},
  isbn      = {978-92-95033-76-4},
  keywords  = {biometrics, ABC, counter-spoofing mechanisms},
  url       = {http://centaur.reading.ac.uk/36547/},
}

@InProceedings{Zhang_IJCB_2011,
  author    = {Zhang, Hui and Sun, Zhenan and Tan, Tieniu and Wang, Jianyu},
  booktitle = C_IJCB,
  title     = {Learning hierarchical visual codebook for iris liveness detection},
  year      = {2011},
  pages     = {1-6},
  volume    = {1},
  abstract  = {Iris liveness detection is an important module in an iris recognition system to reduce the risks of being spoofed by fake iris patterns at the sensor input. A general frame- work is proposed to detect multiple types of fake iris images based on texture analysis. A novel iris pattern representa- tion method namely hierarchical visual codebook (HVC) is proposed to encode the distinctive and robust texture primi- tives of genuine and fake iris images. HVC takes advantages of both locality-constrained linear coding and vocabulary tree. Therefore, it can achieve less visual code quantization error, capture salient texture pattern sparsely, and reduce the dependence on coding at the upper level of vocabulary tree. To establish a benchmark for research of iris liveness detection, we develop a large fake iris image database in- cluding various fake iris images. Extensive experimental results demonstrate that the proposed method achieves 99% accuracy in fake iris detection.},
}

@InProceedings{Tan_2014_IWBF,
  author    = {C. W. Tan and A. Kumar},
  booktitle = W_BF,
  title     = {Integrating ocular and iris descriptors for fake iris image detection},
  year      = {2014},
  month     = {March},
  pages     = {1-4},
  abstract  = {Iris recognition has emerged as one of the most promising contactless biometrics technologies to provide automated human identification. Several national ID programs, such as Aadhar in India, incorporate iris biometrics to provide unique identity to millions of citizens. Therefore it is vital that integrity of such large scale iris deployments must also be safeguarded. Iris recognition technologies are increasingly becoming susceptible to sophisticated sensor level spoof attacks. This paper details the development of a new anti-spoofing approach which exploits the statistical grey-level dependencies in both the localized and global eye regions surrounding iris. We present experimental results on publicly available fake iris image database. The correct classification rate of 99.75% is obtained from the developed spoof iris detection approach using 1200 real and fake iris images and rom a publicly available database.},
  doi       = {10.1109/IWBF.2014.6914251},
  keywords  = {feature extraction;iris recognition;object detection;Aadhar program;India;anti-spoofing approach;contactless biometrics technologies;fake iris image database;fake iris image detection;iris biometrics;iris descriptors;iris recognition;iris recognition technology;ocular descriptors;sensor level spoof attacks;statistical grey-level dependencies;Databases;Feature extraction;Image edge detection;Image segmentation;Iris;Iris recognition;Biometrics;iris liveness detection;iris recognition;spoof iris detection},
}

@Article{Tomeo-Reyes_ISIS_2013,
  author    = {Inmaculada Tomeo-Reyes and Vinod Chandran},
  journal   = {International Journal of Information Science and Intelligent System},
  title     = {Iris based identity verification robust to sample presentation security attacks},
  year      = {2013},
  month     = {June},
  number    = {1},
  pages     = {27--41},
  volume    = {2},
  abstract  = {Iris based identity verification is highly reliable but it can also be subject to attacks. Pupil dilation or constriction stimulated by the application of drugs are examples of sample presentation security attacks which can lead to higher false rejection rates. Suspects on a watch list can potentially circumvent the iris based system using such methods. This paper investigates a new approach using multiple parts of the iris (instances) and multiple iris samples in a sequential decision fusion framework that can yield robust performance. Results are presented and compared with the standard full iris based approach for a number of iris degradations. An advantage of the proposed fusion scheme is that the trade-off between detection errors can be controlled by setting parameters such as the number of instances and the number of samples used in the system. The system can then be operated to match security threat levels. It is shown that for optimal values of these parameters, the fused system also has a lower total error rate.},
  publisher = {Martin Science Publishing},
  url       = {https://eprints.qut.edu.au/60842/},
}

@InCollection{Park_LNCS_2005,
  author    = {Park, Jong Hyun and Kang, Moon Gi},
  booktitle = {Advances in Biometric Person Authentication: International Wokshop on Biometric Recognition Systems, IWBRS 2005, Beijing, China, October 22-23, 2005. Proceedings},
  publisher = {Springer Berlin Heidelberg},
  title     = {Iris Recognition Against Counterfeit Attack Using Gradient Based Fusion of Multi-spectral Images},
  year      = {2005},
  address   = {Berlin, Heidelberg},
  editor    = {Li, Stan Z. and Sun, Zhenan and Tan, Tieniu and Pankanti, Sharath and Chollet, G{\'e}rard and Zhang, David},
  isbn      = {978-3-540-32248-1},
  pages     = {150--156},
  abstract  = {In this paper, we present an iris recognition system considering counterfeit attacks. The proposed system takes multi-spectral images instead of one infrared iris image. The energy of the multi-spectral images is checked and the authentication is failed if the amount of the energy is not in the proper range. Then the images are normalized and merged into a grayscale image by using a gradient-based image fusion algorithm. In the fusion process, the images considered to be from a counterfeited iris are merged into a poor-quality image which successively generates poor matching score. We show that the proposed scheme successfully maintains the performance of real iris images preventing the counterfeit attacks with experimental results.},
  doi       = {10.1007/11569947_19},
  url       = {http://dx.doi.org/10.1007/11569947_19},
}

@InProceedings{Bodade_UMT_2009,
  author    = {R. Bodade and S. Talbar},
  booktitle = C_UMT,
  title     = {Dynamic iris localisation: A novel approach suitable for fake iris detection},
  year      = {2009},
  month     = {Oct},
  pages     = {1-5},
  abstract  = {In iris recognition system, accurate iris segmentation and localisation from eye image is the foremost important step. Success rate of any feature extraction algorithm of iris recognition systems is primarily decides by the performance of iris segmentation from an eye image. In the proposed method, the outer boundary of iris is calculated by tracing objects of various shape and structure. For inner iris boundary, two eye images of same subject at different intensities are compared with each other to detect the variation in pupil size. The variation in pupil size is also used for aliveness detection of iris. Thus, this approach is a very promising technique in making iris recognition systems more robust against fake-iris-based spoofing attempts. The algorithm is tested on Phoenix database of 384 images both eyes of 64 subjects. The success rate of accurate iris localisation from eye image is 99.48% with minimal loss of iris texture features in spatial domain as compared to all existing techniques. The processing time required is also comparable with existing techniques.},
  doi       = {10.1109/ICUMT.2009.5345600},
  issn      = {2157-0221},
  keywords  = {biometrics (access control);feature extraction;image segmentation;Phoenix database;fake iris detection;feature extraction algorithm;iris localisation;iris segmentation;Biometrics;Educational institutions;Feature extraction;Image databases;Image segmentation;Iris recognition;Military communication;Robustness;Shape;Testing;dynamic iris localisation;fake iris detection;iris segmentation;pupil dynamics},
}

@InCollection{Toth_EB_2009,
  author    = {Toth, Bori},
  booktitle = {Encyclopedia of Biometrics},
  publisher = {Springer US},
  title     = {Liveness Detection: Iris},
  year      = {2009},
  address   = {Boston, MA},
  editor    = {Li, Stan Z. and Jain, Anil},
  isbn      = {978-0-387-73003-5},
  pages     = {931--938},
  doi       = {10.1007/978-0-387-73003-5_179},
  url       = {http://dx.doi.org/10.1007/978-0-387-73003-5_179},
}

@Article{Gragnaniell_PRL_2015,
  author   = {Diego Gragnaniello and Carlo Sansone and Luisa Verdoliva},
  journal  = J_PRL,
  title    = {Iris liveness detection for mobile devices based on local descriptors},
  year     = {2015},
  issn     = {0167-8655},
  note     = {Mobile Iris \{CHallenge\} Evaluation part I (MICHE I)},
  pages    = {81 - 87},
  volume   = {57},
  abstract = {Abstract Iris recognition is well suited to authentication on mobile devices, due to its intrinsic security and non-intrusiveness. However, authentication systems can be easily tricked by attacks based on high-quality printing. A liveness detection module is therefore necessary. Here, we propose a fast and accurate technique to detect printed-iris attacks based on the local binary pattern (LBP) descriptor. In order to improve the discrimination ability of \{LBP\} and better explore the image statistics, \{LBP\} is performed on a high-pass version of the image with 3 × 3 integer kernel. In addition a simplified interpolation-free descriptor is considered and finally a linear \{SVM\} classification scheme is used. The detection performance, measured on standard databases, is extremely promising, despite the resulting very low complexity, which makes possible the implementation for the relatively small \{CPU\} processing power of a mobile device.},
  doi      = {https://doi.org/10.1016/j.patrec.2014.10.018},
  keywords = {Iris liveness detection, Local descriptors, Biometric spoofing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865514003511},
}

@Article{Bodade_CA_2011,
  author   = {Rajesh Bodade and Sanjay Talbar},
  journal  = J_CA,
  title    = {Fake Iris Detection: A Holistic Approach},
  year     = {2011},
  month    = {April},
  note     = {Full text available},
  number   = {2},
  pages    = {1-7},
  volume   = {19},
  abstract = {There is tremendous interest in improved methods of reliable and secure identification of people using biometrics. Although, iris is believed to allow very high accuracy, various experiments showed an alarming lack of anti-spoofing mechanisms in devices already protecting many sensitive areas all over the world. This enforces the need for aliveness detection methodology to be quickly introduced. In this paper, all possible types of fake iris has been identified. Previously published work in this field had concentrated only either on active or passive methods for fake iris detection. This has visible shortcomings of detecting only specific types of fake irises corresponding to each method and not of all kinds. This paper proposes a composite method, with promising results, to overcome the shortcomings of existing methods. The FAR and FRR values of the proposed method on realistic database of 160 images are 0.625% and 0.625% respectively. A notable achievement of this work is development of robust iris segmentation algorithm having inherent capability of fake iris detection.},
}

@Article{Bodade_ITKM_2010,
  author   = {Rajesh Bodade and Sanjay Talbar},
  journal  = {International Journal of Information Technology and Knowledge Management},
  title    = {Novel approach of accurate iris localisation form high resolution eye images suitable for fake iris detection},
  year     = {2010},
  month    = {July-December},
  number   = {2},
  pages    = {685-690},
  volume   = {3},
  abstract = {High resolution images not only provide high recognition rate but also useful in safeguarding the iris recognition system from fake iris attack. To safeguard the iris recognition system against fake irises, one of the very popular technique is to detect the change in pupil size due to change in illumination. Many of existing methods assume that iris and pupil are circular or elliptical in nature , which is seldom true but they are actually of irregular shapes. Such methods fails in accurate iris segmentation from high resolution images. because these images shows low intensity gradient across the sclera-iris boundary and iris-pupil boundary. This paper presents a novel approach of accurate iris segmentation using two images captured at two different intensities. This method is completely robust for fake iris detection because it exploits the pupil dynamics for iris localisation. The success rate of accurate iris localisation from an high resolution image (UPOL database) is 99.45% and that from moderate resolution images (UBIRIS database) is 100%. Only occlusion-free images of UBIRIS database has been considered.},
}

@InProceedings{Akhtar_ICCST_2014,
  author    = {Zahid Akhtar and Christian Michelon and Gian Luca Foresti},
  booktitle = C_ICCST,
  title     = {Liveness detection for biometric authentication in mobile applications},
  year      = {2014},
  address   = {Rome, Italy},
  month     = {Oct},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {The vulnerability of biometric authentication systems to spoofing attacks is now a widely accepted fact. A spoofing attack occurs when an impostor attempts to masquerade as genuine user by falsifying biometric data and thereby gaining illegitimate access. Several liveness detection methods have been proposed, which consist in determining whether there is a live person in front of the biometric sensor or an artificial replica. But, the problem is still unresolved owing to high level difficulty in determining efficient features with low computational cost to detect the spoofing attacks. In addition, existing methods are not particularly targeted for liveness detection in mobile biometric applications, thus mainly inapplicable for portable devices. Hence, we present a multi-biometric approach, that can detect face, iris and fingerprint spoofing attacks in mobile applications, by employing a novel real-time feature description based on order permutations, named Locally Uniform Comparison Image Descriptor (LUCID). LUCID is computable in linear time with respect to number of pixels and does not require floating point computation, beside the fact that typical mobile devices perform poorly for floating point applications. Our approach is therefore exclusively simple, fast and efficient, making it thus highly suitable for mobile devices. Moreover, contrary to existing schemes, our method utilize the same lone image descriptor technique effectively for three biometric traits, i.e. face, iris and fingerprint, liveness detection. Additionally, our method uses only one image for liveness detection, which can also be used for recognition. Experiments on publicly available face, iris and fingerprint data sets with real spoofing attacks show promising results.},
  doi       = {10.1109/CCST.2014.6986982},
  issn      = {1071-6572},
  keywords  = {authorisation;face recognition;fingerprint identification;iris recognition;mobile computing;LUCID;artificial replica;biometric authentication system vulnerability;biometric data;biometric sensor;face data sets;face detection;fingerprint data sets;fingerprint spoofing attacks;illegitimate access;iris data sets;iris detection;liveness detection;locally uniform comparison image descriptor;multibiometric approach;Face;Feature extraction;Fingerprint recognition;Iris recognition;Mobile communication;Mobile handsets;Skin;Biometrics;Face Recognition;Fingerprint Recognition;Iris Recognition;Liveness Detection;Mobile Devices;Spoofing Attacks},
}

@InCollection{Toth_EB_2015,
  author    = {Toth, Anna Bori and Galbally, Javier},
  booktitle = {Encyclopedia of Biometrics},
  publisher = {Springer US},
  title     = {Anti-spoofing, Iris},
  year      = {2015},
  address   = {New York},
  editor    = {Li, Stan Z. and Jain, Anil},
  isbn      = {978-1-4899-7487-7},
  pages     = {87--97},
  url       = {http://www.springer.com/us/book/9781489974877},
}

  
  

@Article{Das_PRL_2016,
  author   = {Abhijit Das and Umapada Pal and Miguel Angel Ferrer and Michael Blumenstein},
  journal  = J_PRL,
  title    = {A framework for liveness detection for direct attacks in the visible spectrum for multimodal ocular biometrics},
  year     = {2016},
  issn     = {0167-8655},
  note     = {An insight on eye biometrics},
  pages    = {232 - 241},
  volume   = {82},
  abstract = {In this research a new framework for software-based liveness detection for direct attacks in multimodal ocular biometrics across the visible spectrum is proposed. The framework aims to develop a more realistic method for liveness detection compared to previous frameworks proposed in the literature. To fulfil the above highlighted aims in this framework, intra-class level (i.e. user level) liveness detection is introduced. To detect liveness, a new set of image quality-based features is proposed for multimodal ocular biometrics in the visible spectrum. A variety of transformed domain (focus related) aspect and contrast-related quality features are employed to design the framework. Furthermore a new database is developed for liveness detection of multimodal ocular biometrics, which has the prominent presence of multimodal ocular traits (both sclera and iris). Moreover this database is comprised of a larger variety of fake images; those were prepared by employing versatile forging techniques which can be exhibited by imposters. Therefore the proposed schema has dealt with versatile categories of spoofing methods, which were not considered previously in the literature. The database contains a set of 500 fake and 500 genuine eye images acquired from 50 different eyes. An appreciable liveness detection result is achieved in the experiments. Furthermore, the experimental results conclude that this new framework is more efficient and competitive when compared to previous liveness detection schemes.},
  doi      = {http://dx.doi.org/10.1016/j.patrec.2015.11.016},
  keywords = {Biometrics, Sclera, Liveness, Iris, Visible spectrum},
  url      = {http://www.sciencedirect.com/science/article/pii/S016786551500402X},
}

@InCollection{Galbally_Handbook_2016,
  author    = {Galbally, Javier and Savvides, Marios and Venugopalan, Shreyas and Ross, Arun A.},
  booktitle = {Handbook of Iris Recognition},
  publisher = {Springer London},
  title     = {Iris Image Reconstruction from Binary Templates},
  year      = {2016},
  address   = {London},
  editor    = {Bowyer, Kevin W. and Burge, Mark J.},
  isbn      = {978-1-4471-6784-6},
  pages     = {469--496},
  abstract  = {This chapter explores the possibility of recovering iris images from binary iris templates. It has been generally assumed that the binary iris code is irreversible, i.e., the original iris texture cannot be derived from it. Here, we discuss two distinct approaches to reconstruct the iris texture from the binary iris code. Next, we discuss a method to detect such synthesized iris textures. Finally, we discuss some of the advantages and risks of generating iris texture from iris codes in the context of data privacy and security.},
  doi       = {10.1007/978-1-4471-6784-6_20},
  url       = {http://dx.doi.org/10.1007/978-1-4471-6784-6_20},
}

@InProceedings{Raja_SIN_2016,
  author    = {Kiran B. Raja and Ramachandra Raghavendra and Christoph Busch},
  booktitle = C_SIN,
  title     = {Color Adaptive Quantized Patterns for Presentation Attack Detection in Ocular Biometric Systems},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {9--15},
  publisher = {ACM},
  series    = {SIN'16},
  abstract  = {The challenges of presentation attacks (spoofing attacks) at sensor level is increasing for biometric systems due to the evolving method of artefact presentation. The sophisticated attacks now employ high quality printed artefacts and electronic screens to present the biometric samples which make it difficult to separate the real presentations and artefact presentations. In this work, we propose a new scheme to detect the artefacts in both NIR and visible spectrum biometric sensors for ocular biometric systems using a new set of feature descriptor. The scheme employs adaptive and quantized texture patters obtained from local microfeatures and global spatial features for different color channels in an image. Further, the texture descriptors are used to learn a spectrally regressed discriminant classifier to classify the normal ocular images against the artefact ocular images. The proposed scheme is used to perform extensive experiments on 5 publicly available ocular datasets including 2 datasets acquired in NIR domain and two datasets acquired using smartphones along with a dataset acquired using high quality camera. The experiments conducted on all the datasets have consistently indicated the robust performance against attacks by showing a classification error of 0%.},
  acmid     = {2951959},
  doi       = {10.1145/2947626.2951959},
  isbn      = {978-1-4503-4764-8},
  location  = {Newark, NJ, USA},
  url       = {http://doi.acm.org/10.1145/2947626.2951959},
}

@InProceedings{Kumar_NCVPRIPG_2015,
  author    = {Mohit Kumar and Niladri Bihari Puhan},
  booktitle = C_NCVPRIPG,
  title     = {Iris liveness detection using texture segmentation},
  year      = {2015},
  address   = {Patna, India},
  month     = {Dec},
  pages     = {1-4},
  publisher = {IEEE},
  abstract  = {In order to chisel an iris recognition system and pass in the iris liveness test, an attacker can create semitransparent contact lens spoofing. Such a contact lens is transparent around the iris center and has fake iris texture of another person printed around the outer region. In this paper, such fake iris images are synthetically created which can obviate sleuthing even after using pupillary light reflex technique. Thereafter, new liveness detection is proposed to determine the perceptually invisible boundary between the fake and original iris textures. Response of Gaussian derivative filters with multiple scales and orientations at each pixel location is clustered using K-means to ascertain regions with different textures. To give robustness to the algorithm, it is iterated a certain number of times and a threshold mechanism is imposed to find the correct boundary. The proposed method is shown to achieve high liveness performance by generating a set of 600 fake iris images from the UPOL iris database.},
  doi       = {10.1109/NCVPRIPG.2015.7490042},
  keywords  = {image segmentation;image texture;iris recognition;Gaussian derivative filters;K-means;UPOL iris database;fake iris images;fake iris textures;iris center;iris liveness detection;iris liveness test;iris recognition system;pupillary light reflex technique;semitransparent contact lens spoofing;texture segmentation;threshold mechanism;Clustering algorithms;Feature extraction;Image resolution;Image segmentation;Iris;Iris recognition;Lenses;Iris recognition;contact lens;fake;liveness detection;segmentation;semi-transparent;texture},
}

@Article{Thavalengal_CEM_2016,
  author   = {Shejin Thavalengal and Peter Corcoran},
  journal  = J_CEM,
  title    = {User Authentication on Smartphones: Focusing on iris biometrics},
  year     = {2016},
  issn     = {2162-2248},
  month    = {April},
  number   = {2},
  pages    = {87-93},
  volume   = {5},
  abstract = {A Smartphone is defined as a "mobile phone that can be used as a small computer and that connects to the Internet" [23]. Even though the first smartphone-IBM Simon-was introduced more than two decades ago, the world witnessed an immense evolution of smartphones after the introduction of the first iPhone in 2007 [24]. The current smartphones we carry in our pocket are not only a computer, a phone, a database, an infinite jukebox, a camera, a locator, and all the information in the world on our fingertips but also a personal companion that is a part of our daily life [25]. It is speculated that the smartphone's role as a constant companion, helper, coach, and guardian has only just begun [26].},
  doi      = {10.1109/MCE.2016.2522018},
  keywords = {Internet;iris recognition;mobile computing;smart phones;IBM Simon;Internet;iPhone;iris biometrics;mobile phone;smartphones;user authentication;Authentication;Biometrics;Cameras;Consumer electronics;Iris recognition;Optical imaging;Optical sensors;Smart phones},
}

@Article{Gragnaniello_PRL_2016,
  author   = {Diego Gragnaniello and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  journal  = J_PRL,
  title    = {Using iris and sclera for detection and classification of contact lenses},
  year     = {2016},
  issn     = {0167-8655},
  note     = {An insight on eye biometrics},
  pages    = {251 - 257},
  volume   = {82},
  abstract = {Detecting the presence of contact lenses and their type helps increasing the reliability of iris-based authentication systems. We propose a machine-learning approach for this task, based on expressive local image descriptors. The image is first segmented to extract the iris and sclera regions, then scale-invariant local descriptors (SID) are computed densely on both areas, and summarized through the Bag-of-Features paradigm. Classification is based on a properly trained linear SVM. The major contributions of our proposal concern the segmentation algorithm, the use of information drawn from the sclera, and the use of non-rectified data to preserve local structures. A number of variants of the proposed method are investigated, working on different areas of the image, with alternative local descriptors, and with different encoding techniques. Eventually, results are compared with the state-of-the-art in the field. The experimental analysis, carried out on several publicly available datasets, shows that the proposed classification method based on a dense scale invariant descriptor outperforms all the reference techniques.},
  doi      = {http://dx.doi.org/10.1016/j.patrec.2015.10.009},
  keywords = {Iris liveness detection, Local descriptors, Iris segmentation, Contact lens detection, Dense descriptors, Bag-of-Features},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865515003517},
}

@InProceedings{Shaydyuk_ICCST_2016,
  author    = {Nazariy K. Shaydyuk and Timothy Cleland},
  booktitle = C_ICCST,
  title     = {Biometric identification via retina scanning with liveness detection using speckle contrast imaging},
  year      = {2016},
  address   = {Orlando, FL, USA},
  month     = {Oct},
  pages     = {1-5},
  publisher = {IEEE},
  abstract  = {Current biometric modalities include fingerprint, palm, voice, face, gate, iris and even DNA recognition. Another known biometric technique involves subject identification using retinal blood vasculature pattern matching. Regardless of the modality, however, there is an inherent requirement for liveness detection so as to make the acquisition system less susceptible to deception. One possible solution for retina scanning systems is verification of blood flow in the retinal vessels - the definite feature of live tissue. Laser speckle contrast imaging is a common method of blood flow detection and could be used to explicitly confirm liveness. The dynamics of the speckle pattern can be statistically quantified and interpreted as the regions with and without flow. Using a model of the retinal vasculature, this paper reviews speckle contrast imaging as it applies to liveness verification by means of blood flow detection in retina-based biometric systems.},
  doi       = {10.1109/CCST.2016.7815706},
  keywords  = {authorisation;blood vessels;retinal recognition;speckle;DNA recognition;biometric identification;biometric modalities;blood flow detection;blood llow;face recognition;fingerprint recognition;gate recognition;iris recognition;laser speckle contrast imaging;live tissue;liveness detection;palm recognition;retina scanning;retina-based biometric systems;retinal mood vasculature pattern matching;retinal vasculature;retinal vessels;speckle contrast imaging;subject identification;voice recognition;Biomedical imaging;Blood flow;Fingerprint recognition;Retina;Scattering;Speckle;Biometrics;Liveness detection;Retina;Retina Scanning;Speckle contrast imaging},
}

@InProceedings{Fathy_NRSC_2017,
  author    = {Waleed S-A. Fathy and Hanaa S. Ali and Imbaby I. Mahmoud},
  booktitle = C_NRSC,
  title     = {Statistical representation for iris anti-spoofing using wavelet-based feature extraction and selection algorithms},
  year      = {2017},
  address   = {Alexandria, Egypt},
  month     = {March},
  pages     = {221-229},
  publisher = {IEEE},
  abstract  = {The development of fake iris detection systems, which is one of the most important topics in the biometric field, is growing rapidly. In this paper, discriminative statistical features are used for differing between real and fake iris images. The multilevel 2-D wavelet decomposition is employed to obtain approximation and detail wavelet channels. For feature classification, Euclidean distance and suitable fusion rules are applied. Problems with numerous features require the use of feature selection. Thus, to reduce the computational cost and enhance the system performance, an effective feature selection algorithm is proposed. CASIA-Iris-Syn database, which consists of about 10000 synthesized images, is used. Results show that the variance measure is efficient for detecting deceived attacks with 100% classification accuracy. The kurtosis measure gives 90.4648 %, which is the lowest accuracy obtained. Other feature selection algorithms are applied for a comparison purpose. Results prove the high explanatory capability of the prediction method. The proposed feature selection/classifier ensemble not only achieves dimensionality reduction, but also carefully investigates the dependence between the statistical features, and does not neglect features with complementary information. A poor choice of features may lead to significant deterioration in system performance. The proposed system has the advantage of working with large size database, and thus ensures the generalization ability of the proposed algorithms. Results also show that working with original non-segmented images not only reduces the processing time, but also enhances the classification accuracy, noticeably.},
  doi       = {10.1109/NRSC.2017.7893480},
  keywords  = {feature extraction;feature selection;image classification;image representation;iris recognition;statistical analysis;CASIA-Iris-Syn database;Euclidean distance;biometric field;computational cost;discriminative statistical feature;fake iris detection system;feature classification;feature selection algorithm;iris antispoofing;iris images;kurtosis;multilevel 2-D wavelet decomposition;nonsegmented images;statistical representation;wavelet channel;wavelet-based feature extraction;Algorithm design and analysis;Classification algorithms;Databases;Euclidean distance;Feature extraction;Iris;Iris recognition;Discriminative Statistics;Euclidean Distance;Feature Selection;Iris Spoofing;Multilevel Wavelet Decomposition;Variance},
}

@Article{Czajka_SPIE_2007,
  author   = {Adam Czajka and Przemek Strzelczyk and Andrzej Pacut},
  journal  = {SPIE Newsroom},
  title    = {Making iris recognition more reliable and spoof resistant},
  year     = {2007},
  month    = {June},
  abstract = {By detecting whether or not an iris is part of a living person, its features can be used much more confidently in biometrics.},
  doi      = {10.1117/2.1200706.0614},
  url      = {http://spie.org/newsroom/0614-making-iris-recognition-more-reliable-and-spoof-resistant},
}

@InProceedings{Karunya_ACCS_2015,
  author    = {R. Karunya and S. Kumaresan},
  booktitle = C_ACCS,
  title     = {A study of liveness detection in fingerprint and iris recognition systems using image quality assessment},
  year      = {2015},
  address   = {Coimbatore, India},
  month     = {Jan},
  pages     = {1-5},
  publisher = {IEEE},
  abstract  = {Biometric recognition systems are vulnerable to diverse attacks that emerged as a challenge in adopting these systems in real life scenarios. Development of efficient security measures is a necessity to ensure the actual presence of a genuine or real trait in various biometric recognition systems. This study aims at Liveness Detection in Fingerprint and Iris Recognition systems using Image Quality Assessment. The potential of general image quality assessment as a protection method against different biometric attacks is explored here. The key idea of this approach is to present a software based multi-biometric and multi attack protection method that characterize the real traits. The proposed method extracts 25 image quality assessment features from a single input image to build an appropriate classifier, which classifies the test image as real or fake given the extracted set of features.},
  doi       = {10.1109/ICACCS.2015.7324134},
  keywords  = {authorisation;feature extraction;fingerprint identification;iris recognition;biometric recognition systems;diverse attacks;extracted feature set;fingerprint recognition systems;image quality assessment;iris recognition systems;liveness detection;multiattack protection method;security measures;software based multibiometric;Distortion measurement;Feature extraction;Fingerprint recognition;Image quality;Iris recognition;Software;Biometric Recognition;Classifier;Full reference;Image Quality Assessment;Liveness Detection;Multiattack;Multibiometric;No reference;Spoofing},
}

@InProceedings{Gragnaniello_SITIS_2016,
  author    = {Diego Gragnaniello and Carlo Sansone and Giovanni Poggi and Luisa Verdoliva},
  booktitle = C_SITIS,
  title     = {Biometric Spoofing Detection by a Domain-Aware Convolutional Neural Network},
  year      = {2016},
  address   = {Naples, Italy},
  month     = {Nov},
  pages     = {193-198},
  publisher = {IEEE},
  abstract  = {Biometric authentication systems are pervasive in modern society, but they are quite vulnerable to spoofing attacks. Research on spoofing (or liveness) detection is therefore very active. A number of methods have been proposed in the literature, sometimes with very promising results, but limited robustness with respect to the large variety of biometric traits, sensors, and attacks encountered in real-life. Recently, methods based on Convolutional Neural Networks (CNNs) are drawing great attention, given their success in many other image processing tasks. However, despite some promising results, they seem to suffer the same robustness problem, requiring heavy training to work properly. Here, we propose a new CNN architecture for biometric spoofing detection. Thanks to domain-specific knowledge, accounted for through a suitable loss function, a compact architecture is obtained, allowing reliable training also in the presence of small-size datasets. Experiments prove the proposal to provide state-of-art performance on several widespread datasets for face and iris liveness detection.},
  doi       = {10.1109/SITIS.2016.38},
  keywords  = {face recognition;feedforward neural nets;iris recognition;learning (artificial intelligence);neural net architecture;visual databases;CNN architecture;biometric authentication systems;biometric spoofing detection;domain-aware convolutional neural network training;domain-specific knowledge;face detection;iris liveness detection;loss function;Biomedical imaging;Computer architecture;Convolution;Face;Feature extraction;Neurons;Training;Biometric spoofing;convolutional neural networks;liveness detection},
}

@InProceedings{Pravallika_ICICT_2016,
  author    = {P. Pravallika and K. S. Prasad},
  booktitle = C_ICICT,
  title     = {SVM classification for fake biometric detection using image quality assessment: Application to iris, face and palm print},
  year      = {2016},
  month     = {Aug},
  pages     = {1-6},
  volume    = {1},
  abstract  = {The increasing interest in the evaluation of biometric systems security is an important issue to be considered. The different threats called direct or spoofing attacks where in these attacks, the intruder uses some type of synthetically produced artifact (e.g., gummy finger, printed iris image or face mask), or tries to mimic the behavior of the genuine user, to fraudulent access of the biometric system have motivated to new efficient protection measures. In this paper, we present a novel software-based fake biometric detection method that can be used in multiple biometric systems to detect different types of fraudulent access attempts. The use of image quality assessment for liveness detection is motivated by the assumption that: It is expected that a fake image captured in an attack attempt will have different quality than a real sample acquired in the normal operation scenario for which the sensor was designed. The proposed approach presents a very low degree of complexity, which makes it suitable for real-time applications, using general image quality features extracted from one image to differentiate between real and fake samples. This proposed work enhances the security of biometric recognitions, by using the liveness detection through image quality assessment and by fusion of multiple biometric traits. The SVM classifier is used for differentiating between the real and fake samples.},
  doi       = {10.1109/INVENTIVE.2016.7823189},
  keywords  = {authorisation;face recognition;feature extraction;fingerprint identification;image capture;image classification;iris recognition;palmprint recognition;support vector machines;SVM classification;SVM classifier;biometric recognition security;biometric system security;face mask;face recognition;fraudulent access;gummy finger;image quality assessment;image quality feature extraction;iris recognition;liveness detection;palm print recognition;printed iris image;software-based fake biometric detection;spoofing attacks;Distortion;Distortion measurement;Feature extraction;Image quality;Iris recognition;Measurement uncertainty;biometric system;fake;image quality assessment;liveness detection;real;spoofing attacks},
}

@InProceedings{Czajka_BTAS_2015,
  author    = {Czajka, Adam and Bowyer, Kevin W.},
  booktitle = {2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)},
  title     = {Statistical evaluation of up-to-three-attempt iris recognition},
  year      = {2015},
  pages     = {1-6},
  doi       = {10.1109/BTAS.2015.7358797},
}

@InProceedings{Takano_SCE_2009,
  author    = {Hironobu Takano and Kiyomi Nakamura},
  booktitle = C_SCE,
  title     = {Rotation independent iris recognition by the rotation spreading neural network},
  year      = {2009},
  address   = {Kyoto, Japan},
  month     = {May},
  pages     = {651-654},
  publisher = {IEEE},
  abstract  = {We proposed a iris recognition system using the rotation spreading neural network (R-SAN net). The R-SAN net correctly recognized the orientation of iris images using the iris pattern alone, not the positional arrangement of respective face parts. The orientation recognition performance of R-SAN net allows the accurate compensation of the orientation variation. For the characteristics of the iris pattern recognition, the equal error rate was 0.79%, which was investigated with iris images acquired from 19 subjects. On the other hand, the liveness detection method using a variation in the brightness of an iris pattern induced by a pupillary reflex was developed. The live and artificial irises were classified by a decision threshold of 3.7% brightness variation rate.},
  doi       = {10.1109/ISCE.2009.5156991},
  issn      = {0747-668X},
  keywords  = {biometrics (access control);image recognition;neural nets;orientation recognition performance;rotation independent iris recognition;rotation spreading neural network;Authentication;Biometrics;Brightness;Cameras;Image recognition;Iris recognition;Mobile handsets;Neural networks;Pattern recognition;Waveguide discontinuities},
}

@InProceedings{Adamiak_FedCSIS_2015,
  author    = {Krzysztof Adamiak and Dominik \.{Z}urek and Krzysztof \'{S}lot},
  booktitle = C_FedCSIS,
  title     = {Liveness detection in remote biometrics based on gaze direction estimation},
  year      = {2015},
  address   = {\L\'{o}d\'{z}, Poland},
  month     = {Sept},
  pages     = {225-230},
  publisher = {IEEE},
  abstract  = {The following paper presents a simple and fast liveness detection method based on gaze direction estimation under a challenge-response user authentication scenario. To estimate a line of sight, a procedure composed of several steps, including face and eye detection, derivation of gaze direction representation and subsequent classification, has been proposed. The proposed, novel gaze orientation descriptor is easy to compute and it provides sufficiently accurate estimates for the considered task. To assess a probability of genuine biometric trait presentation, recorded gaze direction responses induced by presentation of a randomly generated on-screen object, are matched against expected patterns.},
  doi       = {10.15439/2015F307},
  keywords  = {face recognition;gaze tracking;image classification;image representation;iris recognition;object detection;probability;challenge-response user authentication scenario;eye detection;face detection;fast liveness detection method;gaze direction estimation;gaze direction representation;gaze orientation descriptor;genuine biometric trait presentation;randomly generated on-screen object;recorded gaze direction responses;remote biometrics;Algorithm design and analysis;Approximation methods;Discrete Fourier transforms;Estimation;Face;Iris recognition},
}

@InProceedings{Villalbos-Castaldi_BF_2014,
  author    = {Fabiola M. Villalbos-Castaldi and Ernesto Suaste-G\'{o}mez},
  booktitle = W_BF,
  title     = {In the use of the spontaneous pupillary oscillations as a new biometric trait},
  year      = {2014},
  address   = {Valletta, Malta},
  month     = {March},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {It is presented a novel pupillary-based identification system, along with the early identity authentication results and analysis, based on the spatio-temporal features computed from the spontaneous pupillary oscillations. The authors demonstrate that this biometric trait has the capability to provide enough discriminative information to authenticate the identity of a subject. A new methodology to compute the spatio-temporal biometric template recordings of the pupil area changes, in a video-oculography sequence under constant luminance level, is also introduced in this paper. According to the authors' knowledge, there is no evidence that other attempts were made, addressing this methodology to distinguish individuals based on the spatiotemporal representations, computed from the normal dilation-contraction behavior of the pupil. Liveness will be detected by using the information obtained from the spontaneous pupillary oscillation mechanism. Preliminary experiments were conducted by using a particular own collected database, resulting in a (Equal Error Rate) in the order of 0.2338%.},
  doi       = {10.1109/IWBF.2014.6914259},
  keywords  = {iris recognition;message authentication;video signal processing;biometric trait;constant luminance level;identity authentication;normal dilation-contraction behavior;pupillary-based identification system;spatio-temporal biometric template;spontaneous pupillary oscillation;video-oculography sequence;Databases;Feature extraction;Image recognition;Iris recognition;Muscles;Oscillators;Video sequences;Spontaneous pupillary oscillations;pupillary hippus;spatio-temporal biometric template;video-based identification system},
}

@Article{Venugopalan_TIFS_2011,
  author   = {Shreyas Venugopalan and Marios Savvides},
  journal  = J_TIFS,
  title    = {How to Generate Spoofed Irises From an Iris Code Template},
  year     = {2011},
  issn     = {1556-6013},
  month    = {June},
  number   = {2},
  pages    = {385-395},
  volume   = {6},
  abstract = {Biometrics has gained a lot of attention over recent years as a way to identify individuals. Of all biometrics-based techniques, the iris-pattern-based systems have recently shown very high accuracies in verifying an individual's identity. The premise here is that iris patterns are unique across people. Only the iris bit code template specific to an individual need be stored for future identity verification. It is generally accepted that this iris bit code is unidentifiable data. However, in this work, we explore methods to generate alternate iris textures for a given person for the purpose of bypassing a system based on this iris bit code. We show that, if this spoof texture is presented to an iris recognition system, it will generate the same score response as that of the original iris texture. Hence, this approach can bypass filter-based feature extraction systems (such as Daugman style systems) without using the actual texture of the target iris that we want to spoof, by obtaining a hamming distance match score that falls within the authentic score range. This approach assumes we know the feature extraction mechanism of the iris matching scheme. We embed features within a person's natural iris texture to spoof another person's iris. A very convincing preliminary investigation into how one can get by any iris recognition system by synthesizing various levels of “natural” looking irises is presented here and we hope to use this knowledge to build countermeasures into the feature extraction scheme of the recognition module.},
  doi      = {10.1109/TIFS.2011.2108288},
  keywords = {Hamming codes;authorisation;computer crime;feature extraction;image coding;image texture;iris recognition;authentic score range;biometrics based technique;bypass filter based feature extraction system;hamming distance;identity verification;iris bit code template;iris code template;iris matching scheme;iris pattern based system;iris recognition system;iris texture;spoof texture;spoofed iris;Databases;Feature extraction;Gabor filters;Iris;Iris recognition;Manganese;Pixel;Access control;biometrics;computer vision;digital signal processing;forgery;iris recognition},
}

@InProceedings{Raghavendra_EUSIPCO_2014,
  author    = {Ramachandra Raghavendra and Christoph Busch},
  booktitle = C_EUSIPCO,
  title     = {Presentation attack detection algorithm for face and iris biometrics},
  year      = {2014},
  address   = {Lisbon, Portugal},
  month     = {Sept},
  pages     = {1387-1391},
  publisher = {IEEE},
  abstract  = {Biometric systems are vulnerable to the diverse attacks that emerged as a challenge to assure the reliability in adopting these systems in real-life scenario. In this work, we propose a novel solution to detect a presentation attack based on exploring both statistical and Cepstral features. The proposed Presentation Attack Detection (PAD) algorithm will extract the statistical features that can capture the micro-texture variation using Binarized Statistical Image Features (BSIF) and Cepstral features that can reflect the micro changes in frequency using 2D Cepstrum analysis. We then fuse these features to form a single feature vector before making a decision on whether a capture attempt is a normal presentation or an artefact presentation using linear Support Vector Machine (SVM). Extensive experiments carried out on a publicly available face and iris spoof database show the efficacy of the proposed PAD algorithm with an Average Classification Error Rate (ACER) = 10.21% on face and ACER = 0% on the iris biometrics.},
  issn      = {2219-5491},
  keywords  = {cepstral analysis;error statistics;face recognition;iris recognition;reliability;statistical analysis;support vector machines;2D cepstrum analysis;ACER;BSIF;PAD algorithm;SVM;artefact presentation;average classification error rate;binarized statistical image features;biometric systems;cepstral features;face biometrics;face spoof database;iris biometrics;iris spoof database;linear support vector machine;microtexture variation;normal presentation;presentation attack detection algorithm;reliability;single feature vector;statistical feature;Cameras;Cepstrum;Databases;Face;Feature extraction;Iris recognition;Attack detection;Biometrics;Face;Iris;Spoof},
}

@InProceedings{Bhogal_BF_2017,
  author    = {Amrit Pal Singh Bhogal and Dominik S\"{o}llinger and Pauline Trung and Andreas Uhl},
  booktitle = W_BF,
  title     = {Non-reference image quality assessment for biometric presentation attack detection},
  year      = {2017},
  address   = {Coventry, UK},
  month     = {April},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Non-reference image quality measures are used to distinguish real biometric data from data as used in presentation / sensor spoofing attacks. An experimental study shows that based on a set of 6 such measures, classification of real vs. fake iris, fingerprint, and face data is feasible with an accuracy of 90% on average. However, we have found that the best quality measure (combination) and classification setting highly depends on the target dataset. Thus, we are unable to provide any other recommendation than to optimise the choice of quality measure and classification setting for each specific application setting.},
  doi       = {10.1109/IWBF.2017.7935080},
  keywords  = {Computational modeling;Databases;Discrete cosine transforms;Distortion;Image quality;Iris recognition;Transform coding},
}

@InProceedings{Tiwari_PDGC_2016,
  author    = {S. Tiwari and S. Tripathi and K. V. Arya},
  booktitle = C_PDGC,
  title     = {Score level fusion of Iris and Fingerprint using wavelet features},
  year      = {2016},
  month     = {Dec},
  pages     = {456-461},
  abstract  = {Unimodal biometric systems have been serving the security demands of real world applications to a great level but these systems show vulnerabilities to certain aspects like noisy inputs, non-universality, intra-class variability and spoofing. To overcome these limitations multimodal biometric systems were developed which use more than one biometric trait for recognition. Iris and Fingerprint were considered as biometric modalities in this work because of their high compatibility in real world applications. A combination of 2-level Discrete Wavelet Transform (DWT) and Discrete Cosine Transform (DCT) are used to obtain features of iris. Similarly, a combination of 2-level DWT and Fast Fourier Transform (FFT) are used to obtain features of FingerprInternational Feature matching was performed using Euclidean distance algorithm. Fusion is done using linear summation of scores obtained from individual modalities. Verification and identification tests were conducted on proposed multimodal biometric systems of iris and fingerprInternational The proposed system has shown better performance than the existing system.},
  doi       = {10.1109/PDGC.2016.7913239},
  keywords  = {discrete cosine transforms;discrete wavelet transforms;fast Fourier transforms;feature extraction;fingerprint identification;image fusion;image matching;iris recognition;DCT;DWT;Euclidean distance algorithm;discrete cosine transform;discrete wavelet transform;fast Fourier transform;feature matching;fingerprint fusion;iris fusion;multimodal biometric systems;score level fusion;unimodal biometric systems;wavelet features;Decision support systems;Discrete cosine transforms;Discrete wavelet transforms;Fingerprint recognition;Iris recognition;Matrix decomposition;Biometrics;Fingerprint;Fusion;Iris;Multimodal;Verification;Z-score Normalization},
}

@InProceedings{Soares_ICCSP_2016,
  author    = {J. Soares and A. N. Gaikwad},
  booktitle = C_ICCSP,
  title     = {A self banking biometric machine with fake detection applied to fingerprint and iris along with GSM technology for OTP},
  year      = {2016},
  month     = {April},
  pages     = {0508-0512},
  abstract  = {The growing direct or spoofing fraudulent attacks of thieves has motivated us to focus our prime concern on the security over money transaction. The accuracy of biometrics in identification is increasing its usage extensively. The method proposed in this paper focuses on how the money transaction in an ATM machine will be secured by providing personal identification by analyzing biometrics like fingerprints and iris patterns which are known for their steadiness and diversity. Use of biometrics provides a paperless banking environment along with the smart ATM access. In this system the samples of the fingerprint and iris along with the registered mobile number of the customer needs to be collected and saved in the database by the banker if the customer is to access the ATM. The actual operation of the system begins when the customer is to access the ATM to make a money transaction. The fingerprint and iris samples will be captured and matched. The system will distinguish between the real legitimate trait and fake self manufactured synthetic or reconstructed samples by comparing it with the samples saved in the database during enrollment. After finding valid samples the system generates a 3 digit code which is received by the customer on his/her registered mobile number. This process is carried out using a GSM modem interfaced with the ARM7. The entered OTP will be checked, after the OTP is found valid the customer is allowed to make further transactions otherwise the account is blocked. The ATM terminal will also be secured from fire and thieves by including a thermistor and a tilt sensor in the system. The experiments were conducted in real time operation by first performing enrollment and then authentication for two individuals.},
  doi       = {10.1109/ICCSP.2016.7754189},
  keywords  = {bank data processing;fingerprint identification;iris recognition;microprocessor chips;ARM7 microprocessor;ATM machine;GSM technology;OTP;automated teller machine;customer needs;fake detection;fingerprint biometric;iris pattern biometric;money transaction;paperless banking environment;personal identification;self-banking biometric machine;Databases;Feature extraction;Fingerprint recognition;Iris recognition;Online banking;Security;Transforms;Biometrics;Fingerprint Matching;Image Quality;Iris Recognition},
}

@InProceedings{Thavalengal_IJCB_2014,
  author    = {Shejin Thavalengal and Ruxandra Vranceanu and Razvan G. Condorovici and Peter Corcoran},
  booktitle = C_IJCB,
  title     = {Iris pattern obfuscation in digital images},
  year      = {2014},
  address   = {Clearwater, FL, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {As the imaging systems in handheld devices continue to improve, both in terms of optical quality and the use of advanced computational imaging techniques, we are close to a point where high quality iris images can be obtained from personal images, which in turn can be used for spoofing attacks against iris recognition system. Thus an emerging challenge for next-generation personal imaging devices is to provide a means to obfuscate iris pattern in digital photographs and videos, but without destroying the photo-realistic qualities of the eye regions in a photograph or video. This will effectively reduce the chance of obtaining iris patterns easily, which can be later used for spoofing. In this paper we propose five different techniques for iris pattern obfuscation and perform some initial testing to evaluate which are more robust. In addition some representative samples are provided of the visible effects on the appearance of eye regions.},
  doi       = {10.1109/BTAS.2014.6996276},
  keywords  = {iris recognition;computational imaging technique;digital image;handheld device;iris pattern obfuscation;iris recognition system;next-generation personal imaging device;optical quality;photo-realistic quality;Cameras;Face;Image color analysis;Iris;Iris recognition;Standards},
}

@InProceedings{Singh_ICT_2011,
  author    = {Yogendra Narain Singh and Sanjay Kumar Singh},
  booktitle = C_ICT,
  title     = {Vitality detection from biometrics: State-of-the-art},
  year      = {2011},
  address   = {Mumbai, India},
  month     = {Dec},
  pages     = {106-111},
  publisher = {IEEE},
  abstract  = {Biometric authentication systems have become the basis of trust to human society from the beginning of last decade, but the security of the system can be breached by presenting a non-live or fake biometric sample which is the cloned of a legitimate user's identity. Different techniques have been proposed to address the problem of vitality detection from biometrics but these techniques are far away from the final solution. This paper proposes a classification of vitality detection techniques for fingerprint, face and iris biometrics for summarizing the current state-of-the-art and presents a critical review of them. We evaluate the potential of multimodal techniques of vitality detection from biometrics and analyze the performance of different vitality detection techniques on the datasets and the test conditions that have used. The use of physiological signals that have the inherited feature of vitality signs as a supplementary information with other conventional biometrics can offer a potential defense against spoofing attacks in the system.},
  doi       = {10.1109/WICT.2011.6141226},
  keywords  = {face recognition;fingerprint identification;iris recognition;biometric authentication systems;face biometrics;fake biometric sample;fingerprint biometrics;iris biometrics;multimodal techniques;nonlive biometric sample;physiological signals;spoofing attacks;vitality detection technique classification;vitality signs;Authentication;Face;Hardware;Image sequences;Iris recognition;Physiology;Biometrics;Fake Biometrics;Security;Spoofing Attacks;Vitality Detection},
}

@InProceedings{Ortiz-Lopez_ICCST_2011,
  author    = {Jaime Ortiz-Lopez and Javier Galbally and Julian Fierrez and Javier Ortega-Garcia},
  booktitle = C_ICCST,
  title     = {Predicting iris vulnerability to direct attacks based on quality related features},
  year      = {2011},
  address   = {Barcelona, Spain},
  month     = {Oct},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {A new vulnerability prediction scheme for direct attacks to iris recognition systems is presented. The objective of the novel technique, based on a 22 quality related parameterization, is to discriminate beforehand between real samples which are easy to spoof and those more resistant to this type of threat. The system is tested on a database comprising over 1,600 real and fake iris images proving to have a high discriminative power reaching an overall rate of 84% correctly classified real samples for the dataset considered. Furthermore, the detection method presented has the added advantage of needing just one iris image (the same used for verification) to decide its degree of robustness against spoofing attacks.},
  doi       = {10.1109/CCST.2011.6095949},
  issn      = {1071-6572},
  keywords  = {iris recognition;direct attacks;fake iris images;iris recognition systems;iris vulnerability prediction scheme;quality related features;real iris images;spoofing attacks;Databases;Feature extraction;Image segmentation;Iris;Iris recognition;Robustness;Security;Iris recognition;Quality assessment;Security;Vulnerability},
}

@InProceedings{Johnson_IFS_2010,
  author    = {Peter Johnson and Bin Tan and Stephanie Schuckers},
  booktitle = W_IFS,
  title     = {Multimodal fusion vulnerability to non-zero effort (spoof) imposters},
  year      = {2010},
  address   = {Seattle, WA, USA},
  month     = {Dec},
  pages     = {1-5},
  publisher = {IEEE},
  abstract  = {In biometric systems, the threat of “spoofing”, where an imposter will fake a biometric trait, has lead to the increased use of multimodal biometric systems. It is assumed that an imposter must spoof all modalities in the system to be accepted. This paper looks at the cases where some but not all modalities are spoofed. The contribution of this paper is to outline a method for assessment of multimodal systems and underlying fusion algorithms. The framework for this method is described and experiments are conducted on a multimodal database of face, iris, and fingerprint match scores.},
  doi       = {10.1109/WIFS.2010.5711469},
  issn      = {2157-4766},
  keywords  = {authorisation;biometrics (access control);pattern recognition;sensor fusion;biometric systems;multimodal database;multimodal fusion vulnerability;nonzero effort imposters;Face;Fingerprint recognition;Iris;Iris recognition;Measurement;Security},
}

@InProceedings{Gragnaniello_SITIS_2014,
  author    = {Diego Gragnaniello and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  booktitle = C_SITIS,
  title     = {Contact Lens Detection and Classification in Iris Images through Scale Invariant Descriptor},
  year      = {2014},
  address   = {Marrakech, Morocco},
  month     = {Nov},
  pages     = {560-565},
  publisher = {IEEE},
  abstract  = {We propose a new machine-learning technique for detecting the presence and type of contact lenses in iris images. Following the usual paradigm, we extract the regions of interest for classification, compute a feature vector based on local descriptors, and feed it to a properly trained SVM classifier. Major improvements w.r.t. Current state of the art concern the design of a more reliable segmentation procedure and the use of a recently proposed dense scale-invariant image descriptor. Experiments on publicly available datasets show the proposed method to outperform significantly all reference techniques.},
  doi       = {10.1109/SITIS.2014.35},
  keywords  = {contact lenses;feature extraction;image classification;iris recognition;learning (artificial intelligence);support vector machines;vectors;SVM classifier;contact lens classification;contact lens detection;feature vector;iris image;machine-learning technique;regions of interest extraction;scale invariant descriptor;Eyelids;Feature extraction;Image edge detection;Image segmentation;Iris;Iris recognition;Lenses;Contact lens classification;Iris biometrics;local descriptor},
}

@InProceedings{Baker_BTAS_2009,
  author    = {Sarah E. Baker and Amanda Hentz and Kevin W. Bowyer and Patrick J. Flynn},
  booktitle = C_BTAS,
  title     = {Contact lenses: Handle with care for iris recognition},
  year      = {2009},
  address   = {Washington, DC, USA},
  month     = {Sept},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {Many iris recognition systems operate under the assumption that non-cosmetic contact lenses will not affect match quality and the convenience using iris biometrics. We show results opposing this belief with a study of 51 contact lens wearing subjects and 64 non contact lens wearing subjects. Our experimental results show that contacts lens wearers are 14 times more likely to be falsely rejected by the IrisBEE iris recognition system at a Hamming distance threshold of 0.32 than non contact lens wearers. We further classify contact lens wearers into four categories according to the type of lens and its visibility in the iris image. The false reject rate varies with different types of contacts and the artifacts they produce on iris images. This is the first work that we are aware of to look at the effects of prescription contact lenses on iris biometrics.},
  doi       = {10.1109/BTAS.2009.5339050},
  keywords  = {biometrics (access control);image recognition;Hamming distance;IrisBEE iris recognition system;iris biometrics;noncosmetic contact lenses;prescription contact lenses;Biometrics;Hamming distance;Image segmentation;Inspection;Iris recognition;Lenses;Optical materials;Security;Stability;Waveguide discontinuities;Iris Biometrics;contact lenses;match distribution stability},
}

@InProceedings{Yambay_ISBA_2017,
  author    = {David Yambay and Brian Walczak and Stephanie Schuckers and Adam Czajka},
  booktitle = C_ISBA,
  title     = {LivDet-Iris 2015 - Iris Liveness Detection Competition 2015},
  year      = {2017},
  address   = {New Delhi, India},
  month     = {Feb},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Presentation attacks such as printed iris images or patterned contact lenses can be used to circumvent an iris recognition system. Different solutions have been proposed to counteract this vulnerability with Presentation Attack Detection (commonly called liveness detection) being used to detect the presence of an attack, yet independent evaluations and comparisons are rare. To fill this gap we have launched the first international iris liveness competition in 2013. This paper presents detailed results of its second edition, organized in 2015 (LivDet-Iris 2015). Four software-based approaches to Presentation Attack Detection were submitted. Results were tallied across three different iris datasets using a standardized testing protocol and large quantities of live and spoof iris images. The Federico Algorithm received the best results with a rate of rejected live samples of 1.68% and rate of accepted spoof samples of 5.48%. This shows that simple static attacks based on paper printouts and printed contact lenses are still challenging to be recognized purely by software-based approaches. Similar to the 2013 edition, printed iris images were easier to be differentiated from live images in comparison to patterned contact lenses.},
  doi       = {10.1109/ISBA.2017.7947701},
  keywords  = {Error analysis;Image color analysis;Iris;Iris recognition;Lenses;Training},
}

@Article{Daugman_WMIP_2003,
  author   = {John Daugman},
  journal  = J_WMIP,
  title    = {Demodulation By Complex-Valued Wavelets For Stochastic Pattern Recognition},
  year     = {2003},
  pages    = {1--17},
  volume   = {1},
  abstract = {Samples from stochastic signals having sufficient complexity need reveal only a little unexpected shared structure, in order to reject the hypothesis that they are indepen- dent. The mere failure of a test of statistical independence can thereby serve as a basis for recognizing stochastic patterns, provided they possess enough degrees-of-freedom, because all unrelated ones would pass such a test. This paper discusses exploitation of this statistical principle, combined with wavelet image coding methods to extract phase descriptions of incoherent patterns. Demodulation and coarse quantization of the phase information creates decision environments characterized by well-separated clusters, and this lends itself to rapid and reliable pattern recognition.},
}

@Article{Daugman_SPIE_2014,
  author   = {John Daugman},
  journal  = {SPIE Newsroom},
  title    = {600 million citizens of India are now enrolled with biometric ID},
  year     = {2014},
  month    = {May 7},
  abstract = {The most ambitious biometric deployment in history, to enroll the iris patterns and other identifying data of all 1.2 billion Indian citizens in three years, has now passed its halfway mark.},
  doi      = {10.1117/2.1201405.005449},
  url      = {http://spie.org/newsroom/5449-600-million-citizens-of-india-are-now-enrolled-with-biometric-id},
}

@InProceedings{Yambay_IJCB_2017,
  author    = {David Yambay and Benedict Becker and Naman Kohli and Daksha Yadav and Adam Czajka and Kevin W. Bowyer and Stephanie Schuckers and Richa Singh and Mayank Vatsa and Afzel Noore and Diego Gragnaniello and C. Sansone and L. Verdoliva and Lingxiao He and Yiwei Ru and Haiqing Li and Nianfeng Liu and Zhenan Sun and Tieniu Tan},
  booktitle = C_IJCB,
  title     = {{LivDet Iris 2017} -- Iris Liveness Detection Competition 2017},
  year      = {2017},
  address   = {Denver, CO, USA},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Presentation attacks such as using a contact lens with a printed pattern or printouts of an iris can be utilized to bypass a biometric security system. Solutions which have been proposed to counteract this vulnerability are called Presentation Attack Detection (also commonly called live- ness detection or anti-spoofing) and detect the presence of such attacks. The first international iris liveness competi- tion was launched in 2013 in order to assess the perfor- mance of currently available PAD algorithms with a second competition in 2015. This paper presents detailed results of its third edition, LivDet-Iris 2017. Three software-based approaches to Presentation Attack Detection were submit- ted. Four datasets of live and spoof images were tested with an additional cross-sensor test. New datasets and novel sit- uations of data have resulted in this competition being of a higher difficulty than previous competitions. Anonymous received the best results with a rate of rejected live samples of 3.36% and rate of accepted spoof samples of 14.71%. The results show that even with advances, print-out iris attacks as well as patterned contacts lenses are still difficult for software-based systems to detect. Printed iris images were easier to be differentiated from live images in compar- ison to patterned contact lenses as was also seen in previous competitions.},
}

@Article{Galbally_CVIU_2013,
  author   = {Javier Galbally and Arun Ross and Marta Gomez-Barrero and Julian Fierrez and Javier Ortega-Garcia},
  journal  = J_CVIU,
  title    = {Iris image reconstruction from binary templates: An efficient probabilistic approach based on genetic algorithms},
  year     = {2013},
  issn     = {1077-3142},
  number   = {10},
  pages    = {1512 - 1525},
  volume   = {117},
  abstract = {A binary iriscode is a very compact representation of an iris image. For a long time it was assumed that the iriscode did not contain enough information to allow for the reconstruction of the original iris. The present work proposes a novel probabilistic approach based on genetic algorithms to reconstruct iris images from binary templates and analyzes the similarity between the reconstructed synthetic iris image and the original one. The performance of the reconstruction technique is assessed by empirically estimating the probability of successfully matching the synthesized iris image against its true counterpart using a commercial matcher. The experimental results indicate that the reconstructed images look reasonably realistic. While a human expert may not be easily deceived by them, they can successfully deceive a commercial matcher. Furthermore, since the proposed methodology is able to synthesize multiple iris images from a single iriscode, it has other potential applications including privacy enhancement of iris-based systems.},
  doi      = {http://dx.doi.org/10.1016/j.cviu.2013.06.003},
  keywords = {Image reconstruction, Biometric systems, Iris recognition, Binary iriscodes, Security, Privacy},
  url      = {http://www.sciencedirect.com/science/article/pii/S1077314213001070},
}

@InProceedings{Pala_CVPR_2017,
  author    = {Federico Pala and Bir Bhanu},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {Iris Liveness Detection by Relative Distance Comparisons},
  year      = {2017},
  address   = {Honolulu, HI, USA},
  month     = {July},
  pages     = {664-671},
  publisher = {IEEE},
  abstract  = {The focus of this paper is on presentation attack de- tection for the iris biometrics, which measures the pattern within the colored concentric circle of the subjects’ eyes, to authenticate an individual to a generic user verification system. Unlike previous deep learning methods that use sin- gle convolutional neural network architectures, this paper develops a framework built upon triplet convolutional net- works that takes as input two real iris patches and a fake patch or two fake patches and a genuine patch. The aim is to increase the number of training samples and to gen- erate a representation that separates the real from the fake iris patches. The smaller architecture provides a way to do early stopping based on the liveness of single patches rather than the whole image. The matching is performed by com- puting the distance with respect to a reference set of real and fake examples. The proposed approach allows for real- time processing using a smaller network and provides equal or better than state-of-the-art performance on three bench- mark datasets of photo-based and contact lens presentation attacks.},
}

@Article{Holland_TIFS_2013,
  author   = {Corey D. Holland and Oleg V. Komogortsev},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {Complex Eye Movement Pattern Biometrics: The Effects of Environment and Stimulus},
  year     = {2013},
  issn     = {1556-6013},
  month    = {Dec},
  number   = {12},
  pages    = {2115-2126},
  volume   = {8},
  abstract = {This paper presents an objective evaluation of the effects of eye tracking specification and stimulus presentation on the biometric viability of complex eye movement patterns. Six spatial accuracy tiers (0.5°, 1.0°, 1.5°, 2.0°, 2.5°, 3.0°), six temporal resolution tiers (1000, 500, 250, 120, 75, 30 Hz), and five stimulus types (simple, complex, cognitive, textual, random) are evaluated to identify acceptable conditions under which to collect eye movement data. The results suggest the use of eye tracking equipment capable of at least 0.5° spatial accuracy and 250 Hz temporal resolution for biometric purposes, whereas stimulus had little effect on the biometric viability of eye movements.},
  doi      = {10.1109/TIFS.2013.2285884},
  keywords = {biometrics (access control);eye;object tracking;biometric purposes;biometric viability;complex eye movement pattern biometrics;eye movement data;eye movements;eye tracking equipment;eye tracking specification;objective evaluation;spatial accuracy tiers;stimulus presentation;stimulus types;temporal resolution tiers;Accuracy;Biometrics (access control);Eyes;Iris recognition;Pattern analysis;Visual systems;Biometrics;eye movements;pattern analysis;security and protection},
}

@Online{ETPAD_v2_URL,
  abstract     = {ETPAD v2 database is composed of 1200 eye movement recordings and 400 iris images collected from 200 unique individuals. The database consists of one dataset of iris images and three datasets of eye movements.},
  author       = {Oleg Komogortsev},
  lastaccessed = {August 8, 2017},
  title        = {{Eye Tracker Print-Attack Database (ETPAD) v2}},
  url          = {http://cs.txstate.edu/~ok11/etpad_v2.html},
  year         = {2014},
}

@Online{ETPAD_v1_URL,
  abstract     = {ETPAD v1 database is composed of 600 eye movement recordings and 200 iris images collected from 100 unique individuals. The database consists of one dataset of iris images and three datasets of eye movements.},
  author       = {Oleg Komogortsev},
  lastaccessed = {August 8, 2017},
  title        = {{Eye Tracker Print-Attack Database (ETPAD) v1}},
  url          = {http://cs.txstate.edu/~ok11/etpad_v1.html},
  year         = {2014},
}

@Online{IIITD_DBs_URL,
  abstract     = {IIIT-D CLI database comprises of 6570 iris images pertaining to 101 subjects. Both left and right iris images of each subject are captured and therefore, there are 202 iris classes. The lenses used in the database are soft lenses manufactured by either CIBA Vision or Bausch and Lomb. For textured lenses, four colors are used. To study the effect of acquisition device on contact lenses, iris images are captured using two iris sensors: (1) Cogent dual iris sensor (CIS 202) and (2) VistaFA2E single iris sensor. The database contains a minimum of three images for each iris class in each of the above mentioned lens categories for both the iris. To perform the experiments for lens detection, images pertaining to the first 50 subjects are used for training and the remaining 51 subjects are used for testing. Combined spoofing database (CSD) consists of images from multiple publicly available spoofing databases. It consists of IIIT-Delhi Contact Lens Iris (CLI) Database, IIITD Iris Spoofing (IIS) Database, and Multi-sensor Iris Database. For experimental purposes, independently download Synthetic Database and IIT Delhi Iris Database to include in CSD.},
  author       = {{Image Analysis and Biometrics Lab}},
  lastaccessed = {August 8, 2017},
  title        = {{IIITD Contact Lens Iris Database, Iris Combined Spoofing Database}},
  url          = {http://iab-rubric.org/resources.html},
  year         = {2016},
}

@Online{CAVE_DB_URL,
  abstract     = {We have created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. For each subject, there are 5 head poses and 21 gaze directions per head pose, giving our data set more images and fixed gaze targets than any other publicly available gaze data set at the time of its release. Our subjects were ethnically diverse and 21 of them wore glasses.},
  author       = {Brian A. Smith and Qi Yin and Steven K. Feiner and Shree K. Nayar},
  lastaccessed = {August 8, 2017},
  title        = {Columbia Gaze Data Set},
  url          = {http://www.cs.columbia.edu/CAVE/databases/columbia_gaze},
  year         = {2013},
}

@InProceedings{Smith_UIST_2013,
  author    = {Smith, Brian A. and Yin, Qi and Feiner, Steven K. and Nayar, Shree K.},
  booktitle = C_UIST,
  title     = {Gaze Locking: Passive Eye Contact Detection for Human-object Interaction},
  year      = {2013},
  address   = {New York, NY, USA},
  pages     = {271--280},
  publisher = {ACM},
  series    = {UIST '13},
  abstract  = {Eye contact plays a crucial role in our everyday social in- teractions. The ability of a device to reliably detect when a person is looking at it can lead to powerful human-object interfaces. Today, most gaze-based interactive systems rely on gaze tracking technology. Unfortunately, current gaze track- ing techniques require active infrared illumination, calibra- tion, or are sensitive to distance and pose. In this work, we propose a different solution—a passive, appearance-based approach for sensing eye contact in an image. By focusing on gaze locking rather than gaze tracking, we exploit the special appearance of direct eye gaze, achieving a Matthews correla- tion coefficient (MCC) of over 0.83 at long distances (up to 18 m) and large pose variations (up to ±30◦ of head yaw rota- tion) using a very basic classifier and without calibration. To train our detector, we also created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We demonstrate how our method facilitates human–object interaction, user analytics, image fil- tering, and gaze-triggered photography.},
  acmid     = {2501994},
  doi       = {10.1145/2501988.2501994},
  isbn      = {978-1-4503-2268-3},
  keywords  = {gaze-based interaction, gaze-triggered photography, human vision, human-object interaction, image filtering, passive eye contact detection, user analytics},
  location  = {St. Andrews, Scotland, United Kingdom},
  url       = {http://doi.acm.org/10.1145/2501988.2501994},
}

@Online{GUC_PAVID_DB_URL,
  abstract     = {Presentation Attack Video Iris Database (PAVID) consists of periocular videos obtained from 76 different subjects in the visible spectrum. The periocular videos are obtained using two different smartphone cameras (Nokia Lumia 1020 and iPhone 5S). The videos are obtained in the unconstrained conditions under the mixed illumination consisting of natural sunlight and artificial room light. Each unique periocular is captured twice (2 samples) in the database and hence a total of 152 videos from each smartphone. The database also consists of 608 artefact periocular videos.},
  author       = {{NISLab, NTNU}},
  lastaccessed = {August 8, 2017},
  title        = {{PAVID - Presentation Attack Video Iris Database}},
  url          = {http://www.nislab.no/biometrics_lab/pavid_db},
  year         = {2016},
}

@Article{Marsico_PRL_2015,
  author   = {Maria De Marsico and Michele Nappi and Daniel Riccio and Harry Wechsler},
  journal  = {Pattern Recognition Letters},
  title    = {{Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols}},
  year     = {2015},
  issn     = {0167-8655},
  note     = {{Mobile Iris CHallenge Evaluation part I (MICHE I)}},
  pages    = {17 - 23},
  volume   = {57},
  abstract = {We introduce and describe here MICHE-I, a new iris biometric dataset captured under uncontrolled settings using mobile devices. The key features of the MICHE-I dataset are a wide and diverse population of subjects, the use of different mobile devices for iris acquisition, realistic simulation of the acquisition process (including noise), several data capture sessions separated in time, and image annotation using metadata. The aim of MICHE-I dataset is to make up the starting core of a wider dataset that we plan to collect, with the further aim to address interoperability, both in the sense of matching samples acquired with different devices and of assessing the robustness of algorithms to the use of devices with different characteristics. We discuss throughout the merits of MICHE-I with regard to biometric dimensions of interest including uncontrolled settings, demographics, interoperability, and real-world applications. We also consider the potential for MICHE-I to assist with developing continuous authentication aimed to counter adversarial spoofing and impersonation, when the bar for uncontrolled settings raises even higher for proper and effective defensive measures.},
  doi      = {http://dx.doi.org/10.1016/j.patrec.2015.02.009},
  keywords = {Iris biometric, Iris challenge, Mobile devices},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167865515000574},
}


@Online{MICHE_I_DB_URL,
  abstract     = {MICHE_Dataset_BIPLab.zip contains iris images (and corresponding xml files for metedata). To obtain the password to unzip the database contact biplab@unisa.it.},
  author       = {{BIPLab -- Biometric and Image Processing Lab, University of Salerno}},
  lastaccessed = {Retrieved August 8, 2017},
  month        = {March},
  title        = {{Mobile Iris CHallenge Evaluation I (MICHE-I) Database}},
  url          = {http://biplab.unisa.it/MICHE/database/},
  year         = {2014},
}

@Online{GUC_LF_VIAr_DB_URL,
  abstract     = {GUC light field visible spectrum iris artifact database is composed in the context of the EU FP7 FIDELITY project. project. The database collection is carried out during the period of November 2012 till April 2014 at Gjøvik University College (GUC), Norway. The database is collected using commercially available light field camera froM Lytro to capture Normal (or real) iris samples and Canon 550D DSLR camera to capture corresponding high quality samples that are used to generate the artifact samples. The whole database is composed of 52 subjects with 104 unique eye patterns. Since normal (or real) samples are captured using both light field and canon DSLR camera, this database has 520 (= 104 X 5 samples/eye) high quality samples and 4327 light field samples. The artifacts are generated by simulating 5 different kinds of attacks that ensued in 7607 samples (artifact).},
  author       = {{NISLab, NTNU}},
  lastaccessed = {August 8, 2017},
  title        = {{GUC Light Field Visible Spectrum Iris Artefact Database (GUC-LF-VIAr-DB)}},
  url          = {http://www.nislab.no/biometrics_lab/guc_lf_viar_db},
  year         = {2016},
}

@Online{GUC_VISIA_DB_URL,
  abstract     = {This database is comprised of eye images captured from 55 subjects (29 males and 26 females) that will result in 110 unique eye patterns. For each subject, we capture 5 samples for each eye in five different instance that will result in 110 x 5 = 550 normal iris samples. In addition this database also comprised a high quality artefact of visible eye image that can be used to attack the visible iris recognition system. All the artefacts are generated by considering the real-life attack Scenarios that one can perform with visible iris recognition system. We generate the artefacts corresponding to 5 different kinds of static presentation attacks.},
  author       = {{NISLab, NTNU}},
  lastaccessed = {August 8, 2017},
  title        = {{GUC Visible Spectrum Iris Artefact (VSIA) Database}},
  url          = {http://www.nislab.no/biometrics_lab/vsia_db},
  year         = {2016},
}

@Article{Lefohn_CGA_2003,
  author   = {Aaron Lefohn and Brian Budge and Peter Shirley and Richard Caruso and Erik Reinhard},
  journal  = J_CGA,
  title    = {An ocularist's approach to human iris synthesis},
  year     = {2003},
  issn     = {0272-1716},
  month    = {Nov},
  number   = {6},
  pages    = {70-75},
  volume   = {23},
  abstract = {We have a particularly fortunate situation in iris synthesis: artificial eye makers (ocularists) have developed a procedure for physical iris synthesis that results in eyes with all the important appearance characteristics of real eyes. They have refined this procedure over decades,and the performance of their products in the real world completely validates the approach. Our approach lets users (other than trained ocularists) create a realistic looking human eye, paying particular attention to the iris. We draw from domain knowledge provided by ocularists to provide a toolkit that composes a human iris by layering semitransparent textures. These textures look decidedly painted and unrealistic. The composited result, however, provides a sense of depth to the iris and takes on a level of realism that we believe others have not previously achieved. Prior work on rendering eyes has concentrated predominantly on producing geometry for facial animation or for medical applications. Some work has focused on accurately modeling the cornea. In contrast, the goal of our work is the easy creation of realistic looking irises for both the ocular prosthetics and entertainment industries.},
  doi      = {10.1109/MCG.2003.1242384},
  keywords = {entertainment;eye;image texture;prosthetics;rendering (computer graphics);artificial eye makers;entertainment industries;human iris synthesis;ocular prosthetics;realistic looking human eye;realistic looking irises;rendering;semitransparent texture layering;Biomedical equipment;Cornea;Eyes;Facial animation;Geometry;Humans;Iris;Medical services;Refining;Waveguide discontinuities},
}

@Article{Zuo_TIFS_2007,
  author   = {Jinyu Zuo and Natalia A. Schmid and Xiaohan Chen},
  journal  = J_TIFS,
  title    = {On Generation and Analysis of Synthetic Iris Images},
  year     = {2007},
  issn     = {1556-6013},
  month    = {March},
  number   = {1},
  pages    = {77-90},
  volume   = {2},
  abstract = {The popularity of iris biometric has grown considerably over the past two to three years. It has resulted in the development of a large number of new iris encoding and processing algorithms. Since there are no publicly available large-scale and even medium-size data bases, neither of the newly designed algorithms has undergone extensive testing. The designers claim exclusively high recognition performance when the algorithms are tested on a small amount of data. In a large-scale setting, systems are yet to be tested. Since the issues of security and privacy slow down the speed of collecting and publishing iris data, an optional solution to the problem of algorithm testing is to synthetically generate a large scale data base of iris images. In this work, we describe a model-based method to generate iris images and evaluate the performance of synthetic irises by using a traditional Gabor filter-based iris recognition system. A comprehensive comparison of synthetic and real data is performed at three levels of processing: 1) image level, 2) texture level, and 3) decision level. A sensitivity analysis is performed to conclude on the importance of various parameters involved in generating iris images},
  doi      = {10.1109/TIFS.2006.890305},
  keywords = {Gabor filters;image coding;image recognition;image texture;Gabor filter-based iris recognition system;decision level;image level;iris encoding;iris processing algorithms;large-scale setting;model-based method;sensitivity analysis;synthetic iris images;texture level;Algorithm design and analysis;Biometrics;Data privacy;Data security;Encoding;Image analysis;Image generation;Iris;Large-scale systems;System testing;Biometrics;iris anatomy;iris-based authentication;performance extrapolation;texture analysis},
}

@InProceedings{Shah_ICIP_2006,
  author    = {Samir Shah and Arun Ross},
  booktitle = {2006 International Conference on Image Processing},
  title     = {Generating Synthetic Irises by Feature Agglomeration},
  year      = {2006},
  address   = {Atlanta, GA, USA},
  month     = {Oct},
  pages     = {317-320},
  publisher = {IEEE},
  abstract  = {We propose a technique to create digital renditions of iris images that can be used to evaluate the performance of iris recognition algorithms. The proposed scheme is implemented in two stages. In the first stage, a Markov random field model is used to generate a background texture representing the global iris appearance. In the next stage a variety of iris features, viz., radial and concentric furrows, collarette and crypts, are generated and embedded in the texture field. The iris images synthesized in this manner are observed to bear close resemblance to real irises. Experiments confirm the potential of this scheme to generate a database of synthetic irises that can be used to evaluate iris recognition algorithms.},
  doi       = {10.1109/ICIP.2006.313157},
  issn      = {1522-4880},
  keywords  = {Markov processes;biometrics (access control);eye;feature extraction;image recognition;image representation;image texture;Markov random field model;background texture representation;digital rendition;feature agglomeration;iris image synthesis;iris recognition algorithm;texture field embedding;Biometrics;Cornea;Image databases;Iris recognition;Markov random fields;Muscles;Pigmentation;Principal component analysis;Spatial databases;Waveguide discontinuities;Biometrics;Feature agglomeration;Markov Random Fields (MRF);Synthetic iris},
}

@Online{WVU_DB1_URL,
  abstract     = {Generation Process: The synthetic irises are generated in two stages. In the first stage, a Markov Random Field model is used to generate a background texture representing the global iris appearance. In the next stage, a variety of iris features, viz., radial and concentric furrows, collaret and crypts, are generated and embedded in the texture field. Dataset Size: 1000 classes with 7 samples per class.},
  author       = {{CITeR}},
  lastaccessed = {March 21, 2018},
  title        = {{Synthetic Iris Textured Based}},
  url          = {https://citer.clarkson.edu/biometric-dataset-collections/synthetic-iris-textured-based/},
  year         = {2006},
}

@Online{WVU_DB2_URL,
  abstract     = {The gallery of synthetic iris images are generated in five steps using a model based, anatomy based approach, with 40 controllable random parameters such as fiber size, pupil size, iris thickness, top layer thickness, fiber cluster degree, iris root blur range, the location of the collaret, the amplitude of the collaret, top layer transparency parameter, eye angle, eye size etc. The software coded with Matlab, generates 10000 classes (5000 subjects, left and right eye). Each class has 16 images, 1 good quality image, 15 degenerated images, with combination effects among noise, rotation, blur, motion blur, low contrast and specular reflection. For each image segmentation results ( unwrapped template, enhanced template and occlusion mask) are provided.},
  author       = {{CITeR}},
  lastaccessed = {March 21, 2018},
  title        = {{Synthetic Iris Model Based}},
  url          = {https://citer.clarkson.edu/biometric-dataset-collections/synthetic-iris-model-based/},
  year         = {2007},
}

@InProceedings{Wei_ICPR_2008,
  author    = {Zhuoshi Wei and Tieniu Tan and Zhenan Sun},
  booktitle = C_ICPR,
  title     = {Synthesis of large realistic iris databases using patch-based sampling},
  year      = {2008},
  address   = {Tampa, FL, USA},
  month     = {Dec},
  pages     = {1-4},
  publisher = {IEEE},
  abstract  = {This paper presents a framework to synthesize large realistic iris databases, providing an alternative to iris database collection. Firstly, iris patch is used as a basic element to characterize visual primitive of iris texture, and patch-based sampling is applied to create an iris prototype. Then a set of pseudo irises with intra-class variations are derived from the prototype. Qualitative and quantitative studies reveal that synthetic databases are well suited for evaluating iris recognition systems by achieving three goals: (1) the synthetic iris images bear a close resemblance to real iris images in terms of visual appearance; (2) the proposed framework is able to generate databases with large capacity; (3) statistical performance shows that the synthetic iris images hold all the major characteristics of real iris images.},
  doi       = {10.1109/ICPR.2008.4761674},
  issn      = {1051-4651},
  keywords  = {image recognition;image sampling;image texture;statistical analysis;visual databases;iris database collection;iris patch;iris recognition;iris texture;large realistic iris database;patch-based sampling;statistical performance;synthetic database;synthetic iris image;visual appearance;Biometrics;Image databases;Image generation;Image sampling;Iris recognition;Prototypes;Sampling methods;Sun;Visual databases;Waveguide discontinuities},
}

@Online{CASIA_IrisSynv4_URL,
  abstract     = {CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in {Wei_ICPR_2008}. Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in {Wei_ICPR_2008} that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.},
  author       = {{Chinese Academy of Sciences}},
  lastaccessed = {September 18, 2019},
  organization = {{Chinese Academy of Sciences}},
  title        = {{CASIA-Iris-Syn v4}},
  url          = {http://biometrics.idealtest.org/dbDetailForUser.do?id=4},
  year         = {2004},
}

@Online{GUC_VISSIV_DB_URL,
  abstract     = {GUC - Visible Spectrum Smartphone Iris Database consists of iris videos obtained from 31 different subjects in visible spectrum. The iris videos are obtained using two different smartphone cameras (Nokia Lumia 1020 and iPhone 5S). The videos are obtained in the unconstrained conditions under the mixed illumination consisting of natural sunlight and artificial room light. Each unique iris has 2 video samples in the database and hence a total of 248 iris videos. Each unique iris video is used to generate the presentation attack videos by replaying them on retina display enabled iPad. The total number of artefact (or spoof/presentation attack) videos in the database is 248.},
  author       = {{NISLab, NTNU}},
  lastaccessed = {August 10, 2017},
  title        = {{GUC - VIsible Spectrum Smartphone Iris Video (VISSIV) Database}},
  url          = {http://www.nislab.no/biometrics_lab/vissiv_db},
  year         = {2016},
}

@Online{WARSAW_DBs_URL,
  abstract     = {This page presents a small fraction of our laboratory databases that we are allowed to publish for non-commercial, research purposes: LivDet-Iris (2013, 2015, 2017), Warsaw-BioBase-Disease, Warsaw-BioBase-Smartphone-Iris, Warsaw-BioBase-Post-Mortem-Iris, Warsaw-BioBase-Pupil-Dynamics.},
  author       = {{Warsaw University of Technology}},
  lastaccessed = {June 23, 2021},
  title        = {{Warsaw Datasets Webpage}},
  url          = {http://zbum.ia.pw.edu.pl/EN/node/46},
  year         = {2013},
}

@Misc{ZBUM,
  howpublished = {http://zbum.ia.pw.edu.pl/EN},
  note         = {Accessed: July 8, 2021},
  title        = {{Biometrics and Machine Learning Group, Warsaw University of Technology, Poland}},
}

@InProceedings{Yadav_IJCB_2017,
  author    = {Daksha Yadav and Naman Kohli and Mayank Vatsa and Richa Singh and Afzel Noore},
  booktitle = C_IJCB,
  title     = {Unconstrained Visible Spectrum Iris with Textured Contact Lens Variations: Database and Benchmarking},
  year      = {2017},
  address   = {Denver, CO, USA},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Iris recognition in visible spectrum has developed into an active area of research. This has elevated the impor- tance of efficient presentation attack detection algorithms, particularly in security based critical applications. In this paper, we present the first detailed analysis of the effect of textured contact lenses on iris recognition in visible spec- trum. We introduce the first contact lens database in visible spectrum, Unconstrained Visible Contact Lens Iris (UVCLI) Database, containing samples from 70 classes with subjects wearing textured contact lenses in indoor and outdoor envi- ronments across multiple sessions. We observe that textured contact lenses degrade the visible spectrum iris recognition performance by over 25% and thus, may be utilized inten- tionally or unintentionally to attack existing iris recogni- tion systems. Next, three iris presentation attack detection (PAD) algorithms are evaluated on the proposed database and highest PAD accuracy of 82.85% is observed. This il- lustrates that there is a significant scope of improvement in developing efficient PAD algorithms for detection of textured contact lenses in unconstrained visible spectrum iris images.},
}

@InProceedings{Shahriar_COMPSAC_2017,
  author    = {H. Shahriar and H. Haddad and M. Islam},
  booktitle = C_COMPSAC,
  title     = {An Iris-Based Authentication Framework to Prevent Presentation Attacks},
  year      = {2017},
  month     = {July},
  pages     = {504-509},
  volume    = {2},
  abstract  = {Attacks on authentication services are major security concerns. Password-based authentication systems can be compromised using known techniques, such as brute force and dictionary-based attacks. Biometric-based authentication systems are becoming the preferred choice to replace password-based authentication systems. Among several variations of biometrics (e.g., face, eye, fingerprint), iris-based authentication is commonly used in various applications. In iris-based authentication systems, iris images from legitimate users are captured and certain features are extracted to be used for matching during the authentication process. Literature works suggest that iris-based authentication systems can be subject to presentation attacks where an attacker obtains printed copy of the victim's eye image and displays it in front of an authentication system to gain unauthorized access. Such attacks can be performed by displaying static eye images on mobile devices or ipads (known as screen attacks). Since human iris features so not changed, once the iris image is compromised, it is hard to avoid this type of attack. To address this challenge, this paper proposes a framework for iris code generation by considering the changes of the area between the pupil and the sclera due to light density level. The proposed approach relies on capturing iris images using near infrared light. We train HaarCascade and LBP classifiers to capture the area between the pupil and the cornea. The image of iris is then stored in the database. This approach also generates a QR code from the iris. The code acts as a password and the user is required to provide it during authentication. A prototype is built using OpenCV platform tool. The prototype has been tested using samples obtained from publicly available iris database. The initial results show that the proposed approach has lower false positive and false negative rates.},
  doi       = {10.1109/COMPSAC.2017.60},
  issn      = {0730-3157},
  keywords  = {QR codes;cryptographic protocols;eye;feature extraction;image matching;iris recognition;mobile computing;HaarCascade;LBP classifiers;OpenCV platform tool;authentication process matching;authentication services;biometric-based authentication systems;brute force attacks;cornea;dictionary-based attacks;feature extraction;human iris features;ipads;iris QR code;iris code generation;iris image;iris-based authentication framework;light density level;mobile devices;password-based authentication systems;presentation attacks;pupil;sclera;screen attacks;security concerns;static eye images;unauthorized access;Authentication;Cameras;Databases;Feature extraction;Fingerprint recognition;Iris recognition;Prototypes;HaarCascade classifier;Iris liveness detection;LBP classifier;Presentation attack},
}

@Online{CASIA_IrisV1_URL,
  abstract     = {CASIA Iris Image Database Version 1.0 (CASIA-IrisV1) includes 756 iris images from 108 eyes. For each eye, 7 images are captured in two sessions with our self-developed device CASIA close-up iris camera (Fig.1), where three samples are collected in the first session (Fig.2(a)) and four in the second session (Fig.2(b)). All images are stored as BMP format with resolution 320*280. In order to protect our IPR in the design of our iris camera (especially the NIR illumination scheme), the pupil regions of all iris images in CASIA-IrisV1 were automatically detected and replaced with a circular region of constant intensity to mask out the specular reflections from the NIR illuminators (see Fig.1). Such editing clearly makes iris boundary detection much easier but has minimal or no effects on other components of an iris recognition system, such as feature extraction and classifier design. It is suggested that you compare two samples from the same eye taken in different sessions when you want to compute the within-class variability. For example, the iris images in the first session can be employed as training dataset and those from the second session are used for testing.},
  author       = {{Chinese Academy of Sciences}},
  howpublished = {http://biometrics.idealtest.org/dbDetailForUser.do?id=1},
  lastaccessed = {December 7, 2017},
  title        = {{CASIA-IrisV1}},
  year         = {2003},
}

@InProceedings{Trokielewicz_BTAS_2016,
  author    = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  booktitle = C_BTAS,
  title     = {Human iris recognition in post-mortem subjects: Study and database},
  year      = {2016},
  address   = {Niagara Falls, NY, USA},
  month     = {Sept},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {This paper presents a unique study of post-mortem human iris recognition and the first known to us database of near-infrared and visible-light iris images of deceased humans collected up to almost 17 days after death. We used four different iris recognition methods to analyze the dynamics of iris quality decay in short-term comparisons (samples collected up to 60 hours after death) and long-term comparisons (for samples acquired up to 407 hours after demise). This study shows that post-mortem iris recognition is possible and occasionally works even 17 days after death. These conclusions contradict a promulgated rumor that iris is unusable shortly after decease. We make this dataset publicly available to let others verify our findings and to research new aspects of this important and unfamiliar topic. We are not aware of any earlier papers offering post-mortem human iris images and such comprehensive analysis employing four different matchers.},
  doi       = {10.1109/BTAS.2016.7791175},
  keywords  = {eye;iris recognition;deceased humans;iris quality decay;near-infrared iris images;post-mortem human iris recognition;time 17 day;time 407 h;time 60 h;visible-light iris images;Biomedical imaging;Cameras;Cornea;Databases;Iris recognition},
}

@InCollection{Puhan_DPPR_2011,
  author    = {Niladri Bihari Puhan and Sudha Natarajan and A. Suhas Hegde},
  booktitle = {Advances in Digital Image Processing and Information Technology},
  publisher = {Springer Berlin Heidelberg},
  title     = {Iris Liveness Detection for Semi-transparent Contact Lens Spoofing},
  year      = {2011},
  address   = {Berlin, Heidelberg},
  editor    = {Nagamalai, Dhinaharan and Renault, Eric and Dhanuskodi, Murugan},
  isbn      = {978-3-642-24055-3},
  pages     = {249--256},
  abstract  = {Park's proposed liveness detection technique in [5] detects the fake irises by checking the pupil size variation and textural feature change in local iris area. A new semi-transparent contact lens based spoofing method is designed to overcome the liveness technique. The spoofing method generates artificial texture image by replicating imposter's natural iris texture adjacent to pupil boundary. Simulation results show that by printing the artificial texture image on semi-transparent contact lens, an imposter can pass through the liveness detection to illegally make an entry despite being in negative-watch lists. A countermeasure algorithm is suggested for improving the liveness detection technique.},
  doi       = {10.1007/978-3-642-24055-3_26},
  url       = {https://doi.org/10.1007/978-3-642-24055-3_26},
}

@Article{Fathy_WPC_2017,
  author   = {Waleed S.-A. Fathy and Hanaa S. Ali},
  journal  = {Wireless Personal Communications},
  title    = {Entropy with Local Binary Patterns for Efficient Iris Liveness Detection},
  year     = {2017},
  issn     = {1572-834X},
  month    = {Dec},
  pages    = {1-14},
  abstract = {Iris anti-spoofing is one of the most important topics, in which the development is increasing rapidly. This paper introduces an efficient system for detecting iris attacks. The system avoids the segmentation and the normalization stages employed traditionally in fake detection systems. Wavelet packets (WPs) are used to decompose the original image into wavelet approximation and detail channels. Entropy values are extracted from the wavelet channels, and also from the local binary pattern (LBP) images of the channels. These features are used for discriminating between real and fake iris images. Support vector machines are used for the classification purpose. The aim is to contribute for improved classification accuracy with less computational complexity and reduced processing time. Entropy of the WP channels gives 99.9237{\%} classification accuracy, and the entropy of the LBP images yields 99.781{\%}, using ATVS-FIr-DB. Fusion of these features yields 100{\%} classification accuracy. Entropy of the wavelet channels is sufficient to obtain 100{\%} accuracy using CASIA-Iris-Syn database, without fusion. All images in both databases are used, without the need to discard images with unsuccessful segmentation. Segmented images from both databases are used for comparison. Results show that more discriminative features can be obtained using the proposed algorithm. System complexity and processing time are reduced noticeably, and the system is robust to different types of fakes.},
  day      = {04},
  doi      = {10.1007/s11277-017-5089-z},
  url      = {https://doi.org/10.1007/s11277-017-5089-z},
}

@Article{Sauerwein_JFO_2017,
  author   = {Sauerwein, Kelly and Saul, Tiffany B. and Steadman, Dawnie Wolfe and Boehnen, Chris B.},
  journal  = J_JFO,
  title    = {The Effect of Decomposition on the Efficacy of Biometrics for Positive Identification},
  year     = {2017},
  issn     = {1556-4029},
  number   = {6},
  pages    = {1599--1602},
  volume   = {62},
  abstract = {Biometrics, unique measurable physiological and behavioral characteristics, are used to identify individuals in a variety of scenarios, including forensic investigations. However, data on the longevity of these indicators are incomplete. This study demonstrated that iris and fingerprint biometric data can be obtained up to four days postmortem in warmer seasons and 50 + days in the winter. It has been generally believed, but never studied, that iris recognition is only obtainable within the first 24 hours after death. However, this study showed that they remain viable for longer (2–34 days) depending upon the environmental conditions. Temperature, precipitation, insects, and scavenger activity were the primary factors affecting the retention of biometrics in decomposing human remains. While this study is an initial step in determining the utility of physiological biometrics across postmortem time, biometric research has the potential to make important contributions to human identification and the law enforcement, military, and medicolegal communities.},
  doi      = {10.1111/1556-4029.13484},
  keywords = {forensic science, anthropology, biometrics, human decomposition, positive identification, fingerprints, iris scan, facial recognition},
  url      = {http://dx.doi.org/10.1111/1556-4029.13484},
}

@Misc{Matthew_PhD_thesis_2016,
  author  = {Peter William Matthew},
  title   = {Novel approaches to biometric security with an emphasis on liveness and coercion detection},
  year    = {January 2016},
  journal = {Ph.D. Dissertation, Department of Computing, Edge Hill University},
}

@InProceedings{Trokielewicz_ICB_2016,
  author    = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  booktitle = C_ICB,
  title     = {Post-mortem human iris recognition},
  year      = {2016},
  address   = {Halmstad, Sweden},
  month     = {June},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {This paper presents a unique analysis of post-mortem human iris recognition. Post-mortem human iris images were collected at the university mortuary in three sessions separated by approximately 11 hours, with the first session organized from 5 to 7 hours after demise. Analysis performed for four independent iris recognition methods shows that the common claim of the iris being useless for biometric identification soon after death is not entirely true. Since the pupil has a constant and neutral dilation after death (the so called “cadaveric position”), this makes the iris pattern perfectly visible from the standpoint of dilation. We found that more than 90% of irises are still correctly recognized when captured a few hours after death, and that serious iris deterioration begins approximately 22 hours later, since the recognition rate drops to a range of 13.3-73.3% (depending on the method used) when the cornea starts to be cloudy. There were only two failures to enroll (out of 104 images) observed for only a single method (out of four employed in this study). These findings show that the dynamics of post-mortem changes to the iris that are important for biometric identification are much more moderate than previously believed. To the best of our knowledge, this paper presents the first experimental study of how iris recognition works after death, and we hope that these preliminary findings will stimulate further research in this area.},
  doi       = {10.1109/ICB.2016.7550073},
  keywords  = {feature extraction;iris recognition;biometric identification;iris pattern;postmortem human iris recognition;Biomedical imaging;Cameras;Cornea;Degradation;Iris recognition;Pathology;Visualization},
}

@Misc{Daugman_patent_1994,
  author   = {John Daugman},
  month    = {July},
  title    = {Biometric personal identification system based on iris analysis, {U}nited {S}tates {P}atent, {US} 5,291,560},
  year     = {1994},
  abstract = {A system for rapid and automatic identification of persons, with very high reliability and confidence levels. The iris of the eye is used an optical fingerprint, having a highly detailed pattern that is unique for each individual and stable over many years. Image analysis algorithms find the iris in a live video image of a person's face, and encode its texture into a compact signature, or "iris code." Iris texture is extracted from the image at multiple scales of analysis by a self-similar set of quadrature (2-D Gabor) bandpass filters defined in a dimensionless polar coordinate system. The sign of the projection of many different parts of the iris onto these multi-scale quadrature filters, determines each bit in an abstract (256-byte) iris code. The degrees-of-freedom in this code are based on the principle forms of variation in a population of irises studied.},
  comment  = {iris:PAD},
  day      = {11},
}

@InBook{Trokielewicz_BuBC_2019,
  author    = {Trokielewicz, Mateusz and Czajka, Adam and Maciejewicz, Piotr},
  editor    = {Nait-Ali, Amine},
  pages     = {41--69},
  publisher = {Springer Singapore},
  title     = {Iris Recognition in Cases of Eye Pathology},
  year      = {2019},
  address   = {Singapore},
  isbn      = {978-981-13-1144-4},
  abstract  = {The purpose of this chapter is to provide an insight on iris recognition system performances for users suffering from some ocular pathologies. Such situations are analyzed in terms of influences and possible means are discussed to take this case into account during the phase of iris samples matching.},
  booktitle = {Biometrics under Biomedical Considerations},
  doi       = {10.1007/978-981-13-1144-4_2},
  url       = {https://doi.org/10.1007/978-981-13-1144-4_2},
}


@InBook{Czajka_Enc_2021,
  author    = {Czajka, Adam},
  editor    = {Jajodia, Sushil and Samarati, Pierangela and Yung, Moti},
  pages     = {1--3},
  publisher = {Springer Berlin Heidelberg},
  title     = {Iris Recognition},
  year      = {2021},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-27739-9},
  booktitle = {Encyclopedia of Cryptography, Security and Privacy},
  doi       = {10.1007/978-3-642-27739-9_1515-1},
  url       = {https://doi.org/10.1007/978-3-642-27739-9_1515-1},
}

@Article{Czajka_JTIT_2010,
  author  = {Adam Czajka and Andrzej Pacut},
  journal = J_JTIT,
  title   = {Iris Recognition System Based on Zak-Gabor Wavelet Packets},
  year    = {2010},
  pages   = {10-18},
  volume  = {4},
}

@Article{Czajka_JTIT_2012,
  author  = {Adam Czajka and Krzysztof Piech},
  journal = J_JTIT,
  title   = {Secure Biometric Verification Station Based on Iris Recognition},
  year    = {2012},
  pages   = {40-49},
  volume  = {3},
}


@Article{Trokielewicz_JTIT_2016,
  author   = {Mateusz Trokielewicz and Ewelina Bartuzi},
  journal  = J_JTIT,
  title    = {Cross-spectral Iris Recognition for Mobile Applications using High-quality Color Images},
  year     = {2016},
  pages    = {91-97},
  volume   = {3},
  abstract = {With the recent shift towards mobile computing, new challenges for biometric authentication appear on the horizon. This paper provides a comprehensive study of cross- spectral iris recognition in a scenario, in which high quality color images obtained with a mobile phone are used against enrollment images collected in typical, near-infrared setups. Grayscale conversion of the color images that employs selec- tive RGB channel choice depending on the iris coloration is shown to improve the recognition accuracy for some combi- nations of eye colors and matching software, when compared to using the red channel only, with equal error rates driven down to as low as 2%. The authors are not aware of any other paper focusing on cross-spectral iris recognition is a scenario with near-infrared enrollment using a professional iris recog- nition setup and then a mobile-based verification employing color images.},
}

@Online{UBIRIS_DB_URL,
  abstract     = {The UBIRIS database has two distinct versions: 1) UBIRIS.v1 -- This version of the database is composed of 1 877 images collected from 241 eyes during September, 2004 in two distinct sessions. It simulates less constrained imaging conditions. It is public and free available. In order to access the database, it must be sent an email to one of the authors asking for the password of the zip file. 2) UBIRIS.v2 -- The second version of the UBIRIS database has over 11 000 images (and continuously growing) and more realistic noise factors. Images were actually captured at-a-distance and on-the-move.},
  author       = {{SOCIA Lab. - Soft Computing and Image Analysis Group}},
  lastaccessed = {January 19, 2018},
  title        = {{Noisy Visible Wavelength Iris Image Databases (UBIRIS)}},
  url          = {http://iris.di.ubi.pt},
  year         = {2004},
}

@Online{NEXUS_URL,
  abstract     = {This is the official Government of Canada NEXUS application website. NEXUS is designed to speed up border crossings for low-risk, pre-approved travellers into Canada and the United States (U.S.). It is jointly run by the Canada Border Services Agency and U.S. Customs and Border Protection.},
  author       = {{Canada Border Services Agency and U.S. Customs and Border Protection}},
  lastaccessed = {January 19, 2018},
  title        = {{NEXUS}},
  url          = {https://www.cbsa-asfc.gc.ca/prog/nexus/menu-eng.html},
  year         = {2018},
}

@Article{Daugman_PAMI_1993,
  author   = {J. G. Daugman},
  journal  = J_TPAMI,
  title    = {High confidence visual recognition of persons by a test of statistical independence},
  year     = {1993},
  issn     = {0162-8828},
  month    = {November},
  number   = {11},
  pages    = {1148-1161},
  volume   = {15},
  abstract = {A method for rapid visual recognition of personal identity is described, based on the failure of a statistical test of independence. The most unique phenotypic feature visible in a person's face is the detailed texture of each eye's iris. The visible texture of a person's iris in a real-time video image is encoded into a compact sequence of multi-scale quadrature 2-D Gabor wavelet coefficients, whose most-significant bits comprise a 256-byte “iris code”. Statistical decision theory generates identification decisions from Exclusive-OR comparisons of complete iris codes at the rate of 4000 per second, including calculation of decision confidence levels. The distributions observed empirically in such comparisons imply a theoretical “cross-over” error rate of one in 131000 when a decision criterion is adopted that would equalize the false accept and false reject error rates. In the typical recognition case, given the mean observed degree of iris code agreement, the decision confidence levels correspond formally to a conditional false accept probability of one in about 1031},
  doi      = {10.1109/34.244676},
  keywords = {decision theory;face recognition;feature extraction;image coding;image texture;statistical analysis;decision confidence levels;exclusive-OR;face recognition;false accept error rates;false reject error rates;iris code;multiscale quadrature 2-D Gabor wavelet coefficients;personal identity recognition;phenotypic feature;probability;rapid visual recognition;statistical decision theory;statistical independence test;Biometrics;Decision theory;Error analysis;Face;Fingerprint recognition;Humans;Image texture analysis;Iris;Pattern recognition;Testing},
}

@Article{Burges_DMKD_1998,
  author   = {Burges, Christopher J.C.},
  journal  = {Data Mining and Knowledge Discovery},
  title    = {A Tutorial on Support Vector Machines for Pattern Recognition},
  year     = {1998},
  issn     = {1573-756X},
  month    = {Jun},
  number   = {2},
  pages    = {121--167},
  volume   = {2},
  abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
  day      = {01},
  doi      = {10.1023/A:1009715923555},
  url      = {https://doi.org/10.1023/A:1009715923555},
}

@InProceedings{Boser_CLT_1992,
  author    = {Bernhard E. Boser and Isabelle M. Guyon and Vladimir N. Vapnik},
  booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
  title     = {A Training Algorithm for Optimal Margin Classifiers},
  year      = {1992},
  address   = {New York, NY, USA},
  pages     = {144--152},
  publisher = {ACM},
  series    = {COLT'92},
  acmid     = {130401},
  doi       = {10.1145/130385.130401},
  isbn      = {0-89791-497-X},
  location  = {Pittsburgh, Pennsylvania, USA},
  url       = {http://doi.acm.org/10.1145/130385.130401},
}

@InProceedings{Ojala_ICPR_1994,
  author    = {Timo Ojala and Matti Pietik\"{a}inen and David Harwood},
  booktitle = {Proceedings of 12th International Conference on Pattern Recognition},
  title     = {Performance evaluation of texture measures with classification based on Kullback discrimination of distributions},
  year      = {1994},
  address   = {Jerusalem, Israel},
  month     = {Oct},
  pages     = {582-585 vol.1},
  publisher = {IEEE},
  volume    = {1},
  abstract  = {This paper evaluates the performance both of some texture measures which have been successfully used in various applications and of some new promising approaches. For classification a method based on Kullback discrimination of sample and prototype distributions is used. The classification results for single features with one-dimensional feature value distributions and for pairs of complementary features with two-dimensional distributions are presented},
  doi       = {10.1109/ICPR.1994.576366},
  keywords  = {image texture;Kullback discrimination;classification;complementary features;one-dimensional feature value distributions;performance evaluation;texture measures;Analysis of variance;Autocorrelation;Automation;Distributed computing;Electric variables measurement;Histograms;Image texture analysis;Performance evaluation;Prototypes;Rotation measurement},
}

@Article{Haralick_TSMC_1973,
  author   = {R. M. Haralick and K. Shanmugam and I. Dinstein},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics},
  title    = {Textural Features for Image Classification},
  year     = {1973},
  issn     = {0018-9472},
  month    = {Nov},
  number   = {6},
  pages    = {610-621},
  volume   = {SMC-3},
  abstract = {Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.},
  doi      = {10.1109/TSMC.1973.4309314},
  keywords = {Application software;Crops;Earth;Humans;Image classification;Image resolution;Piecewise linear techniques;Satellites;Spatial resolution;Testing},
}

@InProceedings{Kannala_ICPR_2012,
  author    = {Juho Kannala and Esa Rahtu},
  booktitle = {Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)},
  title     = {BSIF: Binarized statistical image features},
  year      = {2012},
  address   = {Tsukuba, Japan},
  month     = {Nov},
  pages     = {1363-1366},
  publisher = {IEEE},
  issn      = {1051-4651},
  keywords  = {binary codes;feature extraction;image coding;image recognition;image representation;image segmentation;image texture;independent component analysis;BSIF;binarized statistical image features;binary code string;coordinate binarization;heuristic code constructions;histogram based image region representation;independent component analysis;linearly local image patch projection;local image descriptor construction;local phase quantization;modeling capacity;natural image statistics;natural images;pixel binary codes;texture information encoding;texture recognition;thresholding;Accuracy;Binary codes;Face recognition;Image recognition;Probes;Quantization;Vectors},
}

@InProceedings{Ojansivu_ISP_2008,
  author    = {Ojansivu, Ville and Heikkil{\"a}, Janne},
  booktitle = {Image and Signal Processing},
  title     = {Blur Insensitive Texture Classification Using Local Phase Quantization},
  year      = {2008},
  address   = {Berlin, Heidelberg},
  editor    = {Elmoataz, Abderrahim and Lezoray, Olivier and Nouboud, Fathallah and Mammass, Driss},
  pages     = {236--243},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {In this paper, we propose a new descriptor for texture classification that is robust to image blurring. The descriptor utilizes phase information computed locally in a window for every image position. The phases of the four low-frequency coefficients are decorrelated and uniformly quantized in an eight-dimensional space. A histogram of the resulting code words is created and used as a feature in texture classification. Ideally, the low-frequency phase components are shown to be invariant to centrally symmetric blur. Although this ideal invariance is not completely achieved due to the finite window size, the method is still highly insensitive to blur. Because only phase information is used, the method is also invariant to uniform illumination changes. According to our experiments, the classification accuracy of blurred texture images is much higher with the new method than with the well-known LBP or Gabor filter bank methods. Interestingly, it is also slightly better for textures that are not blurred.},
  isbn      = {978-3-540-69905-7},
}

@InProceedings{Zhang_ICIP_2012,
  author    = {L. Zhang and Z. Zhou and H. Li},
  booktitle = C_ICIP,
  title     = {Binary Gabor pattern: An efficient and robust descriptor for texture classification},
  year      = {2012},
  month     = {Sept},
  pages     = {81-84},
  abstract  = {In this paper, we present a simple yet efficient and effective multi-resolution approach to gray-scale and rotation invariant texture classification. Given a texture image, we at first convolve it with J Gabor filters sharing the same parameters except the parameter of orientation. Then by binarizing the obtained responses, we can get J bits at each location. Then, each location can be assigned a unique integer, namely “rotation invariant binary Gabor pattern (BGPri)”, formed from J bits associated with it using some rule. The classification is based on the image's histogram of its BGPris at multiple scales. Using BGPri, there is no need for a pre-training step to learn a texton dictionary, as required in methods based on clustering such as MR8. Extensive experiments conducted on the CUReT database demonstrate the overall superiority of BGPri over the other state-of-the-art texture representation methods evaluated. The Matlab source codes are publicly available at http://sse.tongji.edu.cn/linzhang/IQA/BGP/BGP.htm.},
  doi       = {10.1109/ICIP.2012.6466800},
  issn      = {1522-4880},
  keywords  = {Gabor filters;image classification;image representation;image texture;statistical analysis;visual databases;CUReT database;MR8 clustering;gray-scale invariant texture classification;image histogram;multiresolution approach;orientation parameter;rotation invariant binary Gabor pattern;rotation invariant texture classification;texton dictionary;texture image;texture representation method;Accuracy;Dictionaries;Feature extraction;Gabor filters;Histograms;Joints;Training;Gabor filter;texture classification},
}

@Article{Tola_TPAMI_2010,
  author   = {E. Tola and V. Lepetit and P. Fua},
  journal  = J_TPAMI,
  title    = {DAISY: An Efficient Dense Descriptor Applied to Wide-Baseline Stereo},
  year     = {2010},
  issn     = {0162-8828},
  month    = {May},
  number   = {5},
  pages    = {815-830},
  volume   = {32},
  abstract = {In this paper, we introduce a local image descriptor, DAISY, which is very efficient to compute densely. We also present an EM-based algorithm to compute dense depth and occlusion maps from wide-baseline image pairs using this descriptor. This yields much better results in wide-baseline situations than the pixel and correlation-based algorithms that are commonly used in narrow-baseline stereo. Also, using a descriptor makes our algorithm robust against many photometric and geometric transformations. Our descriptor is inspired from earlier ones such as SIFT and GLOH but can be computed much faster for our purposes. Unlike SURF, which can also be computed efficiently at every pixel, it does not introduce artifacts that degrade the matching performance when used densely. It is important to note that our approach is the first algorithm that attempts to estimate dense depth maps from wide-baseline image pairs, and we show that it is a good one at that with many experiments for depth estimation accuracy, occlusion detection, and comparing it against other descriptors on laser-scanned ground truth scenes. We also tested our approach on a variety of indoor and outdoor scenes with different photometric and geometric transformations and our experiments support our claim to being robust against these.},
  doi      = {10.1109/TPAMI.2009.77},
  keywords = {computer vision;image matching;object detection;stereo image processing;DAISY;EM-based algorithm;computer vision;correlation-based algorithms;dense depth map estimation;efficient dense descriptor;geometric transformations;image matching;laser-scanned ground truth scenes;local image descriptor;narrow-baseline stereo;occlusion detection;occlusion maps;photometric transformations;wide-baseline image pairs;wide-baseline stereo;Image processing and computer vision;dense depth map estimation;local descriptors.;Algorithms;Artificial Intelligence;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Photogrammetry;Subtraction Technique},
}

@Article{Gragnaniello_PR_2015,
  author   = {Diego Gragnaniello and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  journal  = J_PR,
  title    = {Local contrast phase descriptor for fingerprint liveness detection},
  year     = {2015},
  issn     = {0031-3203},
  number   = {4},
  pages    = {1050 - 1058},
  volume   = {48},
  abstract = {We propose a new local descriptor for fingerprint liveness detection. The input image is analyzed both in the spatial and in the frequency domain, in order to extract information on the local amplitude contrast, and on the local behavior of the image, synthesized by considering the phase of some selected transform coefficients. These two pieces of information are used to generate a bi-dimensional contrast-phase histogram, used as feature vector associated with the image. After an appropriate feature selection, a trained linear-kernel SVM classifier makes the final live/fake decision. Experiments on the publicly available LivDet 2011 database, comprising datasets collected from various sensors, prove the proposed method to outperform the state-of-the-art liveness detection techniques.},
  doi      = {https://doi.org/10.1016/j.patcog.2014.05.021},
  keywords = {Fingerprint liveness detection, Local descriptors},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320314002210},
}

@InProceedings{Lowe_ICCV_1999,
  author    = {D. G. Lowe},
  booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
  title     = {Object recognition from local scale-invariant features},
  year      = {1999},
  pages     = {1150-1157 vol.2},
  volume    = {2},
  abstract  = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  doi       = {10.1109/ICCV.1999.790410},
  keywords  = {computational geometry;feature extraction;image matching;least squares approximations;object recognition;3D projection;blurred image gradients;candidate object matches;cluttered partially occluded images;computation time;inferior temporal cortex;local geometric deformations;local image features;local scale-invariant features;low residual least squares solution;multiple orientation planes;nearest neighbor indexing method;primate vision;robust object recognition;staged filtering approach;unknown model parameters;Computer science;Electrical capacitance tomography;Filters;Image recognition;Layout;Lighting;Neurons;Object recognition;Programmable logic arrays;Reactive power},
}

@InProceedings{Kokkinos_CVPR_2008,
  author    = {Iasonas Kokkinos and Alan Yuille},
  booktitle = C_CVPR,
  title     = {Scale invariance without scale selection},
  year      = {2008},
  address   = {Anchorage, AK, USA},
  month     = {June},
  pages     = {1-8},
  publisher = {IEEE},
  abstract  = {In this work we construct scale invariant descriptors (SIDs) without requiring the estimation of image scale; we thereby avoid scale selection which is often unreliable. Our starting point is a combination of log-polar sampling and spatially-varying smoothing that converts image scalings and rotations into translations. Scale invariance can then be guaranteed by estimating the Fourier transform modulus (FTM) of the formed signal as the FTM is translation invariant. We build our descriptors using phase, orientation and amplitude features that compactly capture the local image structure. Our results show that the constructed SIDs outperform state-of-the-art descriptors on standard datasets. A main advantage of SIDs is that they are applicable to a broader range of image structures, such as edges, for which scale selection is unreliable. We demonstrate this by combining SIDs with contour segments and show that the performance of a boundary-based model is systematically improved on an object detection task.},
  doi       = {10.1109/CVPR.2008.4587798},
  issn      = {1063-6919},
  keywords  = {Fourier transforms;image segmentation;object detection;sampling methods;Fourier transform modulus;boundary-based model;contour segment;image scaling;image translation;local image structure;log-polar sampling;object detection;scale invariant descriptor;spatially-varying smoothing;Band pass filters;Data mining;Fourier transforms;Image converters;Image edge detection;Image sampling;Image segmentation;Object detection;Smoothing methods;Statistics},
}

@Article{Chen_TPAMI_2010,
  author   = {J. Chen and S. Shan and C. He and G. Zhao and M. Pietikainen and X. Chen and W. Gao},
  journal  = J_TPAMI,
  title    = {WLD: A Robust Local Image Descriptor},
  year     = {2010},
  issn     = {0162-8828},
  month    = {Sept},
  number   = {9},
  pages    = {1705-1720},
  volume   = {32},
  abstract = {Inspired by Weber's Law, this paper proposes a simple, yet very powerful and robust local descriptor, called the Weber Local Descriptor (WLD). It is based on the fact that human perception of a pattern depends not only on the change of a stimulus (such as sound, lighting) but also on the original intensity of the stimulus. Specifically, WLD consists of two components: differential excitation and orientation. The differential excitation component is a function of the ratio between two terms: One is the relative intensity differences of a current pixel against its neighbors, the other is the intensity of the current pixel. The orientation component is the gradient orientation of the current pixel. For a given image, we use the two components to construct a concatenated WLD histogram. Experimental results on the Brodatz and KTH-TIPS2-a texture databases show that WLD impressively outperforms the other widely used descriptors (e.g., Gabor and SIFT). In addition, experimental results on human face detection also show a promising performance comparable to the best known results on the MIT+CMU frontal face test set, the AR face data set, and the CMU profile test set.},
  doi      = {10.1109/TPAMI.2009.155},
  keywords = {face recognition;image resolution;image texture;visual databases;visual perception;Brodatz texture database;KTH-TIPS2-a texture database;Weber local descriptor;Weber's Law;concatenated WLD histogram;differential excitation;differential orientation;gradient orientation;human face detection;human perception;image pixel;relative intensity differences;robust local Image Descriptor;stimulus;Pattern recognition;Weber law;face detection.;local descriptor;texture;Algorithms;Artificial Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Subtraction Technique},
}

@Article{Dinca_IEEEAccess_2017,
  author   = {L. M. Dinca and G. P. Hancke},
  journal  = J_ACC,
  title    = {The Fall of One, the Rise of Many: A Survey on Multi-Biometric Fusion Methods},
  year     = {2017},
  pages    = {6247-6289},
  volume   = {5},
  abstract = {Increasing operational and security demands changed biometrics by shifting the focus from single to multi-biometrics. Multi-biometrics are mandatory in the current context of large international biometric databases and to accommodate new emerging security demands. Our paper is a comprehensive survey on multi-biometrics, covering two important topics related to the multi-biometric field: fusion methods and security. Fusion is a core requirement in multi-biometric systems, being the method used to combine multiple biometric methods into a single system. The fusion section surveys recent multi-biometric schemes categorized from the perspective of fusion method. The security section is a comprehensive review of current issues, such as sensor spoofing, template security, and biometric encryption. New research trends and open challenges are discussed, such as soft, adaptive contextual-based biometrics. Finally, an implementation blueprint for a multi-biometric system is presented in the form of a list of questions to be answered when designing the system.},
  doi      = {10.1109/ACCESS.2017.2694050},
  keywords = {biometrics (access control);cryptography;sensor fusion;adaptive contextual-based biometrics;biometric databases;biometric encryption;fusion methods;fusion section surveys;multibiometric field;multibiometric fusion methods;multibiometric systems;sensor spoofing;template security;Feature extraction;Iris recognition;Measurement;Security;Upper bound;Biometric sensor;biometric cryptosystems;biometric key derivation;multi-biometric fusion;multi-biometrics;template protection},
}

@Book{Ross_Handbook_2006,
  author    = {Arun Ross and Karthik Nandakumar and Anil Jain},
  publisher = {Springer US},
  title     = {Handbook of Multibiometrics},
  year      = {2006},
  address   = {United States},
  abstract  = {Reliable human authentication schemes are of paramount importance in our highly networked society.  Advances in the field of biometrics help address the myriad of problems associated with traditional human recognition methods. The performance and benefits of a biometric system can be significantly enhanced by consolidating the evidence presented by multiple biometric sources. Multibiometric systems are expected to meet the stringent performance requirements imposed by large-scale authentication systems.
Handbook of Multibiometrics, a professional book, introduces multibiometric systems, and demonstrates the noteworthy advantages of these systems over their unimodal counterparts. In addition, this book describes in detail the various scenarios that are possible when fusing biometric evidence from multiple information sources.
This comprehensive volume on multibiometric systems concisely and clearly outlines the different fusion methodologies that have been proposed by researchers to integrate multiple biometric traits. Handbook of Multibiometrics is designed for a professional audience, composed of researchers and practitioners in the industry. This book is also suitable for graduate-level students as a secondary text in computer science within advanced biometrics.},
  doi       = {10.1007/0-387-33123-9},
  url       = {http://www.springer.com/us/book/9780387222967},
}

@Misc{ISO_30107-2_2017,
  author  = {{ISO/IEC 30107-2:2017}},
  title   = {{Information technology -- Biometric presentation attack detection -- Part 2: Data formats}},
  year    = {2017},
  comment = {standards:PAD:ISO},
}

@Online{ISO_IEC_30107_2_2016_URL,
  abstract     = {In accordance with ISO/IEC JTC 1 and the ISO and IEC Councils these International Standards are publicly available.
ISO Copyright for the freely available standards. The following standards are made freely available for standardization purposes. They are protected by copyright and therefore and unless otherwise specified, no part of these publications may be reproduced or utilized in any form or by any means, electronic or mechanical, including photocopying, microfilm, scanning, reproduction in whole or in part to another Internet site, without permission in writing from ISO. Requests should be addressed to the ISO Central Secretariat. The documents you are about to download are a single-user, non-revisable Adobe Acrobat PDF file, to store on your personal computer. You may print out and retain one printed copy of the PDF file. This printed copy is fully protected by national and international copyright laws, and may not be photocopied or reproduced in any form. Under no circumstances may it be resold.},
  author       = {{ISO/IEC}},
  lastaccessed = {February 7, 2018},
  title        = {{Information Technology Task Force (ITTF)}},
  url          = {http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html},
  year         = {2016},
}

@Misc{ISO_30107-4_2018,
  author  = {{ISO/IEC NP 30107-4}},
  title   = {{Information technology -- Biometric presentation attack detection -- Part 4: Profile for evaluation of mobile devices}},
  year    = {2018},
  comment = {standards:PAD:ISO},
}

@Article{Rathgeb_EURASIP_2011,
  author   = {Rathgeb, Christian and Uhl, Andreas},
  journal  = {EURASIP Journal on Information Security},
  title    = {A survey on biometric cryptosystems and cancelable biometrics},
  year     = {2011},
  issn     = {1687-417X},
  month    = {Sep},
  number   = {1},
  pages    = {3},
  volume   = {2011},
  abstract = {Form a privacy perspective most concerns against the common use of biometrics arise from the storage and misuse of biometric data. Biometric cryptosystems and cancelable biometrics represent emerging technologies of biometric template protection addressing these concerns and improving public confidence and acceptance of biometrics. In addition, biometric cryptosystems provide mechanisms for biometric-dependent key-release. In the last years a significant amount of approaches to both technologies have been published. A comprehensive survey of biometric cryptosystems and cancelable biometrics is presented. State-of-the-art approaches are reviewed based on which an in-depth discussion and an outlook to future prospects are given.},
  day      = {23},
  doi      = {10.1186/1687-417X-2011-3},
  url      = {https://doi.org/10.1186/1687-417X-2011-3},
}

@Article{Patel_SPM_2015,
  author   = {Vishal M. Patel and Nalini K. Ratha and Rama Chellappa},
  journal  = J_SPM,
  title    = {Cancelable Biometrics: A review},
  year     = {2015},
  issn     = {1053-5888},
  month    = {Sept},
  number   = {5},
  pages    = {54-65},
  volume   = {32},
  abstract = {Recent years have seen an exponential growth in the use of various biometric technologies for trusted automatic recognition of humans. With the rapid adaptation of biometric systems, there is a growing concern that biometric technologies may compromise the privacy and anonymity of individuals. Unlike credit cards and passwords, which can be revoked and reissued when compromised, biometrics are permanently associated with a user and cannot be replaced. To prevent the theft of biometric patterns, it is desirable to modify them through revocable and noninvertible transformations to produce cancelable biometric templates. In this article, we provide an overview of various cancelable biometric schemes for biometric template protection. We discuss the merits and drawbacks of available cancelable biometric systems and identify promising avenues of research in this rapidly evolving field.},
  doi      = {10.1109/MSP.2015.2434151},
  keywords = {biometrics (access control);biometric pattern prevention;biometric template protection;cancelable biometric template production;trusted automatic human recognition;Authentication;Biometrics (access control);Cryptography;Face recognition;Iris recognition;Trust management},
}

@Misc{Phillips_ArXiv_2020,
  author        = {P. Jonathon Phillips and Mark Przybocki},
  title         = {Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2002.01014},
  primaryclass  = {cs.CV},
}

@Misc{Wong_ArXiv_2021,
  author        = {Alexander Wong and Xiao Yu Wang and Andrew Hryniowski},
  title         = {How Much Can We Really Trust You? Towards Simple, Interpretable Trust Quantification Metrics for Deep Neural Networks},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2009.05835},
  primaryclass  = {cs.LG},
}

@Misc{Hryniowski_ArXiv_2020,
  author        = {Andrew Hryniowski and Xiao Yu Wang and Alexander Wong},
  title         = {Where Does Trust Break Down? A Quantitative Trust Analysis of Deep Neural Networks via Trust Matrix and Conditional Trust Densities},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2009.14701},
  primaryclass  = {cs.LG},
}

{Hryniowski_ArXiv_2020,
  author        = {Heinz Hofbauer and Fernando Alonso-Fernandez and Peter Wild and Josef Bigun and Andreas Uhl},
  title         = {Evaluation of the IRISSEG datasets},
  url		= {https://www.wavelab.at/sources/Hofbauer14b/}
  year          = {2020},
}

@Online{FacialBan,
  title = {{Ban Facial Recognition Map}},
  url   = {https://www.banfacialrecognition.com/map/},
  year  = {2021},
}

@Online{LinuxKernel,
  author = {Monica Chin},
  title  = {{How a university got itself banned from the Linux kernel}},
  url    = {https://www.theverge.com/2021/4/30/22410164/linux-kernel-university-of-minnesota-banned-open-source},
  year   = {2021},
}


@Article{Jain_EURASIP_2008,
  author     = {Jain, Anil K. and Nandakumar, Karthik and Nagar, Abhishek},
  journal    = {EURASIP Journal on Advances in Signal Processing},
  title      = {Biometric Template Security},
  year       = {2008},
  issn       = {1110-8657},
  month      = jan,
  pages      = {113:1--113:17},
  volume     = {2008},
  abstract   = {Biometric recognition offers a reliable and natural solution to the problem of user authentication in identity management systems. With the widespread deployment of biometric systems in various applications, there are increasing concerns about the security and privacy of biometric technology. Public confidence and acceptance of the biometrics technology will depend on the ability of system designers to demonstrate that these systems are robust, have low error rates and are tam- per proof. We present a high-level categorization of the various vulnerabilities of a biometric system and discuss countermeasures that have been proposed to address these vulnerabilities. In particular, we focus on biometric template security which is an important issue because unlike passwords and tokens, compromised biometric tem- plates cannot be revoked and reissued. Due to intra-user variability in the acquired biometric traits, ensuring the security of the template while maintaining the recognition performance is a challenging task. We present an overview of various biometric template protection schemes and dis- cuss their advantages and limitations in terms of security, revocability and impact on matching accuracy. A template protection scheme with provable security and acceptable recognition performance has thus far remained elusive. Development of such a scheme is crucial as biometric systems are beginning to proliferate into the core physical and information infrastructure of our society},
  acmid      = {1387883},
  address    = {New York, NY, United States},
  articleno  = {113},
  doi        = {10.1155/2008/579416},
  issue_date = {January 2008},
  numpages   = {17},
  publisher  = {Hindawi Publishing Corp.},
  url        = {http://dx.doi.org/10.1155/2008/579416},
}

@Article{Lecun_ProcIEEE_1998,
  author   = {Yann LeCun and Leon Bottou and Yoshua Bengio and Patrick Haffner},
  journal  = J_PIEEE,
  title    = {Gradient-based learning applied to document recognition},
  year     = {1998},
  issn     = {0018-9219},
  month    = {Nov},
  number   = {11},
  pages    = {2278-2324},
  volume   = {86},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  doi      = {10.1109/5.726791},
  keywords = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis},
}

@Article{Al-Raisi_TI_2008,
  author   = {Ahmad N. Al-Raisi and Ali M. Al-Khouri},
  journal  = {Telematics and Informatics},
  title    = {Iris recognition and the challenge of homeland and border control security in UAE},
  year     = {2008},
  issn     = {0736-5853},
  number   = {2},
  pages    = {117 - 132},
  volume   = {25},
  abstract = {This article discusses the implementation of iris recognition in improving the security of border control systems in the United Arab Emirates. The article explains the significance of the implemented solution and the advantages the government has gained to-date. The UAE deployment of iris recognition technology is currently the largest in the world, both in terms of number of Iris records enrolled (more than 840,751) and number of iris comparisons performed daily 6,225,761,155 (6.2 billion) in ‘all-against-all’ search mode.},
  doi      = {https://doi.org/10.1016/j.tele.2006.06.005},
  keywords = {Border control, Homeland security, Biometrics, Iris recognition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0736585306000360},
}

@InCollection{Ziegler_NIPS_2012,
  author    = {Andrew Ziegler and Eric Christiansen and David Kriegman and Serge J. Belongie},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  title     = {Locally Uniform Comparison Image Descriptor},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1--9},
  abstract  = {Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.},
  url       = {http://papers.nips.cc/paper/4706-locally-uniform-comparison-image-descriptor.pdf},
}

@InProceedings{Posch_SSC_1992,
  author    = {T. E. Posch},
  booktitle = {[1992] Conference Record of the Twenty-Sixth Asilomar Conference on Signals, Systems Computers},
  title     = {The wave packet transform (WPT) as applied to signal processing},
  year      = {1992},
  month     = {Oct},
  pages     = {484-487 vol.1},
  abstract  = {The wave packet transform (WPT) is introduced. It uses the Weyl operator and wave packet functions, i.e., functions that are similar to wavelets, in a linear form to compute coefficients in a two-dimensional space of time and frequency. The importance of the Weyl operator in the WPT and its use with different wave packets are discussed. It is shown that the energetic form of the WPT, the wavepacketgram, i.e., the modulus square of the WPT, is a member of Cohen's class of time-frequency distributions. It is also shown that the wavepacketgram is a positive time-frequency distribution},
  doi       = {10.1109/ACSSC.1992.269225},
  issn      = {1058-6393},
  keywords  = {signal processing;transforms;Cohen's time frequency distributions;Weyl operator;signal processing;two-dimensional space;wave packet functions;wave packet transform;wavepacketgram;Aircraft;Books;Fourier transforms;Kernel;Signal analysis;Signal processing;Spectrogram;Time frequency analysis;Wavelet packets;Wavelet transforms},
}

@InProceedings{Daugman_IMAIP_2000,
  author    = {John Daugman},
  booktitle = {Institute of Mathematics and its Applications, Proc. 2nd IMA-IP},
  title     = {Wavelet Demodulation Codes, Statistical Independence, and Pattern Recognition},
  year      = {2000},
  address   = {Horwood, London, UK},
  pages     = {244--260},
  publisher = {IMA},
  abstract  = {Samples from stochastic signals with sufficient complexity need reveal only a little unexpected agreement, in order to reject the hypothesis that they are independent. The mere failure of a test of statistical independence can thereby serve as a basis for recognizing patterns confidently, provided they possess enough degrees-of-freedom. This paper discusses exploitation of this statistical principle in combination with wavelet image coding to extract phase descriptions of patterns. Demodulation and coarse quantization of the phase information creates decision environments characterized by well separated binomial-class distributions, and this lends itself to rapid and reliable pattern recognition. 1 Introduction The central issue in pattern recognition is the relationship between within-class variability and between-class variability. These are determined by the forms of variation (degrees-of-freedom) spanned by the pattern classes. Ideally the withinclass variability should be small.},
}

@Online{ODIN_URL,
  abstract     = {The goal of this program is to utilize Presentation Attack Detection (PAD) to identify known and unknown Presentation Attacks (PA) in a biometric collection system. A biometric PA, also commonly referred to as biometric spoofing, is a method which inhibits the intended operation of a biometric capture system, interfering with the recording of the true sample/identity, ultimately preventing the subject from being correctly identified. Typical PAs utilize a prosthetic to conceal the biometric signature or present an alternative biometric signature. Existing technology in use primarily relies upon a human security presence to ensure the integrity of the process and that a PA is not being utilized. There are some minimal PAD technologies in use, primarily focused on detecting a specific subset of known PAs. It is anticipated that the use of biometric collection systems will continue to increase. As we become increasingly reliant upon this technology to adjudicate identity, it is important that the technology cannot be easily deceived utilizing a PA. Additionally, reliance upon a human in the loop is cost prohibitive for many applications. Existing PAD approaches focus on methods such as Liveness Detection, Intrinsic Sample Properties, or Artificial Indicators as shown in Table 1. Current sensor hardware captures limited information pertinent to PAD with no intelligence to identify zero-day unknown PAs. A need exists to capture more robust information from a biometric sample to identify, or measure likelihood of, PAs. There needs to be an ‘intelligent’ approach that can identify unknown presentation attacks based on knowledge of what a true sample should look like (e.g., normalcy modeling for anomaly detection). The program is anticipated to be divided into three phases. Phase 1 will last for a period of 18 months and will focus on the ability to detect known PAs. Phase 2 will be 18 months and will focus on the ability to detect unknown PAs. Phase 3 will be 12 months and will focus on operationally relevant performance requirements. Following the conclusion of Phases 1 and 2, respectively, down-selection is possible for a variety of reasons including but not limited to underperforming PAD modalities or proposals.},
  author       = {{Intelligence Advanced Research Projects Activity (IARPA)}},
  lastaccessed = {March 21, 2018},
  title        = {{Odin Program, IARPA-BAA-16-04}},
  url          = {https://www.iarpa.gov/index.php/research-programs/odin/odin-baa},
  year         = {2016},
}

@Article{Anjos_arXiv_2017,
  author = {Andr\'{e} Anjos and Laurent El-Shafey and S\'{e}bastien Marcel},
  title  = {{BEAT: An Open-Source Web-Based Open-Science Platform}},
  year   = {2017},
  eprint = {cs.SE/1704.02319. Retrieved from https://arxiv.org/pdf/1704.02319},
}

@Online{FVC_onGoing_URL,
  abstract     = {VC-onGoing is a web-based automated evaluation system for fingerprint recognition algorithms. Tests are carried out on a set of sequestered datasets and results are reported on-line by using well known performance indicators and metrics. The aim is to track the advances in fingerprint recognition technologies, through continuously updated independent testing and reporting of performances on given benchmarks. The algorithms are evaluated using strongly supervised approaches to maximize trustworthiness.},
  author       = {Dario Maio and Davide Maltoni and Raffaele Cappelli and Annalisa Franco and Matteo Ferrara},
  lastaccessed = {March 7, 2018},
  title        = {{FVC-onGoing: on-line evaluation of fingerprint recognition algorithms}},
  url          = {https://biolab.csr.unibo.it/FVCOnGoing/UI/Form/Home.aspx},
  year         = {2018},
}

@InProceedings{Singh_ISBA_2018,
  author    = {Avantika Singh and Vishesh Mistry and Dhananjay Yadav and Aditya Nigam},
  booktitle = C_ISBA,
  title     = {{GHCLNet: A Generalized Hierarchically tuned Contact Lens detection Network}},
  year      = {2018},
  address   = {Singapore},
  month     = {Jan},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Iris serves as one of the best biometric modality owing to its complex, unique and stable structure. However, it can still be spoofed using fabricated eyeballs and contact lens. Accurate identification of contact lens is must for reliable performance of any biometric authentication system based on this modality. In this paper, we present a novel approach for detecting contact lens using a Generalized Hierarchi- cally tuned Contact Lens detection Network (GHCLNet) . We have proposed hierarchical architecture for three class oculus classification namely: no lens, soft lens and cosmetic lens. Our network architecture is inspired by ResNet-50 model. This network works on raw input iris images without any pre-processing and segmentation requirement and this is one of its prodigious strength. We have performed ex- tensive experimentation on two publicly available data-sets namely: 1)IIIT-D 2)ND and on IIT-K data-set (not publicly available) to ensure the generalizability of our network. The proposed architecture results are quite promising and out- performs the available state-of-the-art lens detection algorithms.},
}

@InProceedings{Kohli_IJCB_2017,
  author    = {Naman Kohli and Daksha Yadav and Mayank Vatsa and Richa Singh and Afzel Noore},
  booktitle = C_IJCB,
  title     = {{Synthetic Iris Presentation Attack using iDCGAN}},
  year      = {2017},
  pages     = {1-6},
  abstract  = {Reliability and accuracy of iris biometric modality has prompted its large-scale deployment for critical applica- tions such as border control and national ID projects. The extensive growth of iris recognition systems has raised ap- prehensions about susceptibility of these systems to various attacks. In the past, researchers have examined the impact of various iris presentation attacks such as textured contact lenses and print attacks. In this research, we present a novel presentation attack using deep learning based synthetic iris generation. Utilizing the generative capability of deep con- volutional generative adversarial networks and iris quality metrics, we propose a new framework, named as iDCGAN (iris deep convolutional generative adversarial network) for generating realistic appearing synthetic iris images. We demonstrate the effect of these synthetically generated iris images as presentation attack on iris recognition by using a commercial system. The state-of-the-art presentation at- tack detection framework, DESIST is utilized to analyze if it can discriminate these synthetically generated iris images from real images. The experimental results illustrate that mitigating the proposed synthetic presentation attack is of paramount importance.},
}

@InCollection{Pinto_DLB_2018,
  author    = {Allan Pinto and Helio Pedrini and Michael Krumdick and Benedict Becker and Adam Czajka and Kevin W. Bowyer and Anderson Rocha},
  booktitle = {Deep Learning in Biometrics},
  publisher = {CRC Press},
  title     = {Counteracting Presentation Attacks in Face, Fingerprint, and Iris Recognition},
  year      = {2018},
  address   = {Boca Raton, London, New York},
  editor    = {Mayank Vatsa, Richa Singh, Angshul Majumdar},
  isbn      = {9781138578234},
  pages     = {245--293},
  url       = {https://www.crcpress.com/Deep-Learning-in-Biometrics/Vatsa-Singh-Majumdar/p/book/9781138578234},
}

@Article{Hsieh_Sensors_2018,
  author   = {Sheng-Hsun Hsieh and Yung-Hui Li and Wei Wang and Chung-Hao Tien},
  journal  = {Sensors},
  title    = {A Novel Anti-Spoofing Solution for Iris Recognition Toward Cosmetic Contact Lens Attack Using Spectral ICA Analysis},
  year     = {2018},
  issn     = {1424-8220},
  number   = {3},
  pages    = {1-15},
  volume   = {18},
  abstract = {In this study, we maneuvered a dual-band spectral imaging system to capture an iridal image from a cosmetic-contact-lens-wearing subject. By using the independent component analysis to separate individual spectral primitives, we successfully distinguished the natural iris texture from the cosmetic contact lens (CCL) pattern, and restored the genuine iris patterns from the CCL-polluted image. Based on a database containing 200 test image pairs from 20 CCL-wearing subjects as the proof of concept, the recognition accuracy (False Rejection Rate: FRR) was improved from FRR = 10.52% to FRR = 0.57% with the proposed ICA anti-spoofing scheme.},
  doi      = {10.3390/s18030795},
  url      = {http://www.mdpi.com/1424-8220/18/3/795},
}

@InProceedings{Malhotra_IICIP_2016,
  author    = {A. Malhotra and R. Gupta},
  booktitle = {2016 1st India International Conference on Information Processing (IICIP)},
  title     = {Iris anti-spoofing under varying illumination conditions},
  year      = {2016},
  month     = {Aug},
  pages     = {1-6},
  abstract  = {Today iris recognition systems are extensively used for security and authentication purposes due to their simplicity and high reliability. But these systems face a major challenge of being spoofed by high quality printed iris images or pictures captured by camera. The problem is aggravated by use of varying illumination conditions in an attack access attempt. This paper investigates spoofing attempts and suggests a simple approach based on statistical parameters to counter against such attacks in the event of varying illumination sources. Self Quotient Image (SQI) is used to combat varying lighting conditions. A feature score is computed based on the statistical parameters. A binary Support Vector Machine (SVM) classifier is used to classify the image under test as real or spoof by matching the feature score of the trained images to that of the test image. The experimental results show that the proposed method performs better than other state-of-the-art techniques.},
  doi       = {10.1109/IICIP.2016.7975358},
  keywords  = {eye;image classification;iris recognition;lighting;security of data;statistics;support vector machines;SQI;SVM classifier;authentication purposes;binary support vector machine;camera;feature score matching;high quality printed iris images;illumination conditions;image classification;iris anti-spoofing;iris recognition systems;security;self quotient image;spoofing attempts;statistical parameters;Authentication;Image edge detection;Iris recognition;Kernel;Lighting;Measurement;Support vector machines;Iris spoofing;Liveness measure;SQI;SVM;Statistical parameters;biometric;countermeasures;normalization;security},
}

@InProceedings{Raja_BIOSIG_2016,
  author    = {Raja, Kiran B. AND Raghavendra, R. AND Braun, Jean-Noel AND Busch, Christoph},
  booktitle = C_BIOSIG,
  title     = {Presentation attack detection using a generalizable statistical approach for periocular and iris systems},
  year      = {2016},
  address   = {Bonn},
  editor    = {Br\"{o}mme, Arslan AND Busch, Christoph AND Rathgeb, Christian AND Uhl, Andreas},
  pages     = {111-122},
  publisher = {Gesellschaft f\"{u}r Informatik e.V.},
}

@InProceedings{Sequeira_VISAPPb_2014,
  author    = {Ana F. Sequeira and Jo{\~{a}}o C. Monteiro and Ana Rebelo and H\'{e}lder P. Oliveira},
  booktitle = C_VISAPP,
  title     = {{MobBIO: A multimodal database captured with a portable handheld device}},
  year      = {2014},
  address   = {Lisbon, Portugal},
  month     = {Jan},
  pages     = {133-139},
  publisher = {IEEE},
  volume    = {3},
  abstract  = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
  keywords  = {Databases;Face;Iris recognition;Lighting;Sensors;Biometrics;Database;Multimodal;Portable Handheld Devices},
}

@InProceedings{Sequeira_IJCB_2017,
  author    = {Ana F. Sequeira and Lulu Chen and James Ferryman and Peter Wild and Fernando Alonso-Fernandez and Josef Bigun and Kiran B. Raja and Ramachandra Raghavendra and Christoph Busch and Tiago de Freitas Pereira and Sebastien Marcel and Sushree Sangeeta Behera and Mahesh Gour and Vivek Kanhangad},
  booktitle = C_IJCB,
  title     = {Cross-eyed 2017: Cross-spectral iris/periocular recognition competition},
  year      = {2017},
  address   = {Denver, CO, USA},
  month     = {Oct},
  pages     = {725-732},
  publisher = {IEEE},
  abstract  = {This work presents the 2nd Cross-Spectrum Iris/Periocular Recognition Competition (Cross-Eyed2017). The main goal of the competition is to promote and evaluate advances in cross-spectrum iris and periocular recognition. This second edition registered an increase in the participation numbers ranging from academia to industry: five teams submitted twelve methods for the periocular task and five for the iris task. The benchmark dataset is an enlarged version of the dual-spectrum database containing both iris and periocular images synchronously captured from a distance and within a realistic indoor environment. The evaluation was performed on an undisclosed test-set. Methodology, tested algorithms, and obtained results are reported in this paper identifying the remaining challenges in path forward.},
  doi       = {10.1109/BTAS.2017.8272762},
  keywords  = {eye;feature extraction;image matching;iris recognition;2nd Cross-Spectrum Iris-Periocular Recognition Competition;cross-spectral iris-periocular recognition competition;dual-spectrum database;iris task;periocular images;periocular task;Databases;Face;Iris recognition;Lighting;Task analysis},
}

@InProceedings{Mishra_ISBI_2019,
  author    = {Suraj Mishra and Adam Czajka and Peixian Liang and Danny Z. Chen and X. Sharon Hu},
  booktitle = {The IEEE Int. Symposium on Biomedical Imaging (ISBI)},
  title     = {CC-Net: Image Complexity Guided Network Compression for Biomedical Image Segmentation},
  year      = {2019},
  address   = {Venice, Italy},
  month     = {April},
  pages     = {1-6},
  publisher = {IEEE},
  url       = {https://arxiv.org/abs/1901.01578},
}


@Article{Speth_CVIU_2021,
  author   = {Jeremy Speth and Nathan Vance and Patrick Flynn and Kevin Bowyer and Adam Czajka},
  journal  = {Computer Vision and Image Understanding},
  title    = {Unifying frame rate and temporal dilations for improved remote pulse detection},
  year     = {2021},
  issn     = {1077-3142},
  pages    = {103246},
  volume   = {210},
  abstract = {Remote photoplethysmography (rPPG) is the monitoring of blood volume pulse from a camera at a distance. 3-Dimensional Convolutional Neural Networks (3DCNNs) have shown promising performance on the rPPG task, although it is critical that we understand the impact of both video and model parameters. In this paper, we explore the effect of frame rate, temporal kernel width, and – more generally – temporal receptive field on the reliability of heart rate and waveform estimation carried out by 3DCNNs. We train and evaluate 32 3DCNNs with different temporal parameters on a new large-scale database for physiological monitoring in an interview scenario. We show that previous studies reporting null effects of frame rate changes on pulse estimators may no longer be valid when using CNNs, and decreasing the frame rate may actually improve performance. In particular, we found that models trained on videos with frame rates as low as 12.9 frames per second (fps) perform better than those trained on videos recorded at a full 90 fps, perhaps due to the temporal receptive fields becoming larger in time dimension when the fps decreases. Using this insight, we propose RemotePulseNet, a novel 3DCNN architecture that exploits temporally dilated convolutions with increasing dilation rate to drastically increase the receptive field. We compare its performance with that of recent state-of-the-art pulse estimation methods, and show that both RemotePulseNet and the low frame rate 3DCNNs produce high-quality pulse signals from faces captured under a challenging interview scenario. The source code and instructions for obtaining a copy of the test data are made available with this paper.},
  doi      = {https://doi.org/10.1016/j.cviu.2021.103246},
  keywords = {Remote photoplethysmography, Face video, Physiological monitoring, Spatiotemporal modeling, Pulse detection},
  url      = {https://www.sciencedirect.com/science/article/pii/S1077314221000904},
}


@Article{Speth_ArXiv_2021,
  author        = {Jeremy Speth and Nathan Vance and Patrick J. Flynn and Kevin W. Bowyer and Adam Czajka},
  journal       = {CoRR},
  title         = {Remote Pulse Estimation in the Presence of Face Masks},
  year          = {2021},
  volume        = {abs/2101.04096},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2101-04096.bib},
  eprint        = {2101.04096},
  timestamp     = {Thu, 21 Jan 2021 14:42:30 +0100},
  url           = {https://arxiv.org/abs/2101.04096},
}

@Article{Raghavendra_TIFS_2016,
  author   = {R. Raghavendra and K. B. Raja and C. Busch},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {Exploring the Usefulness of Light Field Cameras for Biometrics: An Empirical Study on Face and Iris Recognition},
  year     = {2016},
  issn     = {1556-6013},
  month    = {May},
  number   = {5},
  pages    = {922-936},
  volume   = {11},
  abstract = {A light field sensor can provide useful information in terms of multiple depth (or focus) images, holding additional information that is quite useful for biometric applications. In this paper, we examine the applicability of a light field camera for biometric applications by considering two prominently used biometric characteristics: 1) face and 2) iris. To this extent, we employed a Lytro light field camera to construct two new and relatively large scale databases, for both face and iris biometrics. We then explore the additional information available from different depth images, which are rendered by light field camera, in two different manners: 1) by selecting the best focus image from the set of depth images and 2) combining all the depth images using super-resolution schemes to exploit the supplementary information available within the set elements. Extensive evaluations are carried out on our newly constructed database, demonstrating the significance of using additional information rendered by a light field camera to improve the overall performance of the biometric system.},
  doi      = {10.1109/TIFS.2015.2512559},
  keywords = {face recognition;image resolution;iris recognition;Lytro light field camera;best focus image;biometric system;depth images;face recognition;iris recognition;light field sensor;super-resolution schemes;Cameras;Databases;Face;Iris recognition;Lenses;Biometrics;Face recognition;Iris recognition;Light field camera;Visible Iris;biometrics;face recognition;iris recognition;visible iris},
}

@InProceedings{Drozdowski_BIOSIG_2017,
  author    = {Pawel Drozdowski and Christian Rathgeb and Christoph Busch},
  booktitle = C_BIOSIG,
  title     = {{Sic-Gen: A Synthetic Iris-Code Generator}},
  year      = {2017},
  address   = {Darmstadt, Germany},
  month     = {Sept},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Nowadays large-scale identity management systems enrol more than one billion data subjects. In order to limit transaction times, biometric indexing is a suitable method to reduce the search space in biometric identifications. Effective testing of such biometric identification systems and biometric indexing approaches requires large datasets of biometric data. Currently, the size of the publicly available iris datasets is insufficient, especially for system scalability assessments. Synthetic data generation offers a potential solution to this issue; however, it is challenging to generate data that is both statistically sound and visually realistic - for the iris, the currently available approaches prove unsatisfactory. In this paper, we present a method for generation of synthetic binary iris-based templates, i.e. Iris-Codes, which are the de facto standard used throughout major biometric deployments around the world. We validate the statistical properties of the synthetic templates and show that they closely resemble ones produced from real ocular images. With the proposed approach, large databases of synthetic Iris-Codes with flexibly adjustable properties can be generated.},
  doi       = {10.23919/BIOSIG.2017.8053520},
  keywords  = {iris recognition;statistical analysis;SIC-gen;biometric data;biometric deployments;biometric identification systems;biometric indexing approach;eal ocular images;effective testing;large-scale identity management systems;publicly available iris datasets;search space reduction;statistical properties;synthetic Iris-Codes;synthetic Iris-code generator;synthetic binary iris-based templates;synthetic data generation;synthetic templates;system scalability assessments;Correlation;Hidden Markov models;High definition video;Indexing;Iris recognition},
}

@InCollection{Czajka_PAD_Handbook_2018,
  author    = {Adam Czajka and Benedict Becker},
  booktitle = {Handbook of Biometric Anti-Spoofing (2nd Edition, to appear)},
  publisher = {Springer International Publishing AG},
  title     = {{Application of Dynamic Features of the Pupil for Iris Presentation Attack Detection}},
  year      = {2018},
  editor    = {S\'{e}bastien Marcel and Mark Nixon and Julian Fierrez and Nicholas Evans},
  pages     = {1--17},
  abstract  = {This chapter presents a comprehensive study on application of stimulated pupillary light reflex to presentation attack detection (PAD) that can be used in iris recognition systems. A pupil, when stimulated by visible light in a predefined manner, may offer sophisticated dynamic liveness features that cannot be acquired from dead eyes or other static objects such as printed contact lenses, paper printouts or prosthetic eyes. Modeling of pupil dynamics requires a few seconds of observation under varying light conditions that can be supplied by a visible light source in addition to the existing near-infrared illuminants used in iris image acquisition. The central element of the presented approach is an accurate modeling and classification of pupil dynamics that makes mimicking an actual eye reaction difficult. This chapter discusses new data-driven models of pupil dynamics based on recurrent neural networks and compares their PAD performance to solutions based on parametric Clynes-Kohn model and various classification techniques. Experiments with 166 distinct eyes of 84 subjects show that the best data-driven solution, one based on long-short term memory, was able to correctly recognize 99.97\% of attack presentations and 98.62\% of normal pupil reactions. In the approach using the Clynes-Kohn parametric model of pupil dynamics, we were able to perfectly recognize abnormalities and correctly recognize 99.97\% of normal pupil reactions on the same dataset with the same evaluation protocol as the data-driven approach. This means that the data-driven solutions favorably compare to the parametric approaches, which require model identification in exchange for a slightly better performance. We also show that observation times may be as short as 3 seconds when using the parametric model, and as short as 2 seconds when applying the recurrent neural network without substantial loss in accuracy. Along with this chapter we also offer: a) all time series representing pupil dynamics for 166 distinct eyes used in this study, b) weights of the trained recurrent neural network offering the best performance, c) source codes of the reference PAD implementation based on Clynes-Kohn parametric model, and d) all PAD scores that allow the reproduction of the plots presented in this chapter. To our best knowledge, this chapter proposes the first database of pupil measurements dedicated to presentation attack detection and the first evaluation of recurrent neural network-based modeling of pupil dynamics and PAD.},
}

@InCollection{Morales_PAD_Handbook_2018,
  author    = {Aythami Morales and Julian Fierrez and Javier Galbally and Marta Gomez-Barrero},
  booktitle = {Handbook of Biometric Anti-Spoofing (2nd Edition, to appear)},
  publisher = {Springer International Publishing AG},
  title     = {{Introduction to Iris Presentation Attack Detection}},
  year      = {2018},
  editor    = {S\'{e}bastien Marcel and Mark Nixon and Julian Fierrez and Nicholas Evans},
  pages     = {1--15},
  abstract  = {Iris recognition technology has attracted an increasing interest during the last decade in which we have witnessed a migration from laboratories to real world applications. The deployment of this technology in real applications raises questions about the main vulnerabilities and security threats related to these systems. Presen- tation attacks can be defined as presentation of human characteristics or artifacts directly to the input of a biometric system trying to interfere its normal operation. These attacks include the use of real irises as well as artifacts with different level of sophistication. This chapter introduces iris Presentation Attack Detection meth- ods and its main challenges. First, we summarize the most popular types of attacks including the main challenges to address. Secondly, we present a taxonomy of Pre- sentation Attack Detection methods to serve as a brief introduction on this very active research area. Finally, we discuss about the integration of these methods into Iris Recognition Systems according to the most important scenarios of practical application.},
}

@InCollection{Yambay_PAD_Handbook_2018,
  author    = {David Yambay and Stephanie Schuckers},
  booktitle = {Handbook of Biometric Anti-Spoofing (2nd Edition, to appear)},
  publisher = {Springer International Publishing AG},
  title     = {{Review of Iris Presentation Attack Detection Competitions}},
  year      = {2018},
  editor    = {S\'{e}bastien Marcel and Mark Nixon and Julian Fierrez and Nicholas Evans},
  pages     = {1--16},
  abstract  = {This chapter reviews the previous LivDet Iris competitions and the evolution of the competitions over the years. The following sections will describe the methods used in testing for each of the LivDet competitions as well as descriptions of the datasets that been generated from each competition. Also discussed are the trends across the different competitions that reflects changes to the art of presentation at- tacks as well as advances in the state of the art in presentation attack detection. Further, conclusions from previous LivDet competitions and the future of LivDet is discussed.},
}

@InProceedings{Vinyals_CVPR_2015,
  author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Show and tell: A neural image caption generator},
  year      = {2015},
  pages     = {3156-3164},
  doi       = {10.1109/CVPR.2015.7298935},
}


@Article{Karpathy_TPAMI_2017,
  author  = {Karpathy, Andrej and Fei-Fei, Li},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  year    = {2017},
  number  = {4},
  pages   = {664-676},
  volume  = {39},
  doi     = {10.1109/TPAMI.2016.2598339},
}



@InProceedings{Zhou_CVPR_2016,
  author    = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning Deep Features for Discriminative Localization},
  year      = {2016},
  pages     = {2921-2929},
  doi       = {10.1109/CVPR.2016.319},
}

@InProceedings{Desai_WACV_2020,
  author    = {Desai, Saurabh and Ramaswamy, Harish G.},
  booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization},
  year      = {2020},
  pages     = {972-980},
  doi       = {10.1109/WACV45572.2020.9093360},
}

@InProceedings{Selvaraju_ICCV_2017,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  year      = {2017},
  pages     = {618-626},
  doi       = {10.1109/ICCV.2017.74},
}

@InProceedings{Fong_ICCV_2017,
  author    = {Fong, Ruth C. and Vedaldi, Andrea},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
  year      = {2017},
  pages     = {3449-3457},
  doi       = {10.1109/ICCV.2017.371},
}

@Article{Goodman_AIMAG_2017,
  author    = {Bryce Goodman and Seth Flaxman},
  journal   = {{AI} Magazine},
  title     = {European Union Regulations on Algorithmic Decision-Making and a {\textquotedblleft}Right to Explanation{\textquotedblright}},
  year      = {2017},
  month     = oct,
  number    = {3},
  pages     = {50--57},
  volume    = {38},
  doi       = {10.1609/aimag.v38i3.2741},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  url       = {https://doi.org/10.1609/aimag.v38i3.2741},
}

@InProceedings{Chen_WACV_2018,
  author    = {Cunjian Chen and Arun Ross},
  booktitle = C_WACV,
  title     = {A Multi-task Convolutional Neural Network for Joint Iris Detection and Presentation Attack Detection},
  year      = {2018},
  address   = {Lake Tahoe, Nevada, United States},
  month     = {March},
  pages     = {44-51},
  publisher = {IEEE},
  abstract  = {In this work, we propose a multi-task convolutional neural network learning approach that can simultaneously perform iris localization and presentation attack detection (PAD). The proposed multi-task PAD (MT-PAD) is inspired by an object detection method which directly regresses the parameters of the iris bounding box and computes the probability of presentation attack from the input ocular image. Experiments involving both intra-sensor and cross-sensor scenarios suggest that the proposed method can achieve state-of-the-art results on publicly available datasets. To the best of our knowledge, this is the first work that performs iris detection and iris presentation attack detection simultaneously.},
  doi       = {10.1109/WACVW.2018.00011},
  keywords  = {eye;feedforward neural nets;iris recognition;learning (artificial intelligence);object detection;security of data;MT-PAD;cross-sensor scenarios;input ocular image;intra-sensor scenarios;iris bounding box;iris localization;joint iris detection;multitask PAD;multitask convolutional neural network learning approach;object detection method;presentation attack detection;Convolutional neural networks;Feature extraction;Iris;Iris recognition;Lenses;Task analysis;Training},
}

@InCollection{Galbally_IETbook_Ch11_2017,
  author    = {Javier Galbally and Marta Gomez-Barrero},
  booktitle = {Iris and Periocular Biometric Recognition},
  publisher = {IET},
  title     = {Presentation Attack Detection in Iris Recognition},
  year      = {2017},
  address   = {London, UK},
  chapter   = {11},
  editor    = {Christian Rathgeb and Christoph Busch},
  isbn      = {978-1-78561-168-1},
  pages     = {235--263},
  url       = {https://www.theiet.org/resources/books/security/irisper.cfm},
}


@InCollection{VanderKolk_ACEV_2011,
  author    = {John R. VanderKolk},
  booktitle = {Fingerprint Sourcebook},
  publisher = {NIJ},
  title     = {Examination Process},
  year      = {2011},
  address   = {London, UK},
  chapter   = {9},
  note      = {{NCJ 225320}},
  url       = {https://nij.ojp.gov/library/publications/fingerprint-sourcebook},
}


@InCollection{Komulainen_IETbook_Ch12_2017,
  author    = {Jukka Komulainen and Abdenour Hadid and Matti Pietik\"{a}inen},
  booktitle = {Iris and Periocular Biometric Recognition},
  publisher = {IET},
  title     = {Contact lens detection in iris images},
  year      = {2017},
  address   = {London, UK},
  chapter   = {12},
  editor    = {Christian Rathgeb and Christoph Busch},
  isbn      = {978-1-78561-168-1},
  pages     = {265--290},
  url       = {https://www.theiet.org/resources/books/security/irisper.cfm},
}

@InProceedings{Rathgeb_IJCB_2017,
  author    = {Christian Rathgeb and Christoph Busch},
  booktitle = C_IJCB,
  title     = {On the feasibility of creating morphed iris-codes},
  year      = {2017},
  address   = {Denver, CO, USA},
  month     = {Oct},
  pages     = {152-157},
  publisher = {IEEE},
  abstract  = {Morphing techniques can be used to create artificial biometric samples, which resemble the biometric information of two (or more) individuals in image and feature domain. If morphed biometric images or templates are infiltrated to a biometric recognition system the subjects contributing to the morphed image will both (or all) be successfully verified against a single enrolled template. Hence, the unique link between individuals and their biometric reference data is annulled. The vulnerability of face and fingerprint recognition systems to such morphing attacks has been assessed in the recent past. In this paper we investigate the feasibility of morphing iris-codes. Two relevant attack scenarios are discussed and a scheme for morphing pairs of iris-codes depending on the expected stability of their bits is proposed. Different iris recognition systems, which accept comparison scores at a recommended Hamming distance of 0.32, are shown to be vulnerable to attacks based on the presented morphing technique.},
  doi       = {10.1109/BTAS.2017.8272693},
  keywords  = {biometrics (access control);fingerprint identification;iris recognition;Hamming distance;artificial biometric samples;biometric information;biometric recognition system;biometric reference data;feature domain;fingerprint recognition systems;iris recognition systems;morphed biometric images;morphed image;morphed iris-codes;morphing attacks;morphing iris-codes;morphing pairs;morphing technique;single enrolled template;Databases;Face;Iris recognition;Magnetic resonance;Stability analysis},
}

@Article{Sollinger_IET_2017,
  author    = {Dominik S{\"{o}}llinger and Pauline Trung and Andreas Uhl},
  journal   = {IET Biometrics},
  title     = {Non-reference image quality assessment and natural scene statistics to counter biometric sensor spoofing},
  year      = {2018},
  issn      = {2047-4938},
  month     = {January},
  pages     = {1--11},
  abstract  = {Non-reference image quality measures (IQM) as well as their associated natural scene statistics (NSS) are used to distinguish real biometric data from fake data as used in presentation/sensor spoofing attacks. An experimental study shows that a support vector machine directly trained on NSS as used in blind/referenceless image spatial quality evaluator provides highly accurate classification of real versus fake iris, fingerprint, face, and fingervein data in generic manner. This contrasts to using the IQM directly, the accuracy of which turns out to be rather data set and parameter choice-dependent. While providing very low average classification error rate values for complete training data, generalisation to unseen attack types is difficult in open-set scenarios and obtained accuracy varies in almost unpredictable manner. This implies that for each given sensor/attack set-up, the ability of the introduced methods to detect unseen attacks needs to be assessed separately.},
  copyright = {This is an open access article published by the IET under the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/)},
  doi       = {10.1049/iet-bmt.2017.0146},
  keywords  = {fingervein data;associated natural scene statistics;blind/referenceless image spatial quality evaluator;low average classification error rate values;nonreference image quality measures;fake data;biometric data;highly accurate classification;complete training data;NSS;unseen attack types;biometric sensor spoofing;unseen attacks;IQM;versus fake iris;presentation/sensor spoofing attacks;experimental study;nonreference image quality assessment;generic manner;open-set scenarios;given sensor/attack;support vector machine;},
  language  = {English},
  publisher = {Institution of Engineering and Technology},
  url       = {http://digital-library.theiet.org/content/journals/10.1049/iet-bmt.2017.0146},
}

@TechReport{IREX_IX,
  author      = {George W. Quinn and Patrick Grother and James Matey},
  institution = {NIST},
  title       = {{IREX IX Part One: Performance of Iris Recognition Algorithms}},
  year        = {2018},
  note        = {{Interagency Report 7629}},
  abstract    = {The IREX program supports the development of interoperable iris imagery for use in high performance biometric applications. The IREX evaluation, was conducted in cooperation with the iris recognition industry to demonstrate that standardized image formats can be interoperable and compact. This is required for federated applications in which iris data is exchanged between interoperating systems, passed across bandwidth-limited networks, or stored on identity credentials. The IREX I study was initiated to give quantitative support to the revision of the ISO/IEC 19794-6 and ANSI/NIST TYPE 17 standards, and to form a multi-provider marketplace around those standards.},
  url         = {https://doi.org/10.6028/NIST.IR.8207},
}

@TechReport{IREX_XI,
  author      = {George W. Quinn and Patrick Grother and James Matey},
  institution = {NIST},
  title       = {{IREX IX Part One: Performance of Iris Recognition Algorithms}},
  year        = {2018},
  note        = {{Interagency Report 7629}},
  abstract    = {The IREX program supports the development of interoperable iris imagery for use in high performance biometric applications. The IREX evaluation, was conducted in cooperation with the iris recognition industry to demonstrate that standardized image formats can be interoperable and compact. This is required for federated applications in which iris data is exchanged between interoperating systems, passed across bandwidth-limited networks, or stored on identity credentials. The IREX I study was initiated to give quantitative support to the revision of the ISO/IEC 19794-6 and ANSI/NIST TYPE 17 standards, and to form a multi-provider marketplace around those standards.},
  url         = {https://doi.org/10.6028/NIST.IR.8207},
}


@Article{Trokielewicz_TIFS_2019,
  author   = {M. {Trokielewicz} and A. {Czajka} and P. {Maciejewicz}},
  journal  = J_TIFS,
  title    = {Iris Recognition After Death},
  year     = {2019},
  issn     = {1556-6013},
  month    = {June},
  number   = {6},
  pages    = {1501-1514},
  volume   = {14},
  abstract = {This paper presents a comprehensive study of post-mortem human iris recognition carried out for 1200 near-infrared and 1787 visible-light samples collected from 37 deceased individuals kept in mortuary conditions. We used four independent iris recognition methods (three commercial and one academic) to analyze genuine and impostor comparison scores and check the dynamics of iris quality decay over a period of up to 814 h after death. This study shows that post-mortem iris recognition may be close-to-perfect approximately 5-7 h after death and occasionally is still viable even 21 days after death. These conclusions contradict the statements present in the past literature that the iris is unusable as a biometrics shortly after death, and show that the dynamics of post-mortem changes to the iris that are important for biometric identification are more moderate than previously hypothesized. This paper contains a thorough medical commentary that helps to understand which post-mortem metamorphoses of the eye may impact the performance of automatic iris recognition. An important finding is that false-match probability is higher when live iris images are compared with post-mortem samples than when only live samples are used in comparisons. This paper conforms to reproducible research and the database used in this study is made publicly available to facilitate research on post-mortem iris recognition. To the best of our knowledge, this paper offers the most comprehensive evaluation of post-mortem iris recognition and the largest database of post-mortem iris images.},
  doi      = {10.1109/TIFS.2018.2881671},
  keywords = {biometrics (access control);eye;image matching;image recognition;iris recognition;biometric identification;post-mortem human iris recognition;iris images;automatic iris recognition;post-mortem metamorphoses;post-mortem changes;post-mortem iris recognition;death;iris quality decay;independent iris recognition methods;Iris recognition;Cadaver;Databases;Biomedical imaging;Cameras;Image color analysis;Sensors;Iris recognition;post-mortem biometrics;forensics},
}



@InProceedings{Moreira_WACV_2019,
  author    = {Moreira, Daniel and Trokielewicz, Mateusz and Czajka, Adam and Bowyer, Kevin and Flynn, Patrick},
  booktitle = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Performance of Humans in Iris Recognition: The Impact of Iris Condition and Annotation-Driven Verification},
  year      = {2019},
  pages     = {941-949},
  doi       = {10.1109/WACV.2019.00105},
}



@InProceedings{Trokielewicz_WACV_2020,
  author    = {Trokielewicz, Mateusz and Czajka, Adam and Maciejewicz, Piotr},
  booktitle = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes},
  year      = {2020},
  pages     = {2296-2304},
  doi       = {10.1109/WACV45572.2020.9093281},
}



@Article{Othman_PRL_2016,
  author   = {Nadia Othman and Bernadette Dorizzi and Sonia Garcia-Salicetti},
  journal  = {Pattern Recognition Letters},
  title    = {OSIRIS: An open source iris recognition software},
  year     = {2016},
  issn     = {0167-8655},
  pages    = {124 - 131},
  volume   = {82},
  abstract = {In this paper, we present the evolution of the open source iris recognition system OSIRIS through its more relevant versions: OSIRISV2, OSIRISV4, and OSIRISV4.1. We developed OSIRIS in the framework of BioSecure Association as an open source software aiming at providing a reference for the scientific community. The software is mainly composed of four key modules, namely segmentation, normalization, feature extraction and template matching, which are described in detail for each version. A novel approach for iris normalization, based on a non geometric parameterization of contours is proposed in the latest version: OSIRISV4.1 and is detailed in particular here. Improvements in performance through the different versions of OSIRIS are reported on two public databases commonly used, ICE2005 and CASIA-IrisV4-Thousand. We note the high verification rates obtained by the last version. For this reason, OSIRISV4.1 can be proposed as a baseline system for comparison to other algorithms, this way supplying a helpful research tool for the iris recognition community.},
  doi      = {10.1016/j.patrec.2015.09.002},
  keywords = {Iris recognition, Open source system, OSIRIS, Viterbi, Non geometric parameterization of contours, Iris normalization},
  url      = {https://doi.org/10.1016/j.patrec.2015.09.002},
}

@Misc{Conlin_Dev_2012,
  author       = {Linda Conlin},
  howpublished = {online},
  month        = {November},
  title        = {{Embryonic Eye Development}},
  year         = {2012},
  url          = {https://www.2020mag.com/ce/embryonic-eye-development-abo-and-7A85C},
}

@Misc{Trokielewicz_PhD_2019,
  author       = {Mateusz Trokielewicz},
  howpublished = {PhD Thesis, Warsaw University of Technology},
  title        = {Iris recognition methods resistant to biological changes in the eye},
  year         = {2019},
  url          = {http://zbum.ia.pw.edu.pl/PAPERS/Trokielewicz\_PhD\_2019.pdf},
}

@Article{Boyd_ICCV_2021,
  author        = {Aidan Boyd and Kevin W. Bowyer and Adam Czajka},
  journal       = {CoRR},
  title         = {Human-Aided Saliency Maps Improve Generalization of Deep Learning},
  year          = {2021},
  volume        = {abs/2105.03492},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2105-03492.bib},
  eprint        = {2105.03492},
  timestamp     = {Fri, 14 May 2021 12:13:30 +0200},
  url           = {https://arxiv.org/abs/2105.03492},
}

@Article{Trokielewicz_ArXiv_2019,
  author        = {{Trokielewicz}, Mateusz and {Czajka}, Adam and {Maciejewicz}, Piotr},
  journal       = {arXiv e-prints},
  title         = {{Post-mortem Iris Recognition with Deep-Learning-based Image Segmentation}},
  year          = {2019},
  month         = {Jan},
  pages         = {arXiv:1901.01708},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190101708T},
  archiveprefix = {arXiv},
  eprint        = {1901.01708},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}

@Article{Trokielewicz_ArXiv_2019_Decomposition,
  author        = {{Trokielewicz}, Mateusz and {Czajka}, Adam and {Maciejewicz}, Piotr},
  journal       = {arXiv e-prints},
  title         = {{Post-mortem Iris Decomposition and its Dynamics in Morgue Conditions}},
  year          = {2019},
  month         = {Nov},
  pages         = {arXiv:1911.02837},
  archiveprefix = {arXiv},
  eprint        = {1911.02837},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}


@Article{Trokielewicz_IVC_2020,
  author   = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  journal  = {Image and Vision Computing},
  title    = {Post-mortem iris recognition with deep-learning-based image segmentation},
  year     = {2020},
  issn     = {0262-8856},
  pages    = {103866},
  volume   = {94},
  abstract = {This paper proposes the first known to us iris recognition methodology designed specifically for post-mortem samples. We propose to use deep learning-based iris segmentation models to extract highly irregular iris texture areas in post-mortem iris images. We show how to use segmentation masks predicted by neural networks in conventional, Gabor-based iris recognition method, which employs circular approximations of the pupillary and limbic iris boundaries. As a whole, this method allows for a significant improvement in post-mortem iris recognition accuracy over the methods designed only for ante-mortem irises, including the academic OSIRIS and commercial IriCore implementations. The proposed method reaches the EER less than 1% for samples collected up to 10 hours after death, when compared to 16.89% and 5.37% of EER observed for OSIRIS and IriCore, respectively. For samples collected up to 369 h post-mortem, the proposed method achieves the EER 21.45%, while 33.59% and 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the method is tested on a database of iris images collected from ophthalmology clinic patients, for which it also offers an advantage over the two other algorithms. This work is the first step towards post-mortem-specific iris recognition, which increases the chances of identification of deceased subjects in forensic investigations. The new database of post-mortem iris images acquired from 42 subjects, as well as the deep learning-based segmentation models are made available along with the paper, to ensure all the results presented in this manuscript are reproducible.},
  doi      = {https://doi.org/10.1016/j.imavis.2019.103866},
  keywords = {Biometrics, Iris recognition, Post-mortem, Image segmentation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0262885619304597},
}

@Article{Trokielewicz_JFS_2020_Decomposition,
  author   = {Trokielewicz, Mateusz and Czajka, Adam and Maciejewicz, Piotr},
  journal  = {Journal of Forensic Sciences},
  title    = {{Post-mortem Iris Decomposition and its Dynamics in Morgue Conditions}},
  year     = {2020},
  note     = {ArXiv pre-print: \url{https://arxiv.org/abs/1911.02837}},
  number   = {5},
  pages    = {1530-1538},
  volume   = {65},
  abstract = {Abstract With increasing interest in employing iris biometrics as a forensic tool for identification by investigation authorities, there is a need for a thorough examination and understanding of postmortem decomposition processes that take place within the human eyeball, especially the iris. This can prove useful for fast and accurate matching of antemortem with postmortem data acquired at crime scenes or mass casualties, as well as for ensuring correct dispatching of bodies from the incident scene to a mortuary or funeral homes. Following these needs of forensic community, this paper offers an analysis of the coarse effects of eyeball decay done from a perspective of automatic iris recognition. We analyze postmortem iris images acquired for a subject with a very long postmortem observation time horizon (34 days), in both visible light and near-infrared light (860 nm), as the latter wavelength is used in commercial iris recognition systems. Conclusions and suggestions are provided that may aid forensic examiners in successfully utilizing iris patterns in postmortem identification of deceased subjects. Initial guidelines regarding the imaging process, types of illumination, and resolution are also given, together with expectations with respect to the iris features decomposition rates. Visible iris features possible for human, expert-based matching persists even up to 407 h postmortem, and near-infrared illumination is suggested for better mitigation of corneal opacity while imaging cadaver eyes.},
  doi      = {10.1111/1556-4029.14488},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1556-4029.14488},
  keywords = {iris recognition, eye, decomposition, postmortem, biometrics},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1556-4029.14488},
}


@Article{Trokielewicz_ArXiv_WACV2020,
  author        = {{Trokielewicz}, Mateusz and {Czajka}, Adam and {Maciejewicz}, Piotr},
  journal       = {arXiv e-prints},
  title         = {{Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes}},
  year          = {2019},
  month         = {Dec},
  pages         = {arXiv:1912.02512},
  archiveprefix = {arXiv},
  eid           = {arXiv:1912.02512},
  eprint        = {1912.02512},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}

@Article{Nguyen_Access_2018,
  author   = {K. {Nguyen} and C. {Fookes} and A. {Ross} and S. {Sridharan}},
  journal  = {IEEE Access},
  title    = {Iris Recognition With Off-the-Shelf CNN Features: A Deep Learning Perspective},
  year     = {2018},
  issn     = {2169-3536},
  pages    = {18848-18855},
  volume   = {6},
  abstract = {Iris recognition refers to the automated process of recognizing individuals based on their iris patterns. The seemingly stochastic nature of the iris stroma makes it a distinctive cue for biometric recognition. The textural nuances of an individual's iris pattern can be effectively extracted and encoded by projecting them onto Gabor wavelets and transforming the ensuing phasor response into a binary code - a technique pioneered by Daugman. This textural descriptor has been observed to be a robust feature descriptor with very low false match rates and low computational complexity. However, recent advancements in deep learning and computer vision indicate that generic descriptors extracted using convolutional neural networks (CNNs) are able to represent complex image characteristics. Given the superior performance of CNNs on the ImageNet large scale visual recognition challenge and a large number of other computer vision tasks, in this paper, we explore the performance of state-of-the-art pre-trained CNNs on iris recognition. We show that the off-the-shelf CNN features, while originally trained for classifying generic objects, are also extremely good at representing iris images, effectively extracting discriminative visual features and achieving promising recognition results on two iris datasets: ND-CrossSensor-2013 and CASIA-IrisThousand. We also discuss the challenges and future research directions in leveraging deep learning methods for the problem of iris recognition.},
  doi      = {10.1109/ACCESS.2017.2784352},
  keywords = {computational complexity;computer vision;feature extraction;image classification;image matching;image representation;image texture;iris recognition;learning (artificial intelligence);neural nets;wavelet transforms;biometric recognition;robust feature descriptor;low computational complexity;computer vision;iris recognition;off-the-shelf CNN features;iris images;iris datasets;iris patterns;iris stroma;ImageNet large scale visual recognition;textural nuances;Gabor wavelets;binary code;phasor response;textural descriptor;false match rates;generic descriptors;deep learning method;convolutional neural networks;complex image characteristics;generic object classification;discriminative visual feature extraction;Iris recognition;Machine learning;Computer architecture;Visualization;Computer vision;Feature extraction;Iris recognition;biometrics;deep learning;convolutional neural network},
}

@InProceedings{Boyd_BTAS_2019,
  author    = {Aidan Boyd and Adam Czajka and Kevin Bowyer},
  booktitle = C_BTAS,
  title     = {{Deep Learning-Based Feature Extraction in Iris Recognition: Use Existing Models, Fine-tune or Train From Scratch?}},
  year      = {2019},
  address   = {Tampa, FL, USA},
  month     = {Sept},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {Modern deep learning techniques can be employed to generate effective feature extractors for the task of iris recognition. The question arises: should we train such structures from scratch on a relatively large iris image dataset, or it is better to fine-tune the existing models to adapt them to a new domain? In this work we explore five different sets of weights for the popular ResNet-50 architecture to find out whether iris-specific feature extractors perform better than models trained for non-iris tasks. Features are extracted from each convolutional layer and the classification accuracy achieved by a Support Vector Machine is measured on a dataset that is disjoint from the samples used in training of the ResNet-50 model. We show that the optimal training strategy is to fine-tune an off-the-shelf set of weights to the iris recognition domain. This approach results in greater accuracy than both off-the-shelf weights and a model trained from scratch. The winning, fine-tuned approach also shows an increase in performance when compared to previous work, in which only off-the-shelf (not fine-tuned) models were used in iris feature extraction. We make the best-performing ResNet-50 model, fine-tuned with more than 360,000 iris images, publicly available along with this paper.},
}

@Article{OToole_TSMC_2007,
  author  = {A. J. {O'Toole} and H. {Abdi} and F. {Jiang} and P. J. {Phillips}},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title   = {Fusing Face-Verification Algorithms and Humans},
  year    = {2007},
  number  = {5},
  pages   = {1149-1155},
  volume  = {37},
  doi     = {10.1109/TSMCB.2007.907034},
}

@InProceedings{He_ICCV_2019,
  author    = {S. {He} and H. R. {Tavakoli} and A. {Borji} and N. {Pugeault}},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Human Attention in Image Captioning: Dataset and Analysis},
  year      = {2019},
  pages     = {8528-8537},
  doi       = {10.1109/ICCV.2019.00862},
}

@InProceedings{Peterson_ICCV_2019,
  author    = {J. {Peterson} and R. {Battleday} and T. {Griffiths} and O. {Russakovsky}},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Human Uncertainty Makes Classification More Robust},
  year      = {2019},
  pages     = {9616-9625},
  doi       = {10.1109/ICCV.2019.00971},
}

  
@InProceedings{Benenson_CVPR_2019,
  author    = {R. {Benenson} and S. {Popov} and V. {Ferrari}},
  booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Large-Scale Interactive Object Segmentation With Human Annotators},
  year      = {2019},
  pages     = {11692-11701},
  doi       = {10.1109/CVPR.2019.01197},
}

@InProceedings{Czajka_WACV_2019b,
  author    = {A. {Czajka} and D. {Moreira} and K. {Bowyer} and P. {Flynn}},
  booktitle = C_WACV,
  title     = {Domain-Specific Human-Inspired Binarized Statistical Image Features for Iris Recognition},
  year      = {2019},
  address   = {Waikoloa Village, Hawai, United States},
  month     = {Jan},
  pages     = {959-967},
  publisher = {IEEE},
  abstract  = {Binarized statistical image features (BSIF) have been successfully used for texture analysis in many computer vision tasks, including iris recognition and biometric presentation attack detection. One important point is that all applications of BSIF in iris recognition have used the original BSIF filters, which were trained on image patches extracted from natural images. This paper tests the question of whether domain-specific BSIF can give better performance than the default BSIF. The second important point is in the selection of image patches to use in training for BSIF. Can image patches derived from eye-tracking experiments, in which humans perform an iris recognition task, give better performance than random patches? Our results say that (1) domain-specific BSIF features can out-perform the default BSIF features, and (2) selecting image patches in a task-specific manner guided by human performance can out-perform selecting random patches. These results are important because BSIF is often regarded as a generic texture tool that does not need any domain adaptation, and human-task-guided selection of patches for training has never (to our knowledge) been done. This paper follows the reproducible research requirements, and the new iris-domain-specific BSIF filters, the patches used in filter training, the database used in testing and the source codes of the designed iris recognition method are made available along with this paper to facilitate applications of this concept.},
  doi       = {10.1109/WACV.2019.00107},
  issn      = {1550-5790},
  keywords  = {biometrics (access control);computer vision;feature extraction;image classification;image representation;image texture;iris recognition;sampling methods;statistical analysis;natural images;recognition task;default BSIF features;human performance;human-task-guided selection;iris-domain-specific BSIF filters;filter training;designed iris recognition method;computer vision tasks;biometric presentation attack detection;original BSIF filters;domain-specific BSIF features;image patches;binarized statistical image features;Iris recognition;Feature extraction;Histograms;Standards;Task analysis;Training;Pipelines},
}

@InCollection{Rathgeb_HoIR_2016,
  author    = {Christian Rathgeb and Andreas Uhl and Peter Wild and Heinz Hofbauer},
  booktitle = {Handbook of Iris Recognition},
  publisher = {Springer},
  title     = {Design Decisions for an Iris Recognition SDK},
  year      = {2016},
  address   = {London, UK},
  edition   = {second},
  editor    = {Kevin Bowyer and Mark J. Burge},
  series    = {Advances in Computer Vision and Pattern Recognition},
}

@Online{VeriEye,
  author       = {Neurotechnology},
  organization = {Neurotechnology},
  title        = {{VeriEye SDK v11.1}},
  url          = {https://www.neurotechnology.com/verieye.html},
  year         = {2019},
}

@InProceedings{Trokielewicz_IWBF_2018,
  author    = {M. {Trokielewicz} and A. {Czajka}},
  booktitle = {2018 International Workshop on Biometrics and Forensics (IWBF)},
  title     = {Data-driven segmentation of post-mortem iris images},
  year      = {2018},
  address   = {Sassari, Italy},
  month     = {June},
  pages     = {1-7},
  publisher = {IEEE},
  abstract  = {This paper presents a method for segmenting iris images obtained from the deceased subjects, by training a deep convolutional neural network (DCNN) designed for the purpose of semantic segmentation. Post-mortem iris recognition has recently emerged as an alternative, or additional, method useful in forensic analysis. At the same time it poses many new challenges from the technological standpoint, one of them being the image segmentation stage, which has proven difficult to be reliably executed by conventional iris recognition methods. Our approach is based on the SegNet architecture, fine-tuned with 1,300 manually segmented post-mortem iris images taken from the Warsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented in this paper show that this data-driven solution is able to learn specific deformations present in post-mortem samples, which are missing from alive irises, and offers a considerable improvement over the state-of-the-art, conventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU) metric was improved from 73.6\% (for OSIRIS) to 83\% (for DCNN-based presented in this paper) averaged over subject-disjoint, multiple splits of the data into train and test subsets. This paper offers the first known to us method of automatic processing of post-mortem iris images. We offer source codes with the trained DCNN that perform end-to-end segmentation of post-mortem iris images, as described in this paper. Also, we offer binary masks corresponding to manual segmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database to facilitate development of alternative methods for post-mortem iris segmentation.},
  doi       = {10.1109/IWBF.2018.8401558},
  keywords  = {feature extraction;image classification;image segmentation;iris recognition;neural nets;data-driven segmentation;deep convolutional neural network;semantic segmentation;Post-mortem iris recognition;image segmentation stage;Warsaw-BioBase-Post-Mortem-Iris v;post-mortem iris segmentation;end-to-end segmentation;Image segmentation;Iris recognition;Iris;Training;Forensics;Task analysis;Convolutional neural networks},
  url       = {https://arxiv.org/abs/1807.04154},
}

@Article{Chen_TIFS_2016,
  author   = {J. {Chen} and F. {Shen} and D. Z. {Chen} and P. J. {Flynn}},
  journal  = J_TIFS,
  title    = {Iris Recognition Based on Human-Interpretable Features},
  year     = {2016},
  issn     = {1556-6013},
  month    = {July},
  number   = {7},
  pages    = {1476-1485},
  volume   = {11},
  abstract = {The iris is a stable biometric trait that has been widely used for human recognition in various applications. However, deployment of iris recognition in forensic applications has not been reported. A primary reason is the lack of human-friendly techniques for iris comparison. To further promote the use of iris recognition in forensics, the similarity between irises should be made visualizable and interpretable. Recently, a human-in-the-loop iris recognition system was developed, based on detecting and matching iris crypts. Building on this framework, we propose a new approach for detecting and matching iris crypts automatically. Our detection method is able to capture iris crypts of various sizes. Our matching scheme is designed to handle potential topological changes in the detection of the same crypt in different images. Our approach outperforms the known visible-feature-based iris recognition method on three different data sets. In particular, our approach achieves over 22% higher rank one hit rate in identification, and over 51% lower equal error rate in verification. In addition, the benefit of our approach on multi-enrollment is experimentally demonstrated.},
  doi      = {10.1109/TIFS.2016.2535901},
  keywords = {iris recognition;iris recognition;human-interpretable features;human recognition;human-friendly techniques;human-in-the-loop iris recognition system;matching scheme;visible-feature-based iris recognition method;Iris recognition;Cryptography;Feature extraction;Probes;Algorithm design and analysis;Forensics;Image segmentation;iris recognition;forensics;visible feature;human-in-the-loop;Iris recognition;forensics;visible feature;human-in-the-loop},
}

@Article{Belsey_JFLM_2016,
  author   = {S.L. Belsey and R.J. Flanagan},
  journal  = J_FLM,
  title    = {Postmortem biochemistry: Current applications},
  year     = {2016},
  issn     = {1752-928X},
  pages    = {49 - 57},
  volume   = {41},
  abstract = {The results of biochemical analyses in specimens obtained postmortem may aid death investigation when diabetic and alcoholic ketoacidosis is suspected, when death may have been the result of drowning, anaphylaxis, or involved a prolonged stress response such as hypothermia, and in the diagnosis of disease processes such as inflammation, early myocardial infarction, or sepsis. There is often cross-over with different disciplines, in particular with clinical and forensic toxicology, since some endogenous substances such as sodium chloride, potassium chloride, and insulin can be used as poisons. The interpretation of results is often complicated because of the likelihood of postmortem change in analyte concentration or activity, and proper interpretation must take into account all the available evidence. The unpredictability of postmortem changes means that use of biochemical measurements in time of death estimation has little value. The use of vitreous humour is beneficial for many analytes as the eye is in a physically protected environment, this medium may be less affected by autolysis or microbial metabolism than blood, and the assays can be performed with due precaution using standard clinical chemistry analysers. However, interpretation of results may not be straightforward because (i) defined reference ranges in life are often lacking, (ii) there is a dearth of knowledge regarding, for example, the speed of equilibration of many analytes between blood, vitreous humour, and other fluids that may be sampled, and (iii) the effects of post-mortem change are difficult to quantify because of the lack of control data. A major limitation is that postmortem vitreous glucose measurements are of no help in diagnosing antemortem hypoglycaemia.},
  doi      = {https://doi.org/10.1016/j.jflm.2016.04.011},
  keywords = {Forensic biochemistry, Postmortem, Death investigation, Vitreous humour, Alternative matrices, Ketoacidosis},
  url      = {https://doi.org/10.1016/j.jflm.2016.04.011},
}

@Article{Coe_FSI_1989,
  author   = {John I. Coe},
  journal  = J_FSI,
  title    = {Vitreous potassium as a measure of the postmortem interval: An historical review and critical evaluation},
  year     = {1989},
  issn     = {0379-0738},
  number   = {3},
  pages    = {201 - 213},
  volume   = {42},
  abstract = {This is an historical review of the articles published in English on the use of vitreous potassium to determine the PMI. External factors which influence the validity of the test are sampling techniques, analytical instrumentation and environmental temperature during the PMI. Internal factors that are recognized at the present time which influence vitreous potassium are the age of the individual, the duration of the terminal episode and the presence or absence of nitrogen retention.},
  doi      = {10.1016/0379-0738(89)90087-X},
  keywords = {Vitreous potassium, Postmortem interval, Time of death},
  url      = {https://doi.org/10.1016/0379-0738(89)90087-X},
}

@Article{Jaafar_FSI_1994,
  author   = {S. Jaafar and L.D.M. Nokes},
  journal  = {Forensic Science International},
  title    = {Examination of the eye as a means to determine the early postmortem period: A review of the literature},
  year     = {1994},
  issn     = {0379-0738},
  number   = {2},
  pages    = {185 - 189},
  volume   = {64},
  abstract = {Reported are various techniques to determine the early postmortem period by examining the eye. These include corneal opacity, retinal vessel segmentation, pupil reaction, retinal changes and intraocular pressure. All are subjective, requiring experience to implement the techniques.},
  doi      = {10.1016/0379-0738(94)90230-5},
  keywords = {Literature survey, Time of death, Eye},
  url      = {https://doi.org/10.1016/0379-0738(94)90230-5},
}

@Article{Weichen_FSM_2018,
  author   = {Li, Weichen and Chang, Yunfeng and Cheng, Zijia and Ling, Jiang and Han, Leiming and Li, Xingmei and Ding, Yanjun.},
  journal  = J_FSM,
  title    = {{Vitreous humor: A review of biochemical constituents in postmortem interval estimation}},
  year     = {2018},
  number   = {2},
  pages    = {85-90},
  volume   = {4},
  abstract = {Postmortem changes in the biochemical constituents of the vitreous humor have been widely used to estimate the postmortem interval (PMI) over the past several decades. However, few reviews have summarized the relationship between the postmortem vitreous biochemical constituents and time of death. Herein, the relationship between PMI and single biochemical components, including vitreous potassium, hypoxanthine, and amino acids, as well as comparisons of each statistical parameter in the formula, is summarized. We also discuss other compounds such as urea and uric acid, which have no direct relationship with PMI. Utility of multiple constituent simultaneous analysis for estimating PMI is being increasingly investigated. The promising idea of using simultaneous analysis of multiple constituents to determine PMI is proposed as a future research direction.},
  doi      = {10.4103/jfsm.jfsm_13_18},
  eprint   = {http://www.jfsmonline.com/article.asp?issn=2349-5014;year=2018;volume=4;issue=2;spage=85;epage=90;aulast=Li;t=6},
  url      = {http://www.jfsmonline.com/article.asp?issn=2349-5014;year=2018;volume=4;issue=2;spage=85;epage=90;aulast=Li;t=6},
}

@Article{Kusum_FS_2010,
  author  = {Jashnani, Kusum and Kale, Smita and Rupani, Asha},
  journal = J_FS,
  title   = {Vitreous Humor: Biochemical Constituents in Estimation of Postmortem Interval},
  year    = {2010},
  month   = {11},
  pages   = {1523-7},
  volume  = {55},
  doi     = {10.1111/j.1556-4029.2010.01501.x},
}

@Article{Madea_FSI_2005,
  author  = {Madea, Burkhard},
  journal = J_FSI,
  title   = {Is there recent progress in the estimation of the postmortem interval by means of thanatochemistry?},
  year    = {2005},
  month   = {08},
  pages   = {139-49},
  volume  = {151},
  doi     = {10.1016/j.forsciint.2005.01.013},
}

@Article{Brooks_VP_2016,
  author   = {J. W. Brooks},
  journal  = J_VP,
  title    = {Postmortem Changes in Animal Carcasses and Estimation of the Postmortem Interval},
  year     = {2016},
  note     = {PMID: 26945004},
  number   = {5},
  pages    = {929-940},
  volume   = {53},
  abstract = {A thorough understanding of the physical and chemical changes that occur in the body after death is critical for accurate interpretation of gross and microscopic pathology at autopsy. Furthermore, knowledge of the postmortem processes and the factors that affect them will aid in the estimation of the postmortem interval (PMI). The estimation of the PMI is important in many human and animal death investigations. Despite many decades of research, accuracy in estimation of the time of death has not significantly improved, and no single method can be reliably used to accurately estimate the time of death. Great care should be taken when formulating such an estimate, for it is dependent on multiple circumstantial and environmental factors, and the accuracy and precision of the estimate decrease as the PMI increases. The majority of the research in the field has been conducted on human bodies, but many relevant conclusions may be drawn regarding the expected postmortem changes in animals and the estimation of the PMI. The veterinary pathologist must use great caution when attempting to extrapolate data and apply formulas designed for use in humans. Methods reviewed include gross changes, microscopic changes, temperature-based methods, postmortem chemistry, molecular methods, microbial assay, ocular changes, radiography, entomology, and others. Although only several of these methods are currently practical for use in the workup of cases, it is expected that future research will result in improved techniques with enhanced accuracy in the estimation of the PMI, which will benefit both human and veterinary forensic investigations.},
  doi      = {10.1177/0300985816629720},
  eprint   = {https://doi.org/10.1177/0300985816629720},
  url      = {https://doi.org/10.1177/0300985816629720},
}

@Article{Bevalot_FT_2015,
  author  = {B\'{e}valot, Fabien and Cartiser, Nathalie and Charline, Bottinelli and Fanton, Laurent and Guitton, J\'{e}r\^{o}me},
  journal = J_FT,
  title   = {Vitreous humor analysis for the detection of xenobiotics in forensic toxicology: a review},
  year    = {2016},
  month   = {10},
  pages   = {12--40},
  volume  = {34},
  doi     = {10.1007/s11419-015-0294-5},
}

@Article{Canturk_CBM_2018,
  author  = {Cant\"{u}rk, Ismail and \"{O}zyilmaz, Lale},
  journal = J_CBM,
  title   = {A computational approach to estimate postmortem interval using opacity development of eye for human subjects},
  year    = {2018},
  month   = {05},
  pages   = {93-99},
  volume  = {98},
  doi     = {10.1016/j.compbiomed.2018.04.023},
}

@Article{Fleischer_FS_2017,
  author   = {Fleischer, Luise and Sehner, Susanne and Gehl, Axel and Riemer, Martin and Raupach, Tobias and Anders, Sven},
  journal  = J_FS,
  title    = {Measurement of Postmortem Pupil Size: A New Method with Excellent Reliability and Its Application to Pupil Changes in the Early Postmortem Period},
  year     = {2017},
  number   = {3},
  pages    = {791-795},
  volume   = {62},
  abstract = {Abstract Measurement of postmortem pupil width is a potential component of death time estimation. However, no standardized measurement method has been described. We analyzed a total of 71 digital images for pupil–iris ratio using the software ImageJ. Images were analyzed three times by four different examiners. In addition, serial images from 10 cases were taken between 2 and 50 h postmortem to detect spontaneous pupil changes. Intra- and inter-rater reliability of the method was excellent (ICC > 0.95). The method is observer independent and yields consistent results, and images can be digitally stored and re-evaluated. The method seems highly eligible for forensic and scientific purposes. While statistical analysis of spontaneous pupil changes revealed a significant polynomial of quartic degree for postmortem time (p = 0.001), an obvious pattern was not detected. These results do not indicate suitability of spontaneous pupil changes for forensic death time estimation, as formerly suggested.},
  doi      = {10.1111/1556-4029.13318},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1556-4029.13318},
  keywords = {forensic science, forensic pathology, forensic medicine, postmortem diagnosis, postmortem changes, legal medicine},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1556-4029.13318},
}

@Article{Kaliszan_LM_2013,
  author   = {Michał Kaliszan},
  journal  = J_LM,
  title    = {Studies on time of death estimation in the early post mortem period – Application of a method based on eyeball temperature measurement to human bodies},
  year     = {2013},
  issn     = {1344-6223},
  number   = {5},
  pages    = {278 - 282},
  volume   = {15},
  abstract = {This paper presents a verification of the thermodynamic model allowing an estimation of the time of death (TOD) by calculating the post mortem interval (PMI) based on a single eyeball temperature measurement at the death scene. The study was performed on 30 cases with known PMI, ranging from 1h 35min to 5h 15min, using pin probes connected to a high precision electronic thermometer (Dostmann-electronic). The measured eye temperatures ranged from 20.2 to 33.1°C. Rectal temperature was measured at the same time and ranged from 32.8 to 37.4°C. Ambient temperatures which ranged from −1 to 24°C, environmental conditions (still air to light wind) and the amount of hair on the head were also recorded every time. PMI was calculated using a formula based on Newton’s law of cooling, previously derived and successfully tested in comprehensive studies on pigs and a few human cases. Thanks to both the significantly faster post mortem decrease of eye temperature and a residual or nonexistent plateau effect in the eye, as well as practically no influence of body mass, TOD in the human death cases could be estimated with good accuracy. The highest TOD estimation error during the post mortem intervals up to around 5h was 1h 16min, 1h 14min and 1h 03min, respectively in three cases among 30, while for the remaining 27 cases it was not more than 47min. The mean error for all 30 cases was ±31min. All that indicates that the proposed method is of quite good precision in the early post mortem period, with an accuracy of ±1h for a 95% confidence interval. On the basis of the presented method, TOD can be also calculated at the death scene with the use of a proposed portable electronic device (TOD-meter).},
  doi      = {10.1016/j.legalmed.2013.06.003},
  keywords = {Time of death (TOD), Post mortem interval (PMI), Post mortem body cooling, Eyeball temperature, Rectal temperature, Plateau effect, TOD-meter},
  url      = {https://doi.org/10.1016/j.legalmed.2013.06.003},
}

@Article{Napoli_SR_2016,
  author  = {Napoli, Pietro and Nioi, Matteo and d'Aloja, Ernesto and Fossarello, Maurizio},
  journal = J_SR,
  title   = {Post-Mortem Corneal Thickness Measurements with a Portable Optical Coherence Tomography System: A Reliability Study},
  year    = {2016},
  month   = {07},
  pages   = {30428},
  volume  = {6},
  doi     = {10.1038/srep30428},
}

@Article{PrietoBonete_LM_2015,
  author   = {Gemma Prieto-Bonete and Maria D. Perez-Carceles and Aurelio Luna},
  journal  = J_LM,
  title    = {Morphological and histological changes in eye lens: Possible application for estimating postmortem interval},
  year     = {2015},
  issn     = {1344-6223},
  number   = {6},
  pages    = {437 - 442},
  volume   = {17},
  abstract = {Establishing the postmortem interval is a very complex problem in Forensic Science despite the existence of several macro- and microscopic methods. In the case of ocular methods, most are based on an evaluation of the biochemical components of the vitreous humour 24–36h after death, but, to our knowledge, there are no studies on the relationship between lens and the postmortem interval. Since the lens is protected between the vitreous humour and the aqueous humour inside the eyeball, postmortem changes are assumed to start later in the lens. To evaluate the usefulness of using the lens to establish the postmortem interval, we examined 80 rabbit lens enucleated 24, 48, 72 and 96h after death, assessing changes in sphericity and absorbance at different wavelengths and any histological alterations. Both sphericity and absorbance were seen to decrease to a statistically significant extent, and there was a gradual loss of structure and organisation of the lens components as a function of the postmortem interval. Modifications in the lens were seen to be useful for determining the postmortem interval between 24 and 96h.},
  doi      = {10.1016/j.legalmed.2015.09.002},
  keywords = {Postmortem interval, Eye lens, Corneal transparency, Proteins, Histology},
  url      = {https://doi.org/10.1016/j.legalmed.2015.09.002},
}

@Article{Tsunenari_FS_1978,
  author   = {Shigeyuki Tsunenari and Mizuho Kanda},
  journal  = J_FS,
  title    = {The roles of corneal mucopolysaccharides and water contents in post-mortem corneal clouding},
  year     = {1978},
  issn     = {0300-9432},
  number   = {1},
  pages    = {87 - 92},
  volume   = {11},
  abstract = {The quantitative changes in mucopolysaccharides (MPS) and water contents in post-mortem rabbit corneas were studied. On the basis of the results, the post-mortem periods were classified into 4 stages. The first stage was during the first day after death when the MPS and the corneal water contents were observed to have maintained their initial values and the cornea was “clear”. The second was 1 to 3 days after death when the MPS remained at approximately their initial value, whereas the water contents were very pronounced. In this second stage the cornea was “a little clear” or “cloudy”. The third was 3 to 6 days after death when the MPS were found to have decreased gradually, but the water contents maintained their high level. The cornea in this third stage was “cloudy” or “very cloudy”. The fourth was 6 or more days after death when the two parameters were found to have decreased significantly, and the cornea was “very cloudy”.},
  doi      = {10.1016/0379-0738(78)90101-9},
  url      = {https://doi.org/10.1016/0379-0738(78)90101-9},
}

@Article{Koehler_IJLM_2018,
  author   = {Koehler, Katja and Sehner, Susanne and Riemer, Martin and Gehl, Axel and Raupach, Tobias and Anders, Sven},
  journal  = J_IJLM,
  title    = {Post-mortem chemical excitability of the iris should not be used for forensic death time diagnosis},
  year     = {2018},
  issn     = {1437-1596},
  month    = {Nov},
  number   = {6},
  pages    = {1693--1697},
  volume   = {132},
  abstract = {Post-mortem chemical excitability of the iris is one of the non-temperature-based methods in forensic diagnosis of the time since death. Although several authors reported on their findings, using different measurement methods, currently used time limits are based on a single dissertation which has recently been doubted to be applicable for forensic purpose. We investigated changes in pupil-iris ratio after application of acetylcholine (n{\thinspace}={\thinspace}79) or tropicamide (n{\thinspace}={\thinspace}58) and in controls at upper and lower time limits that are suggested in the current literature, using a digital photography-based measurement method with excellent reliability. We observed ``positive,'' ``negative,'' and ``paradox'' reactions in both intervention and control conditions at all investigated post-mortem time points, suggesting spontaneous changes in pupil size to be causative for the finding. According to our observations, post-mortem chemical excitability of the iris should not be used in forensic death time estimation, as results may cause false conclusions regarding the correct time point of death and might therefore be strongly misleading.},
  day      = {01},
  doi      = {10.1007/s00414-018-1846-0},
  url      = {https://doi.org/10.1007/s00414-018-1846-0},
}

@Article{Poposka_Makedonska_2011,
  author  = {Poposka, Verica and Janeska, Biljana and Gutevska, A and Duma, A},
  journal = {Prilozi / Makedonska akademija na naukite i umetnostite, Oddelenie za biolo\v{s}ki i medicinski nauki = Contributions / Macedonian Academy of Sciences and Arts, Section of Biological and Medical Sciences},
  title   = {Estimation of time since death through electric and chemical excitability of muscles},
  year    = {2011},
  month   = {07},
  pages   = {211-8},
  volume  = {32},
}

@Article{Zhou_HUST_2010,
  author   = {Zhou, Lan and Liu, Yan and Liu, Liang and Zhuo, Luo and Liang, Man and Yang, Fan and Ren, Liang and Zhu, Shaohua},
  journal  = {Journal of Huazhong Univ. of Science and Technology [Medical Sciences]},
  title    = {Image analysis on corneal opacity: A novel method to estimate postmortem interval in rabbits},
  year     = {2010},
  issn     = {1993-1352},
  month    = {Apr},
  number   = {2},
  pages    = {235--239},
  volume   = {30},
  abstract = {Corneal opacity is one of the most commonly used parameters for estimating postmortem interval (PMI). This paper proposes a new method to study the relationship between changes of corneal opacity and PMI by processing and analyzing cornea images. Corneal regions were extracted from images of rabbits' eyes and described by color-based and texture-based features, which could represent the changes of cornea at different PMI. A KNN classifier was used to reveal the association of image features and PMI. The result of the classification showed that the new method was reliable and effective.},
  day      = {01},
  doi      = {10.1007/s11596-010-0221-2},
  url      = {https://doi.org/10.1007/s11596-010-0221-2},
}


High-level, overview (invited) paper: Adam Czajka, Mateusz Trokielewicz, Piotr Maciejewicz, “The Eyes have it: New iris recognition techniques can tell whether an Eye is healthy, diseased, or dead,” in IEEE Spectrum, Vol. 56, No. 09, pp. 44-49, Sept. 2019, DOi: 10.1109/MSPEC.2019.8818591

Research databases available to non-commercial entities: http://zbum.ia.pw.edu.pl/EN/node/46 ("Warsaw-BioBase-Disease-Iris v1.0" and "Warsaw-BioBase-Disease-Iris v2.1")

Research papers:
Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz, “Implications of Ocular Pathologies for Iris Recognition Reliability,” Image and Vision Computing, Vol. 58, pp. 158–167, Elsevier, 2016, DOI: 10.1016/j.imavis.2016.08.001; pre-print: https://arxiv.org/abs/1809.00168

Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz, “Iris Recognition in Cases of Eye Pathology,” in: Amine Nait-Ali (Ed.), Biometrics under Biomedical Considerations, pp. 41-69, Series in BioEngineering, Springer, 2019, DOI: 10.1007/978-981-13-1144-4 (invited); pre-print: https://arxiv.org/abs/1809.01040 

Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz, “Assessment of iris recognition reliability for eyes affected by ocular pathologies,” The 7th IEEE Int. Conference on Biometrics: Theory, Applications and Systems (BTAS), pp. 1–6, September 8–11, 2015, Arlington, USA, DOI: 10.1109/BTAS.2015.7358747 (best paper award); pre-print: http://zbum.ia.pw.edu.pl/PAPERS/BTAS2015_Trokielewicz_Czajka_Maciejewicz.pdf

Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz, “Database of iris images acquired in the presence of ocular pathologies and assessment of iris recognition reliability for disease-affected eyes,” The 2nd IEEE Int. Conference on Cybernetics (CYBCONF 2015), Special Session on Reliable Biometrics (BIORELIABILITY), Gdynia, Poland, pp. 495–500, June 24–26, 2015, DOI: 10.1109/CYBConf.2015.7175984; pre-print: https://arxiv.org/abs/1809.00212






@Book{Galton_1888,
  author    = {Francis Galton},
  publisher = {MacMillan and Co.},
  title     = {{Finger Prints}},
  year      = {1892},
  address   = {London and New York},
}

@Book{Knight_Book_1997,
  author    = {Bernard Knight and Keith Simpson},
  publisher = {Edward Arnold},
  title     = {Simpson's Forensic Medicine},
  year      = {1997},
  address   = {London, UK},
}

@Book{Saukko_Book_2004,
  author    = {Pekka Saukko and Bernard Knight},
  publisher = {CRC Press, Taylor \& Francis Group},
  title     = {Knight's Forensic Pathology, 3rd Edition},
  year      = {2004},
  address   = {Boca Raton, FL, USA},
}

@Article{Abraham_FS_2008,
  author   = {Abraham, Elizabeth and Cox, Margaret and Quincey, David},
  journal  = {Journal of Forensic Sciences},
  title    = {Pig-mentation: Postmortem Iris Color Change in the Eyes of Sus scrofa*},
  year     = {2008},
  number   = {3},
  pages    = {626-631},
  volume   = {53},
  abstract = {Abstract:  Experienced forensic pathologists and examiners may be familiar with the phenomenon of postmortem iris color change; however, only Knight, Simpson’s forensic medicine, Arnold, London, 1997; Ref. 1 and Saukko and Knight, Knight’s forensic pathology, 3rd ed., Arnold, London, 2004; Ref. 2 have referred to it in the literature, and to date, there have been no published scientific research studies on this taphonomic artifact. A controlled experiment was conducted of postmortem changes to isolated Sus scrofa eyes. The eyes (n = 137) were separated into three groups and each sample was observed for 3-day postmortem at a different temperature. In addition, a Sus scrofa head was obtained to observe postmortem changes of eyes in situ. All isolated blue eyes in the experiment, at room temperature and higher, changed to brown/black within 48 h. The in situ blue eye, at room temperature, turned brown/black within 72 h. If iris color consistently changes postmortem in humans, then this taphonomic artifact must be incorporated into victim identification protocol, including disaster victim identification software, and autopsy reports to prevent inaccurate victim identification and inappropriate exclusion from the identification process.},
  doi      = {10.1111/j.1556-4029.2008.00729.x},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1556-4029.2008.00729.x},
  keywords = {forensic science, postmortem, iris color change, eye color change, victim identification, forensic taphonomy},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1556-4029.2008.00729.x},
}

@Book{Dolinak_Book_2005,
  author    = {David Dolinak and Evan Matshes and Emma Lew},
  publisher = {Elsevier Academic Science},
  title     = {Forensic Pathology, 1st Edition},
  year      = {2005},
  address   = {Burlington, San Diego, London},
  isbn      = {9780080470665},
}

@Article{Wilkinson2014,
  author    = {Wilkinson, Caroline},
  journal   = {Forensic Science International},
  title     = {A review of the changing culture and social context relating to forensic facial depiction of the dead},
  year      = {2014},
  pages     = {95--100},
  volume    = {245},
  publisher = {Elsevier},
}

@Article{Caplova2018,
  author    = {Caplova, Zuzana and Gibelli, Daniele Maria and Poppa, Pasquale and Cummaudo, Marco and Obertova, Zuzana and Sforza, Chiarella and Cattaneo, Cristina},
  journal   = {International journal of legal medicine},
  title     = {3D quantitative analysis of early decomposition changes of the human face},
  year      = {2018},
  number    = {2},
  pages     = {649--653},
  volume    = {132},
  publisher = {Springer},
}

@Article{Lain2003,
  author  = {Lain, Russell and Griffiths, Chris and Hilton, John MN},
  journal = {Medical Journal of Australia},
  title   = {Forensic dental and medical response to the Bali bombing},
  year    = {2003},
  number  = {7},
  pages   = {362--365},
  volume  = {179},
}

@Book{Mulawka2014,
  author    = {Mulawka, Marzena},
  publisher = {Routledge},
  title     = {Postmortem Fingerprinting and Unidentified Human Remains},
  year      = {2014},
}

@Article{Fields2008,
  author    = {Fields, Roy and Molina, D Kimberley},
  journal   = {Journal of Forensic Sciences},
  title     = {A Novel Approach for Fingerprinting Mummified Hands},
  year      = {2008},
  number    = {4},
  pages     = {952--955},
  volume    = {53},
  publisher = {Wiley Online Library},
}

@InProceedings{Rajeev2016,
  author       = {Srijith Rajeev and Shreyas Kamath K.M. and Sos S. Agaian},
  booktitle    = {Mobile Multimedia/Image Processing, Security, and Applications},
  title        = {{Method for Modeling Post-Mortem Biometric 3D Fingerprints}},
  year         = {2016},
  editor       = {Sos S. Agaian and Sabah A. Jassim},
  organization = {International Society for Optics and Photonics},
  pages        = {159 -- 168},
  publisher    = {SPIE},
  volume       = {9869},
  doi          = {10.1117/12.2224386},
  keywords     = {Biometric, 3-D, Fingerprint, Post-moterm, Unrolling, Non-parametric, Verification, Deformed},
  url          = {https://doi.org/10.1117/12.2224386},
}

@Article{Panetta2019,
  author    = {Panetta, Karen and Rajeev, Srijith and Kamath, KM Shreyas and Agaian, Sos S},
  journal   = {IEEE Access},
  title     = {Unrolling post-mortem 3D fingerprints using mosaicking pressure simulation technique},
  year      = {2019},
  pages     = {88174--88185},
  volume    = {7},
  publisher = {IEEE},
}


@InCollection{Wilkinson2012,
  author    = {Wilkinson, Caroline and Tillotson, Amy},
  booktitle = {Craniofacial Identification},
  publisher = {Cambridge University Press},
  title     = {Post-mortem prediction of facial appearance},
  year      = {2012},
  pages     = {166--183},
}

@Article{daugman2009,
  author    = {Daugman, John},
  title     = {How iris recognition works},
  year      = {2009},
  pages     = {715--739},
  booktitle = {The essential guide to image processing},
  publisher = {Elsevier},
}

@InProceedings{bobeldyk2018,
  author    = {Bobeldyk, Denton and Ross, Arun},
  booktitle = C_ICB,
  title     = {Predicting Eye Color from Near Infrared Iris Images},
  year      = {2018},
  doi       = {10.1109/icb2018.2018.00026},
}

  
@Article{Aulsebrook1995,
  author  = {W.A. Aulsebrook and M.Y. {\.I}{\c{s}}can and J.H. Slabbert and P. Becker},
  journal = {Forensic Science International},
  title   = {Superimposition and reconstruction in forensic facial identification: a survey},
  year    = {1995},
  issn    = {0379-0738},
  number  = {2},
  pages   = {101 - 120},
  volume  = {75},
  doi     = {https://doi.org/10.1016/0379-0738(95)01770-4},
  url     = {http://www.sciencedirect.com/science/article/pii/0379073895017704},
}


@InProceedings{Cornett2019,
  author    = {Cornett {III}, David and Bolme, David and Steadman, Dawnie W and Sauerwein, Kelly A and Saul, Tiffany B},
  booktitle = C_BTAS,
  title     = {{Effects of Postmortem Decomposition on Face Recognition}},
  year      = {2019},
  address   = {Tampa, Florida, USA},
}

@Article{BTT_2015,
  journal  = {Biometric Technology Today},
  title    = {{Ghana, Tanzania and Somaliland} introduce biometric voter verification},
  year     = {2015},
  issn     = {0969-4765},
  number   = {10},
  pages    = {3 - 12},
  volume   = {2015},
  abstract = {GenKey's biometric voter verification solution has been deployed at the District Level Elections in Ghana, which took place on 1 September 2015. The solution was delivered in cooperation with local partner STL. The Electoral Commission (EC) of Ghana had selected STL/GenKey's solution to verify the identity of eligible voters on Election Day.},
  doi      = {10.1016/S0969-4765(15)30151-X},
  editor   = {Elsevier},
  url      = {https://doi.org/10.1016/S0969-4765(15)30151-X},
}

@Article{RichardWebster_TPAMI_2018,
  author  = {B. {Richard Webster} and S. E. {Anthony} and W. J. {Scheirer}},
  journal = {{IEEE Transactions on Pattern Analysis and Machine Intelligence}},
  title   = {PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition},
  year    = {2019},
  number  = {9},
  pages   = {2280-2286},
  volume  = {41},
  doi     = {10.1109/TPAMI.2018.2849989},
}

@InProceedings{RichardWebster_ECCV_2018,
  author    = {Brandon RichardWebster and So Yon Kwon and Christopher Clarizio and Samuel E. Anthony and W. Scheirer},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title     = {Visual Psychophysics for Making Face Recognition Algorithms More Explainable},
  year      = {2018},
  pages     = {263-281},
}


@Article{Li_JV_2007,
  author  = {Fei-Fei Li and Asha Iyer and Christof Koch and Pietro Perona},
  journal = {Journal of Vision},
  title   = {What do we perceive in a glance of a real-world scene?},
  year    = {2007},
  number  = {10},
  volume  = {7},
  doi     = {10.1167/7.1.10},
}

@Misc{AADHAAR,
  author = {{Unique Identification Authority of India}},
  title  = {{AADHAAR:} https://uidai.gov.in},
  year   = {accessed on July 8, 2021},
}




@Misc{NEXUSjoint,
  author = {{NEXUS: Joint USA and Canada Trusted Traveler Program}},
  title  = {{US official site:} https://www.cbp.gov/travel/trusted-traveler-programs/nexus; {Canada} official site: http://www.nexus.gc.ca},
  year   = {accessed on August 13, 2016},
}

@Article{balas2003,
  author  = {Balas, Benjamin J and Sinha, Pawan},
  journal = {Journal of Vision},
  title   = {{STICKS: Image-representation via non-local comparisons}},
  year    = {2003},
  pages   = {12--12},
  volume  = {3},
  issue   = {9},
}

@InProceedings{raja2014,
  author       = {Raja, Kiran B and Raghavendra, Ramachandra and Busch, Christoph},
  booktitle    = {IEEE 2nd International Workshop on Biometrics and Forensics},
  title        = {Binarized Statistical Features for Improved Iris and Periocular Recognition in Visible Spectrum},
  year         = {2014},
  address      = {Valletta, Malta},
  organization = {IEEE},
  pages        = {1--6},
  publisher    = {IEEE},
}

@InProceedings{raja2013,
  author    = {Raja, Kiran B and Raghavendra, Ramachandra and Cheikh, Faouzi Alaya and Yang, Bian and Busch, Christoph},
  booktitle = {IEEE Colour and Visual Computing Symposium (CVCS)},
  title     = {Robust Iris Recognition using Light-field Camera},
  year      = {2013},
  pages     = {1--6},
}

@InProceedings{gangwar2016,
  author    = {Abhishek Gangwar and Akanksha Joshi},
  booktitle = {2016 IEEE International Conference on Image Processing (ICIP)},
  title     = {DeepIrisNet: Deep iris representation with applications in iris recognition and cross-sensor iris recognition},
  year      = {2016},
  address   = {Phoenix, AZ, USA},
  month     = {Sep.},
  pages     = {2301-2305},
  publisher = {IEEE},
  doi       = {10.1109/ICIP.2016.7532769},
  issn      = {2381-8549},
  keywords  = {image representation;iris recognition;learning (artificial intelligence);neural nets;DeepIrisNet;deep iris representation;cross-sensor iris recognition;robust IR;deep convolution neural networks;DCNN;visual models;deep learning-based method;iris representation;CNN;iris microstructures;cross-sensor IR;Iris recognition;Computer architecture;Convolution;Training;Iris;Robustness;Databases;CNN;iris recognition;cross-sensor iris recognition;deep iris representation;deep learning},
}

@InCollection{jalilian2017,
  author    = {Jalilian, Ehsaneddin and Uhl, Andreas},
  booktitle = {Deep Learning for Biometrics},
  publisher = {Springer},
  title     = {Iris segmentation using fully convolutional encoder--decoder networks},
  year      = {2017},
  address   = {Salzburg, Austria},
  pages     = {133--155},
}

@Article{Singh2019,
  author    = {Singh, Maneet and Singh, Richa and Ross, Arun},
  journal   = {Information Fusion},
  title     = {A comprehensive overview of biometric fusion},
  year      = {2019},
  pages     = {187--205},
  volume    = {52},
  publisher = {Elsevier},
}

@InProceedings{Trokielewicz_WACV2020,
  author    = {{Trokielewicz}, Mateusz and {Czajka}, Adam and {Maciejewicz}, Piotr},
  booktitle = C_WACV,
  title     = {{Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes}},
  year      = {2020},
  address   = {Aspen, CO, USA},
  month     = {March},
  pages     = {1-8},
  publisher = {IEEE},
  url       = {https://arxiv.org/abs/1912.02512},
}

@InProceedings{Ross_ICB_2019,
  author    = {Arun Ross and Sudipta Banerjee and Cunjian Chen and Anurag Chowdhury and Vahid Mirjalili and Renu Sharma and Thomas Swearingen and Shivangi Yadav},
  booktitle = C_ICB,
  title     = {Some Research Problems in Biometrics: The Future Beckons},
  year      = {2019},
  address   = {Crete, Greece},
  month     = {June},
  pages     = {1-6},
  publisher = {IEEE},
  abstract  = {The need for reliably determining the identity of a per- son is critical in a number of different domains ranging from personal smartphones to border security; from au- tonomous vehicles to e-voting; from tracking child vacci- nations to preventing human trafficking; from crime scene investigation to personalization of customer service. Bio- metrics, which entails the use of biological attributes such as face, fingerprints and voice for recognizing a person, is being increasingly used in several such applications. While biometric technology has made rapid strides over the past decade, there are several fundamental issues that are yet to be satisfactorily resolved. In this article, we will discuss some of these issues and enumerate some of the exciting challenges in this field.},
}

@Article{Nguyen_PR_2017,
  author   = {Kien Nguyen and Clinton Fookes and Raghavender Jillela and Sridha Sridharan and Arun Ross},
  journal  = J_PR,
  title    = {Long range iris recognition: A survey},
  year     = {2017},
  issn     = {0031-3203},
  pages    = {123 - 143},
  volume   = {72},
  abstract = {The term “iris” refers to the highly textured annular portion of the human eye that is externally visible. An iris recognition system exploits the richness of these textural patterns to distinguish individuals. Iris recognition systems are being used in a number of human recognition applications such as access control, national ID schemes, border control, etc. To capture the rich textural information of the iris pattern regardless of the eye color, traditional iris recognition systems utilize near-infrared (NIR) sensors to acquire images of the iris. This, however, restricts the iris image acquisition distance to close quarters (less than 1 m). Over the last several years, there have been numerous attempts to design and implement iris recognition systems that operate at longer standoff distances ranging from 1 m to 60 m. Such long range iris acquisition and recognition systems can provide high user convenience and improved throughput. This paper reviews the state-of-the-art design and implementation of iris-recognition-at-a-distance (IAAD) systems. In this regard, the design of such a system from both the image acquisition (hardware) and image processing (algorithms) perspectives are presented. The major contributions of this paper include: (1) discussing the significance and applications of IAAD systems in the context of human recognition, (2) providing a review of existing IAAD systems, (3) presenting a complete solution to the design problem of an IAAD system, from both hardware and algorithmic perspectives, (4) discussing the use of additional ocular information, along with iris, for improving IAAD accuracy, and (5) discussing the current research challenges and providing recommendations for future research in IAAD.},
  doi      = {10.1016/j.patcog.2017.05.021},
  keywords = {Biometrics, Iris recognition, Long range iris recognition, Iris recognition at a distance, Stand-off iris recognition, Non-ideal iris recognition},
  url      = {https://doi.org/10.1016/j.patcog.2017.05.021},
}

@InProceedings{Burge_SPIE_2009,
  author       = {Mark J. Burge and Matthew K. Monaco},
  booktitle    = {Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XV},
  title        = {{Multispectral iris fusion for enhancement, interoperability, and cross wavelength matching}},
  year         = {2009},
  address      = {Orlando, Florida, United States},
  editor       = {Sylvia S. Shen and Paul E. Lewis},
  organization = {International Society for Optics and Photonics},
  pages        = {494 -- 501},
  publisher    = {SPIE},
  volume       = {7334},
  abstract     = {Traditionally, only a narrow band of the Near-Infrared (NIR) spectrum (700-900nm) is utilized for iris recognition
since this alleviates any physical discomfort from illumination, reduces specular reflections and increases the
amount of texture captured for some iris colors. However, previous research has shown that matching performance
is not invariant to iris color and can be improved by imaging outside of the NIR spectrum. Building on this
research, we demonstrate that iris texture increases with the frequency of the illumination for lighter colored
sections of the iris and decreases for darker sections. Using registered visible light and NIR iris images captured
using a single-lens multispectral camera, we illustrate how physiological properties of the iris (e.g., the amount
and distribution of melanin) impact the transmission, absorbance, and reflectance of different portions of the
electromagnetic spectrum and consequently affect the quality of the imaged iris texture. We introduce a novel
iris code, Multispectral Enhanced irisCode (MEC), which uses pixel-level fusion algorithms to exploit texture
variations elicited by illuminating the iris at different frequencies, to improve iris matcher performance and
reduce Failure-To-Enroll (FTE) rates. Finally, we present a model for approximating an NIR iris image using
features derived from the color and structure of a visible light iris image. The simulated NIR images generated
by this model are designed to improve the interoperability between legacy NIR iris images and those acquired
under visible light by enabling cross wavelength matching of NIR and visible light iris images.},
  doi          = {10.1117/12.819058},
  keywords     = {cross wavelength matching, iris biometrics, ocular biometrics, multispectral image processing, iris quality},
  url          = {https://doi.org/10.1117/12.819058},
}

@InProceedings{Webb_ACSSC_2016,
  author    = {J. {Webb} and D. M. {Etter} and V. {Barboza} and E. {Sharp}},
  booktitle = {2016 50th Asilomar Conference on Signals, Systems and Computers},
  title     = {Iris recognition using cross-spectral comparison},
  year      = {2016},
  address   = {Pacific Grove, CA, USA},
  month     = {Nov},
  pages     = {434-438},
  publisher = {IEEE},
  abstract  = {The SMU Biometrics Engineering Research Group recently completed a four-year data collection project that collected iris images from a large number of subjects with LED illumination at different wavelengths ranging from 405 nanometers to 1550 nanometers [1]. This dataset contains largely high quality images, and all images with any quality issues are marked. For this project, we used only good quality images along with government-provided segmentation and template algorithms to examine how images taken with different illumination compare with each other.},
  doi       = {10.1109/ACSSC.2016.7869076},
  issn      = {null},
  keywords  = {image segmentation;iris recognition;lighting;iris recognition;cross-spectral comparison;SMU biometrics engineering research group;LED illumination;high quality images;government-provided segmentation;template algorithms;High definition video;Iris recognition;Heating;Histograms;Lighting;Cameras;Hamming distance},
}



@Article{Trokielewicz_IMAVIS_2020,
  author   = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  journal  = {Image and Vision Computing},
  title    = {Post-mortem iris recognition with deep-learning-based image segmentation},
  year     = {2020},
  issn     = {0262-8856},
  pages    = {103866},
  volume   = {94},
  abstract = {This paper proposes the first known to us iris recognition methodology designed specifically for post-mortem samples. We propose to use deep learning-based iris segmentation models to extract highly irregular iris texture areas in post-mortem iris images. We show how to use segmentation masks predicted by neural networks in conventional, Gabor-based iris recognition method, which employs circular approximations of the pupillary and limbic iris boundaries. As a whole, this method allows for a significant improvement in post-mortem iris recognition accuracy over the methods designed only for ante-mortem irises, including the academic OSIRIS and commercial IriCore implementations. The proposed method reaches the EER less than 1\% for samples collected up to 10 hours after death, when compared to 16.89\% and 5.37\% of EER observed for OSIRIS and IriCore, respectively. For samples collected up to 369 h post-mortem, the proposed method achieves the EER 21.45%, \while 33.59\% and 25.38\% are observed for OSIRIS and IriCore, respectively. Additionally, the method is tested on a database of iris images collected from ophthalmology clinic patients, for which it also offers an advantage over the two other algorithms. This work is the first step towards post-mortem-specific iris recognition, which increases the chances of identification of deceased subjects in forensic investigations. The new database of post-mortem iris images acquired from 42 subjects, as well as the deep learning-based segmentation models are made available along with the paper, to ensure all the results presented in this manuscript are reproducible.},
  doi      = {10.1016/j.imavis.2019.103866},
  keywords = {Biometrics, Iris recognition, Post-mortem, Image segmentation},
  url      = {https://doi.org/10.1016/j.imavis.2019.103866},
}

@InProceedings{Tomeo2016,
  author    = {Tomeo-Reyes, Inmaculada and Ross, Arun and Chandran, Vinod},
  booktitle = C_BTAS,
  title     = {{Investigating the Impact of Drug Induced Pupil Dilation on Automated Iris Recognition}},
  year      = {2016},
  address   = {Niagara Falls, NY, USA},
  month     = {Sept},
  pages     = {1--8},
  publisher = {IEEE},
}


@InProceedings{Tomeo2015,
  author    = {Tomeo-Reyes, Inmaculada and Ross, Arun and Clark, Antwan D and Chandran, Vinod},
  booktitle = C_ICB,
  title     = {{A Biomechanical Approach to Iris Normalization}},
  year      = {2015},
  address   = {Phuket, Thailand},
  month     = {May},
  pages     = {9--16},
  publisher = {IEEE},
}

@Article{JainRossIntroduction,
  author  = {Jain, A.K. and Ross, A. and Prabhakar, S.},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  title   = {An Introduction to Biometric Recognition},
  year    = {2004},
  month   = {Jan},
  pages   = {4-20},
  volume  = {14},
}

@Article{jain50years,
  author  = {A. K. Jain and K. Nandakumar and A. Ross},
  journal = {Pattern Recognition Letters},
  title   = {50 Years of Biometric Research: Accomplishments, Challenges, and Opportunities},
  year    = {2016},
  pages   = {80 - 105},
  volume  = {79},
}

@Article{Rothstein_NYT_2007,
  author    = {Edward Rothstein},
  journal   = J_NYT,
  title     = {{Is It Live ... or Yamaha? Channeling Glenn Gould}},
  year      = {2007},
  month     = {March},
  number    = {21},
  publisher = {NYT},
}

@InProceedings{Engel_ICLR_2019,
  author    = {Jesse Engel and Kumar Krishna Agrawal and Shuo Chen and Ishaan Gulrajani and Chris Donahue and Adam Roberts},
  booktitle = C_ICLR,
  title     = {GANSynth: Adversarial Neural Audio Synthesis},
  year      = {2019},
  address   = {New Orleans, Louisiana, United States, May 6-9},
  pages     = {1-15},
  url       = {https://openreview.net/pdf?id=H1xQVn09FX},
}

@Online{LOCKLUCK_URL,
  author       = {Agnieszka Marczak-Czajka},
  lastaccessed = {July 14, 2020},
  title        = {{LockLuck -- More Than Cards}},
  url          = {http://www.lockluck.eu},
  year         = {2015},
}


@Online{UBER,
  author       = {Phil McCausland},
  lastaccessed = {July 8, 2021},
  title        = {Self-driving Uber car that hit and killed woman did not recognize that pedestrians jaywalk},
  url          = {https://www.nbcnews.com/tech/tech-news/self-driving-uber-car-hit-killed-woman-did-not-recognize-n1079281},
  year         = {2019},
}

@Online{FACE_BIAS,
  author       = {Steve Lohr},
  lastaccessed = {July 19, 2021},
  title        = {Facial Recognition Is Accurate, if You’re a White Guy},
  url          = {https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html},
  year         = {2018},
}




@Misc{FOTEH,
  howpublished = {http://www.elka.pw.edu.pl/eng/Research/Projects/Structural-projects/FOTEH},
  note         = {Accessed: July 8, 2021},
  title        = {{Photonics and Terahertz Technologies -- Development of the Faculty Research Center (FOTEH); project under Innovative Economy EU Program (POIG), 2010 -- 2011, 38,177,390.68 PLN} ($\approx$ \$10,484,233)},
  year         = {2011},
}

@Misc{TSHEPII,
  note  = {{sponsored by the FBI Biometric Center of Excellence (via West Virginia University, USA), 2017-2018}},
  title = {{A Tool Supporting Human Examination of Post-mortem Iris Images (TSHEPII)}},
}

@Misc{SICLD,
  note  = {{sponsored by the FBI Biometric Center of Excellence (via West Virginia University, USA), 2020-2021}},
  title = {{Synthetic Iris and Contact Lens Detection (SICLD)}},
}

@Misc{TRIO,
  howpublished = {http://www.trio.nd.edu/programs/upward-bound},
  note         = {Accessed: July 8, 2021},
  title        = {{Upward Bound -- The University of Notre Dame's College Preparatory Program}},
}

@Misc{SFL,
  howpublished = {https://sfl.nd.edu},
  note         = {Accessed: July 8, 2021},
  title        = {{AME Student Fabrication Lab}},
}

@Misc{CVRL,
  howpublished = {https://cvrl.nd.edu},
  note         = {Accessed: July 8, 2021},
  title        = {{ND Computer Vision Research Lab}},
}

@Misc{CVRL_DATA,
  howpublished = {https://cvrl.nd.edu/projects/data/},
  note         = {Accessed: July 8, 2021},
  title        = {{ND Computer Vision Research Lab}},
}

@Misc{Kaggle,
  howpublished = {https://www.kaggle.com/competitions},
  note         = {Accessed: July 9, 2021},
  title        = {{Kaggle Competitions}},
}

@Misc{IEG,
  howpublished = {https://www.nist.gov/programs-projects/iris-experts-group-ii-homepage},
  note         = {Accessed: July 9, 2021},
  title        = {{Iris Experts Group II -- A forum for the discussion of technical questions of interest to USG agencies and their staff that are employing or may employ iris recognition to carry out their mission.}},
}



@Online{Falcon_Forbes_2018,
  author       = {William Falcon},
  lastaccessed = {July 14, 2020},
  title        = {{What Happens Now That An AI-Generated Painting Sold For \$432,500?}},
  url          = {https://www.forbes.com/sites/williamfalcon/2018/10/25/what-happens-now-that-an-ai-generated-painting-sold-for-432500},
  year         = {2018},
}

@Article{ShahIrisSeg09,
  author  = {Shah, S. and Ross, A.},
  journal = {IEEE Transactions on Information Forensics and Security},
  title   = {{Iris segmentation using geodesic active contours}},
  year    = {2009},
  number  = {4},
  pages   = {824--836},
  volume  = {4},
}

@InProceedings{Kane_OptoSwimPoster_2020,
  author       = {Alden Kane and Adam Czajka},
  booktitle    = {The 2020 University of Arizona College of Optical Sciences Winter Workshop},
  title        = {{A Two-Feature Underwater Swimmer Detection System}},
  year         = {2020},
  lastaccessed = {July 15, 2020},
  url          = {https://www.youtube.com/watch?v=dr5E5heM_BU},
}


@Online{Satter_Reuters_2020,
  author       = {Raphael Satter},
  day          = {15},
  lastaccessed = {July 15, 2020},
  month        = {July},
  title        = {{Deepfake used to attack activist couple shows new disinformation frontier}},
  url          = {https://www.reuters.com/article/us-cyber-deepfake-activist-idUSKCN24G15E},
  year         = {2020},
}


@Article{Acuna_bioRxiv_2018,
  author       = {Acuna, Daniel E. and Brookes, Paul S. and Kording, Konrad P.},
  journal      = {bioRxiv},
  title        = {Bioscience-scale automated detection of figure element reuse},
  year         = {2018},
  abstract     = {Scientists reuse figure elements sometimes appropriately, e.g. when comparing methods, and sometimes inappropriately, e.g. when presenting an old experiment as a new control. To understand such reuse, automatically detecting it would be important. Here we present an analysis of figure element reuse on a large dataset comprising 760 thousand open access articles and 2 million figures. Our algorithm detects figure region reuse, while being robust to rotation, cropping, resizing, and contrast changes, and estimates which of the reuses have biological meaning. Then a three-person panel analyzes how problematic these biological reuses are using contextual information such as captions and full texts. Based on the panel reviews, we estimate that 9\% of the biological reuses would be unanimously perceived as at least suspicious. We further estimate that 0.6\% of all articles would be unanimously perceived as fraudulent, with inappropriate reuses occurring 43\% across articles, 28\% within article, and 29\% within a figure. Our tool rapidly detects image reuse at scale, promising to be useful to a broad range of people that campaign for scientific integrity. We suggest that a great deal of scientific fraud will be, sooner or later, detectable by automatic methods.},
  doi          = {10.1101/269415},
  elocation-id = {269415},
  eprint       = {https://www.biorxiv.org/content/early/2018/02/23/269415.full.pdf},
  publisher    = {Cold Spring Harbor Laboratory},
  url          = {https://www.biorxiv.org/content/early/2018/02/23/269415},
}

@Misc{Czajka_PatentApp_2019,
  author = {Adam Czajka and Zhaoyuan Fang and Kevin W. Bowyer},
  month  = {October},
  title  = {{Method of Textured Contact Lens Detection, {U}nited {S}tates {P}atent {A}pplication, PCT/US19/57829}},
  year   = {2019},
  day    = {24},
}

@Misc{Czajka_PatentApp_2018,
  author = {Adam Czajka and Zhaoyuan Fang and Kevin W. Bowyer},
  month  = {October},
  title  = {{Method of Ocular Contact Lens Detection, {P}rovisional {U}nited {S}tates {P}atent {A}pplication 62/749,864}},
  year   = {2018},
  day    = {24},
}



@Article{Czajka_CSUR_2018,
  author    = {Adam Czajka and Kevin W. Bowyer},
  journal   = J_ACMCS,
  title     = {Presentation Attack Detection for Iris Recognition: An Assessment of the State of the Art},
  year      = {2018},
  issn      = {0360-0300},
  number    = {4},
  pages     = {86:1-86:35},
  volume    = {54},
  acmid     = {3232849},
  address   = {New York, NY, USA},
  articleno = {86},
  doi       = {10.1145/3232849},
  numpages  = {35},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3232849},
}

@Article{Boyd_PRL_2020,
  author   = {Aidan Boyd and Zhaoyuan Fang and Adam Czajka and Kevin W. Bowyer},
  journal  = {Pattern Recognition Letters},
  title    = {Iris presentation attack detection: Where are we now?},
  year     = {2020},
  issn     = {0167-8655},
  pages    = {483-489},
  volume   = {138},
  abstract = {As the popularity of iris recognition systems increases, the importance of effective security measures against presentation attacks becomes paramount. This work presents an overview of the most important advances in the area of iris presentation attack detection published in the recent two years. Newly-released, publicly-available datasets for development and evaluation of iris presentation attack detection are discussed. Recent literature can be seen to be broken into three categories: traditional “hand-crafted” feature extraction and classification, deep learning-based solutions, and hybrid approaches fusing both methodologies. Conclusions of modern approaches underscore the difficulty of this task. Finally, commentary on possible directions for future research is provided.},
  doi      = {https://doi.org/10.1016/j.patrec.2020.08.018},
  keywords = {Biometrics, Iris presentation attack detection, Security},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167865520303226},
}

@Article{Boyd_Access_2020,
  author  = {Boyd, Aidan and Yadav, Shivangi and Swearingen, Thomas and Kuehlkamp, Andrey and Trokielewicz, Mateusz and Benjamin, Eric and Maciejewicz, Piotr and Chute, Dennis and Ross, Arun and Flynn, Patrick and Bowyer, Kevin and Czajka, Adam},
  journal = {IEEE Access},
  title   = {{Post-Mortem Iris Recognition -- A Survey and Assessment of the State of the Art}},
  year    = {2020},
  pages   = {136570-136593},
  volume  = {8},
  doi     = {10.1109/ACCESS.2020.3011364},
}

@Article{Ramachandra_CSUR_2017,
  author     = {Ramachandra, Raghavendra and Busch, Christoph},
  journal    = J_ACMCS,
  title      = {Presentation Attack Detection Methods for Face Recognition Systems: A Comprehensive Survey},
  year       = {2017},
  issn       = {0360-0300},
  month      = mar,
  number     = {1},
  volume     = {50},
  address    = {New York, NY, USA},
  articleno  = {8},
  doi        = {10.1145/3038924},
  issue_date = {April 2017},
  keywords   = {face recognition, countermeasure, antispoofing, attacks, security, Biometrics},
  numpages   = {37},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3038924},
}

@Article{Sousedik_IET_2014,
  author  = {Ctirad Sousedik and Christoph Busch},
  journal = {IET Biometrics},
  title   = {Presentation attack detection methods for fingerprint recognition systems: a survey},
  year    = {2014},
  number  = {4},
  pages   = {219-233},
  volume  = {3},
}

@InProceedings{Goodfellow_ICLR_2015,
  author    = {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
  booktitle = {International Conference on Learning Representations},
  title     = {Explaining and Harnessing Adversarial Examples},
  year      = {2015},
  pages     = {1-11},
  url       = {http://arxiv.org/abs/1412.6572},
}

@InProceedings{Eykholt_CVPR_2018,
  author    = {K. {Eykholt} and I. {Evtimov} and E. {Fernandes} and B. {Li} and A. {Rahmati} and C. {Xiao} and A. {Prakash} and T. {Kohno} and D. {Song}},
  booktitle = C_CVPR,
  title     = {Robust Physical-World Attacks on Deep Learning Visual Classification},
  year      = {2018},
  pages     = {1625-1634},
}

@Article{Xu_IJAC_2020,
  author  = {Han Xu and Yao Ma and Hao-Chen Liu and Debayan Deb and Hui Liu and Ji-Liang Tang and Anil K. Jain},
  journal = J_IJAC,
  title   = {{Adversarial Attacks and Defenses in Images, Graphs and Text: A Review}},
  year    = {2020},
  month   = {April},
  number  = {2},
  pages   = {219-233},
  volume  = {17},
  doi     = {10.1007/s11633-019-1211-x},
}


@Misc{NeoInnovate,
  howpublished = {\url{https://neoinfo.iu.edu}},
  note         = {Accessed: July 16, 2020},
  title        = {{The NeoInnovate Collaborative Consortium}},
}


@InProceedings{Das_IJCB_2020,
  author    = {Das, Priyanka and McGrath, Joseph and Fang, Zhaoyuan and Boyd, Aidan and Jang, Ganghee and Mohammadi, Amir and Purnapatra, Sandip and Yambay, David and Marcel, Sébastien and Trokielewicz, Mateusz and Maciejewicz, Piotr and Bowyer, Kevin and Czajka, Adam and Schuckers, Stephanie and Tapia, Juan and Gonzalez, Sebastian and Fang, Meiling and Damer, Naser and Boutros, Fadi and Kuijper, Arian and Sharma, Renu and Chen, Cunjian and Ross, Arun},
  booktitle = C_IJCB,
  title     = {{Iris Liveness Detection Competition (LivDet-Iris) - The 2020 Edition}},
  year      = {2020},
  pages     = {1-9},
  doi       = {10.1109/IJCB48548.2020.9304941},
}



@Misc{LivDetIris2020,
  howpublished = {\url{http://www.iris2020.livdet.org/}},
  note         = {Accessed: July 16, 2020},
  title        = {{LivDet-Iris 2020 -- Liveness Detection Competition Series}},
}


@Article{Fang_TIFS_2020,
  author       = {Z. {Fang} and A. {Czajka} and K. W. {Bowyer}},
  journal      = J_TIFS,
  title        = {{Robust Iris Presentation Attack Detection Fusing 2D and 3D Information}},
  year         = {2021},
  pages        = {510-520},
  volume       = {16},
  howpublished = {\url{https://arxiv.org/abs/2002.09137}},
}

@InProceedings{Czajka_WACV_2019a,
  author    = {Adam Czajka and Zhaoyuan Fang and Kevin W. Bowyer},
  booktitle = C_WACV,
  title     = {{Iris Presentation Attack Detection Based on Photometric Stereo Features}},
  year      = {2019},
  address   = {Waikoloa Village, Hawaii, January 8-10},
  pages     = {877-885},
  abstract  = {We propose a new iris presentation attack detection method using three-dimensional features of an observed iris region estimated by photometric stereo. Our implementation uses a pair of iris images acquired by a common commercial iris sensor (LG 4000). No hardware modifications of any kind are required. Our approach should be applicable to any iris sensor that can illuminate the eye from two different directions. Each iris image in the pair is captured under near-infrared illumination at a different angle relative to the eye. Photometric stereo is used to estimate surface normal vectors in the non-occluded portions of the iris region. The variability of the normal vectors is used as the presentation attack detection score. This score is larger for a texture that is irregularly opaque and printed on a convex contact lens, and is smaller for an authentic iris texture. Thus the problem is formulated as binary classification into (a) an eye wearing textured contact lens and (b) the texture of an actual iris surface (possibly seen through a clear contact lens). Experiments were carried out on a database of approx. 2,900 iris image pairs acquired from approx. 100 subjects. Our method was able to correctly classify over 95{\%} of samples when tested on contact lens brands unseen in training, and over 98{\%} of samples when the contact lens brand was seen during training. The source codes of the method are made available to other researchers.},
  doi       = {10.1109/WACV.2019.00098},
}

@Article{McGrath_ArXiv_2019,
  author        = {Joseph McGrath and Kevin W. Bowyer and Adam Czajka},
  journal       = {arXiv e-prints},
  title         = {{Open Source Presentation Attack Detection Baseline for Iris Recognition}},
  year          = {2019},
  month         = {May},
  pages         = {1--8},
  archiveprefix = {arXiv},
  eid           = {arXiv:1809.10172},
  eprint        = {1809.10172},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
}

@InProceedings{Fang_IJCB_2020,
  author    = {Zhaoyuan Fang and Adam Czajka},
  booktitle = C_IJCB,
  title     = {{Open Source Iris Recognition Hardware and Softwarewith Presentation Attack Detection}},
  year      = {2020},
  address   = {Houston, Texas, Sept 28 -- Oct 1},
  pages     = {1--8},
}

@Misc{ISO_IEC_SC37,
  howpublished = {\url{https://www.iso.org/committee/313770.html}},
  note         = {Accessed: July 16, 2020},
  title        = {{ISO/IEC JTC 1/SC 37 -- Biometrics}},
}

@Misc{FBI_NGI_webpage,
  howpublished = {\url{https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi}},
  note         = {Accessed: July 16, 2020},
  title        = {{Next Generation Identification (NGI)}},
}

@InProceedings{Husseis_ICCST_2019,
  author    = {A. {Husseis} and J. {Liu-Jimenez} and I. {Goicoechea-Telleria} and R. {Sanchez-Reillo}},
  booktitle = C_ICCST,
  title     = {{A Survey in Presentation Attack and Presentation Attack Detection}},
  year      = {2019},
  pages     = {1-13},
}

@InProceedings{Battiato_ICCSTACM_2016,
  author    = {Battiato, Sebastiano and Giudice, Oliver and Paratore, Antonino},
  booktitle = C_ICCST_ACM,
  title     = {Multimedia Forensics: Discovering the History of Multimedia Contents},
  year      = {2016},
  address   = {New York, NY, USA},
  pages     = {5–16},
  publisher = {Association for Computing Machinery},
  series    = {CompSysTech ’16},
  doi       = {10.1145/2983468.2983470},
  isbn      = {9781450341820},
  keywords  = {Forgery Detection, Multimedia Forensics, Camera Source Identification, Image Ballistics},
  location  = {Palermo, Italy},
  numpages  = {12},
}

@Article{Bondielli_IS_2019,
  author   = {Alessandro Bondielli and Francesco Marcelloni},
  journal  = J_IS,
  title    = {A survey on fake news and rumour detection techniques},
  year     = {2019},
  issn     = {0020-0255},
  pages    = {38 - 55},
  volume   = {497},
  abstract = {False or unverified information spreads just like accurate information on the web, thus possibly going viral and influencing the public opinion and its decisions. Fake news and rumours represent the most popular forms of false and unverified information, respectively, and should be detected as soon as possible for avoiding their dramatic effects. The interest in effective detection techniques has been therefore growing very fast in the last years. In this paper we survey the different approaches to automatic detection of fake news and rumours proposed in the recent literature. In particular, we focus on five main aspects. First, we report and discuss the various definitions of fake news and rumours that have been considered in the literature. Second, we highlight how the collection of relevant data for performing fake news and rumours detection is problematic and we present the various approaches, which have been adopted to gather these data, as well as the publicly available datasets. Third, we describe the features that have been considered in fake news and rumour detection approaches. Fourth, we provide a comprehensive analysis on the various techniques used to perform rumour and fake news detection. Finally, we identify and discuss future directions.},
  doi      = {https://doi.org/10.1016/j.ins.2019.05.035},
  keywords = {Fake news, Rumours, Natural language processing, Data mining, Text mining, Classification, Machine learning, Deep learning},
}

@Article{Kotzerke_AJFS_2019,
  author    = {J. Kotzerke and S. A. Davis and R. Hayes and K. J. Horadam},
  journal   = {Australian Journal of Forensic Sciences},
  title     = {Newborn and infant discrimination: revisiting footprints},
  year      = {2019},
  number    = {1},
  pages     = {95-108},
  volume    = {51},
  abstract  = {ABSTRACT Formal registration of newborns and infants is a necessity to secure their rights for health care or education and to support law enforcement agencies in the fight against the trafficking of children or their illegal adoption. Ideally, all these requirements can be met using a newborn and infant biometric. Difficulties arise due to the small size and fragility of the infants’ physical structures and the effects of rapid physical growth. We review the literature for suitable biometrics and investigate the footprint; asking (a) if there is a time frame shortly after birth when the friction ridge skin pattern of a newborn can be reliably captured; and (b) if the footprint crease pattern is a suitable newborn and infant biometric. For (a), we were unable to confirm the existence of such a time frame. For (b), we performed automatic verification experiments on a small test set of 20 pairs of crease patterns, and then a larger test set, achieving EERs of 22.22\% and 46.39\%, respectively. The comparison of two dizygotic twins did not show any noteworthy performance difference. Based on these results we do not recommend the foot crease pattern as a newborn or infant biometric for automatic verification.},
  doi       = {10.1080/00450618.2017.1324582},
  eprint    = {https://doi.org/10.1080/00450618.2017.1324582},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.1080/00450618.2017.1324582},
}

@InProceedings{Lemes_IJCB_2011,
  author    = {R. P. {Lemes} and O. R. P. {Bellon} and L. {Silva} and A. K. {Jain}},
  booktitle = C_IJCB,
  title     = {Biometric recognition of newborns: Identification using palmprints},
  year      = {2011},
  pages     = {1-6},
}

@InProceedings{Barra_BIOMS_2014,
  author    = {S. {Barra} and A. {Casanova} and M. {De Marsico} and D. {Riccio}},
  booktitle = {The IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS) Proceedings},
  title     = {Babies: Biometric authentication of newborn identities by means of ear signatures},
  year      = {2014},
  month     = {Oct},
  pages     = {1-7},
  abstract  = {Many research studies demonstrated that recognition based on ear biometrics offers an accuracy which is comparable to face trait, especially in controlled settings. Our proposal is to exploit it to avoid the problem of newborn swap, which is possible and actually happens, most of all in crowded maternity wards of big hospitals. We tested the viability of this solution using a dataset of ear images of newborns, and the obtained results testify that it is possible to decrease the probability of an error using this technique.},
  doi       = {10.1109/BIOMS.2014.6951528},
  keywords  = {biometrics (access control);ear;image processing;probability;BABIES;biometric authentication;newborn identities;ear signatures;ear biometrics;newborn swap;ear images;probability;Ear;Pediatrics;Shape;DNA;Fingerprint recognition;Training;Image recognition;newborn swap;ear biometrics},
}


@InProceedings{Tiwari_ACSEA_2012,
  author    = {Tiwari, Shrikant and Singh, Aruni and Singh, Sanjay Kumar},
  booktitle = {Advances in Computer Science, Engineering {\&} Applications},
  title     = {Can Ear and Soft-Biometric Traits Assist in Recognition of Newborn?},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  editor    = {Wyld, David C. and Zizka, Jan and Nagamalai, Dhinaharan},
  pages     = {179--192},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Missing, swapping, mixing, and illegal adoption of newborns is a global challenge and research done to solve this issue is minimal and least reported in the literature. Most of the biometric systems developed are for adults and very few of them address the issue of newborn identification.The ear of newborn is a perfect source of data for passive identification of newborn as they are the highly non cooperative users of biometrics. The four characteristics of ear biometrics: universality, uniqueness, permanence and collectability make it a very potential biometric trait for the identification of newborn. Further the use of soft-biometric data like gender, blood group, height and weight along with ear enhances the accuracy for identification of newborn. The objective of this paper is to demonstrate the concept of using ear and soft-biometrics recognition for identification of newborn. The main contribution of the research are (a) the preparation of ear and soft biometric database of newborn. (b)Fusion of ear and soft-biometrics data for identification of 210 newborn, results in an improvement of approximately 5.59{\%} over the primary biometric system i.e. ear.},
  isbn      = {978-3-642-30157-5},
}



@InBook{Szczepanski_IFIP_2014,
  author    = {Szczepa{\'{n}}ski, Adam and Misztal, Krzysztof and Saeed, Khalid},
  editor    = {Saeed, Khalid and Sn{\'a}{\v{s}}el, V{\'a}clav},
  pages     = {141--150},
  publisher = {Springer Berlin Heidelberg},
  title     = {Pupil and Iris Detection Algorithm for Near-Infrared Capture Devices},
  year      = {2014},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-662-45237-0},
  abstract  = {In this paper a simple and robust solution for the pupil and iris detection is presented. The procedure is based on simple operations, such as erosion, dilation, binarization, flood filling and Sobel filter and, with proper implementation, is effective. The novelty of the approach is the use of distances of black points from nearest white points to estimate and then adjust the position of the center and the radius of the pupil which is also used for iris detection. The obtained results are promising, the pupil is extracted properly and all the information necessary for human identification and verification can be extracted from the found parts of the iris. The paper, being both review and research, contains also a state of the art in the described topic.},
  booktitle = {Computer Information Systems and Industrial Management: 13th IFIP TC8 International Conference, CISIM 2014, Ho Chi Minh City, Vietnam, November 5-7, 2014. Proceedings},
  doi       = {10.1007/978-3-662-45237-0\_15},
  url       = {http://dx.doi.org/10.1007/978-3-662-45237-0\_15},
}

@Article{Czajka_Spectrum_2019,
  author  = {Czajka, Adam and Trokielewicz, Mateusz and Maciejewicz, Piotr},
  journal = {IEEE Spectrum},
  title   = {The eyes have it: new iris recognition techniques can tell whether an eye is healthy, diseased, or dead},
  year    = {2019},
  number  = {09},
  pages   = {44-49},
  volume  = {56},
  doi     = {10.1109/MSPEC.2019.8818591},
}


@Misc{IrisGuard_Ad_2017,
  author       = {{IrisGuard Inc.}},
  howpublished = {on-line},
  title        = {{IrisGuard. EyeBank Solution, http://www.irisguard.com/eyebank/downloads/EyeBankPer\%20Page.pdf}},
  year         = {(access: July 3, 2017)},
}

@Misc{IriTech_Blog_2015,
  author       = {{IriTech Inc.}},
  howpublished = {on-line},
  title        = {{IriTech. Biometric Access Control Can Iris Biometric Enhance Better Security?, http://www.iritech.com/blog/iris-biometric-access-control}},
  year         = {August 20, 2015 (access: July 3, 2017)},
}

@Misc{Daugman_BBC_2001,
  author       = {John Daugman},
  howpublished = {on-line},
  title        = {{The eyes have it, http://news.bbc.co.uk/2/hi/science/nature/1477655.stm, BBC News}},
  year         = {August 9, 2001 (access: July 3, 2017)},
}

@Misc{OSIRIS_SDK,
  author = {Guillaume Sutra and Bernadette Dorizzi and Sonia Garcia-Salitcetti and Nadia Othman},
  title  = {{A biometric reference system for iris. OSIRIS version 4.1}: http://svnext.it-sudparis.eu/svnview2-eph/ref\textunderscore syst/Iris\textunderscore Osiris\textunderscore v4.1/},
  year   = {(access: October 1, 2014)},
}

@Misc{MIRLIN_SDK,
  author = {{Smart Sensors Ltd.}},
  title  = {{MIRLIN SDK}, v. 2.23, offered now by Fotonation: https://www.fotonation.com/products/biometrics/iris-recognition (access: September 10, 2017)},
  year   = {2013},
}

@Misc{VeriEye_SDK,
  author = {Neurotechnology},
  title  = {{VeriEye SDK, v. 4.3: www.neurotechnology.com/verieye.html}},
  year   = {(access: February 28, 2015)},
}

@TechReport{IREX_I,
  author      = {Patrick J. Grother and Elham Tabassi and George W. Quinn and Wayne J. Salamon},
  institution = {NIST},
  title       = {{IREX I}: Performance of Iris Recognition Algorithms on Standard Images},
  year        = {2009},
  note        = {{Interagency Report 7629}},
  abstract    = {The IREX program supports the development of interoperable iris imagery for use in high performance biometric applications. The IREX evaluation, was conducted in cooperation with the iris recognition industry to demonstrate that standardized image formats can be interoperable and compact. This is required for federated applications in which iris data is exchanged between interoperating systems, passed across bandwidth-limited networks, or stored on identity credentials. The IREX I study was initiated to give quantitative support to the revision of the ISO/IEC 19794-6 and ANSI/NIST TYPE 17 standards, and to form a multi-provider marketplace around those standards.},
}

@Article{Daugman_TCSVT_2004,
  author   = {J. Daugman},
  journal  = J_TCSVT,
  title    = {How iris recognition works},
  year     = {2004},
  issn     = {1051-8215},
  month    = {January},
  number   = {1},
  pages    = {21-30},
  volume   = {14},
  abstract = {Algorithms developed by the author for recognizing persons by their iris patterns have now been tested in many field and laboratory trials, producing no false matches in several million comparison tests. The recognition principle is the failure of a test of statistical independence on iris phase structure encoded by multi-scale quadrature wavelets. The combinatorial complexity of this phase information across different persons spans about 249 degrees of freedom and generates a discrimination entropy of about 3.2 b/mm2 over the iris, enabling real-time decisions about personal identity with extremely high confidence. The high confidence levels are important because they allow very large databases to be searched exhaustively (one-to-many "identification mode") without making false matches, despite so many chances. Biometrics that lack this property can only survive one-to-one ("verification") or few comparisons. The paper explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea.},
  doi      = {10.1109/TCSVT.2003.818350},
  keywords = {biometrics (access control);decision theory;entropy;image matching;statistical analysis;transform coding;wavelet transforms;biometrics;decision theory;discrimination entropy;iris pattern recognition;iris phase structure;iris recognition;multi-scale quadrature wavelet coding;one-to-many identification mode;one-to-one identification mode;personal identity;statistical independence;verification;Demodulation;Entropy;Image databases;Iris recognition;Ligaments;Lighting;Pattern matching;Pattern recognition;Pigmentation;Testing},
}

@InProceedings{Trokielewicz_BF_2015,
  author    = {M. Trokielewicz},
  booktitle = W_BF,
  title     = {Linear regression analysis of template aging in iris biometrics},
  year      = {2015},
  month     = {March},
  pages     = {1-6},
  abstract  = {The aim of this work is to determine how vulnerable different iris coding methods are in relation to biometric template aging phenomenon. This is considered to be particularly important when the time lapse between gallery and probe samples extends significantly, to more than a few years. Our experiments employ iris aging analysis conducted using three different iris recognition algorithms and a database of 583 samples from 58 irises collected up to nine years apart. To determine the degradation rates of similarity scores with extending time lapse and also in relation to multiple image quality and geometrical factors of sample images, a linear regression analysis was performed. 29 regression models have been tested with both the time parameter and geometrical factors being statistically significant in every model. Quality measures that showed statistically significant influence on the predicted variable were, depending on the method, image sharpness and local contrast or their mutual relations. To our best knowledge, this is the first paper describing aging analysis using multiple regression models with data covering such a wide time period. Results presented suggest that template aging effect occurs in iris biometrics to a statistically significant extent. Image quality and geometrical factors may contribute to the degradation of similarity score. However, the estimate of time parameter showed statistical significance and similar value in each of the tested models. This reveals that the aging phenomenon may as well be unrelated to quality and geometrical measures of the image.},
  doi       = {10.1109/IWBF.2015.7110233},
  keywords  = {geometry;iris recognition;regression analysis;visual databases;biometric template aging phenomenon;database;gallery samples;geometrical factors;iris biometrics;iris coding methods;iris recognition algorithms;linear regression analysis;multiple image quality;probe samples;quality measures;Aging;Analytical models;Biological system modeling;Iris;Iris recognition;biometrics;biometrictemplate aging;iris recognition;linear regression},
}

@Article{Grother_IET_2015,
  author   = {P. Grother and J. R. Matey and G. W. Quinn},
  journal  = J_IETB,
  title    = {{IREX VI: mixed-effects longitudinal models for iris ageing: response to Bowyer and Ortiz}},
  year     = {2015},
  issn     = {2047-4938},
  number   = {4},
  pages    = {200-205},
  volume   = {4},
  abstract = {Bowyer and Ortiz, in their study `A Critical Examination of the IREX VI Results', make seven criticisms of the authors application of linear mixed-effects models to longitudinally collected iris recognition Hamming distances. We reject these as either irrelevant, misinterpretations, or qualitatively correct, but quantitatively irrelevant.},
  doi      = {10.1049/iet-bmt.2015.0043},
  keywords = {iris recognition;IREX VI;iris ageing;linear mixed-effect longitudinal models;longitudinally collected iris recognition Hamming distances},
}

@InProceedings{Tinsley_WACV_2021,
  author    = {Tinsley, Patrick and Czajka, Adam and Flynn, Patrick},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {This Face Does Not Exist... But It Might Be Yours! Identity Leakage in Generative Models},
  year      = {2021},
  pages     = {1319-1327},
  doi       = {10.1109/WACV48630.2021.00136},
}



@InProceedings{Pacut_NATO_2005,
  author    = {Pacut, Andrzej and Czajka, Adam and Strzelczyk, Przemek},
  booktitle = {Cyberspace Security and Defense: Research Issues},
  title     = {IRIS Biometrics for Secure Remote Access},
  year      = {2005},
  address   = {Dordrecht},
  editor    = {Kowalik, Janusz S. and Gorski, Janusz and Sachenko, Anatoly},
  pages     = {259--278},
  publisher = {Springer Netherlands},
  abstract  = {We propose a new iris texture coding technique with optimal feature extraction, and design a secure remote (internet) access system using the proposed biometrics. The proposed iris coding method is based on Zak-Gabor coefficients sequence, and additionally uses an optimal selection of a subset of iris features. The secure access involves a communication scenario that employs a usual client-server network model, thus incorporating standard security mechanisms with biometric enhancements. The proposed access scenario enables to include the aliveness detection capability and the biometric replay attack prevention.},
  isbn      = {978-1-4020-3381-0},
}



@InProceedings{Czajka_BEST_2013,
  author    = {Czajka, Adam},
  booktitle = {Biomedical Engineering Systems and Technologies},
  title     = {Influence of Iris Template Aging on Recognition Reliability},
  year      = {2014},
  address   = {Berlin, Heidelberg},
  editor    = {Fern{\'a}ndez-Chimeno, Mireya and Fernandes, Pedro L. and Alvarez, Sergio and Stacey, Deborah and Sol{\'e}-Casals, Jordi and Fred, Ana and Gamboa, Hugo},
  pages     = {284--299},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The paper presents an iris aging analysis based on comparison results obtained for four different iris matchers. We collected an iris aging database of samples captured even eight years apart. To our best knowledge, this is the only database worldwide of iris images collected with such a large time distance between capture sessions. We evaluated the influence of the intra- vs. inter-session accuracy of the iris recognition, as well as the accuracy between the short term (up to two years) vs. long term comparisons (from 5 to 9 years). The average genuine scores revealed statistically significant differences with respect to the time distance between examined samples (up to 14 {\%} of degradation in the average genuine scores is observed). These results may suggest that the iris pattern ages to some extent, and thus appropriate countermeasures should be deployed in application assuming large time distances between iris template replacements (or adaptations).},
  isbn      = {978-3-662-44485-6},
}

@Article{Bowyer_IET_2015,
  author   = {K. W. Bowyer and E. Ortiz},
  journal  = {IET Biometrics},
  title    = {{Critical examination of the IREX VI results}},
  year     = {2015},
  issn     = {2047-4938},
  number   = {4},
  pages    = {192-199},
  volume   = {4},
  abstract = {The authors analyse why Iris Exchange Report (IREX) VI conclusions about `iris ageing' differ significantly from results of previous research on `iris template ageing'. They observe that IREX VI uses a definition of `iris ageing' that is restricted to a subset of International Organization for Standardization (ISO)-definition template ageing. They also explain how IREX VI commits various methodological errors in obtaining what it calls its `best estimate of iris recognition ageing'. The OPS-XING dataset that IREX VI analyses for its `best estimate of iris recognition ageing' contains no matches with Hamming distance >0.27. A `truncated regression' technique should be used to analyse such a dataset, which IREX VI fails to do so, biasing its `best estimate' to be lower-than-correct. IREX VI mixes Hamming distances from first, second and third attempts together in its regression, creating another source of bias towards a lower-than-correct value. In addition, the match scores in the OPS-XING dataset are generated from a `1-to-first' matching strategy, meaning that they contain a small but unknown number of impostor matches, constituting another source of bias towards an artificially low value for ageing. Finally, IREX VI makes its `best estimate of iris recognition ageing' by interpreting its regression model without taking into account the correlation among independent variables. This is another source of bias towards an artificially low value for ageing. Importantly, the IREX VI report does not acknowledge the existence of any of these sources of bias. They conclude with suggestions for a revised, improved IREX VI.},
  doi      = {10.1049/iet-bmt.2014.0007},
  keywords = {image matching;iris recognition;regression analysis;Hamming distance;OPS-XING dataset;critical IREX VI results examination;iris ageing;iris recognition ageing;iris template ageing;lower-than-correct;methodological error;pattern matching strategy;truncated regression technique},
}

@TechReport{IREX_VI_2013,
  author      = {P. Grother and J. R. Matey and E. Tabassi and G. Quinn and M. Chumakov},
  institution = {NIST},
  title       = {{IREX VI}, Temporal Stability of Iris Recognition Accuracy},
  year        = {2013},
  month       = {July},
  note        = {{Interagency Report 7948}},
  abstract    = {Background: Stability is a required definitional property for a biometric to be useful. Quantitative statements of stability are operationally important as they dictate re-enrollment schedules e.g. of a face on a passport. Ophthalmologists consider the iris to be stable, and accordingly iris recognition accuracy was thought to be invariant over time[26, 16]. This view held until several recent empirical studies suggested otherwise. Two of these, using separate iris image collections from the University of Notre Dame, reported a large template-ageing effect[5, 24]. The studies claimed to have excluded several possible causes of the observed ageing, but could not conclude that the iris texture itself was changing. Their results, however, were widely publicized [53, 19, 3] with statements such as “irises, rather being stable over a lifetime, are susceptible to ageing effects that steadily change the appearance over time” [28]. A further study, however, noted pupil-dilation [22] as the primary causal variable. Detection of long-term ageing trends is complicated by short-term stochastic variations inherent in acquiring digital images from a live analog anatomical source - see Figure 1. Approach: We quantify time variation in iris recognition accuracy in two ways. First we produce rate-of-change estimates for up to 122,000 frequent travelers using a fixed iris recognition system for up to 9 years. Second, we apply iris recognition algorithms to the images of 217 individuals used in the Notre Dame studies. The algorithms produce pupil dilation and exposed iris area measures which we relate to recognition outcomes. Additionally, we review other published studies, formulate recommendations for conduct of biometric ageing studies and for the mitigation of ageing effects in operational systems.},
}

@TechReport{ICAO_9303,
  author      = {{International Civil Aviation Organization (ICAO)}},
  institution = {ICAO},
  title       = {{Doc 9303, Machine Readable Travel Documents, Part 1: Introduction}},
  year        = {2015},
  note        = {{Seventh Edition}},
}

@InProceedings{Czajka_EUSIPCO_2007,
  author    = {Czajka, Adam and Strzelczyk, Przemek and Chochowski, Marcin and Pacut, Andrzej},
  booktitle = {2007 15th European Signal Processing Conference},
  title     = {Iris recognition with match-on-card},
  year      = {2007},
  pages     = {189-192},
}

@TechReport{Wayman_NBTCCW_2000,
  author      = {James Wayman},
  institution = {San Jose State University},
  title       = {Evaluation of the {INSPASS} Hand Geometry Data},
  year        = {August 2000},
  note        = {{National Biometric Test Center Collected Works, v.1.2}},
}

@InProceedings{Czajka_Carnahan_2008,
  author    = {Czajka, Adam and Pacut, Andrzej},
  booktitle = {2008 42nd Annual IEEE International Carnahan Conference on Security Technology},
  title     = {Replay attack prevention for iris biometrics},
  year      = {2008},
  pages     = {247-253},
  doi       = {10.1109/CCST.2008.4751309},
}

@InProceedings{Reillo_ICARCV_2008,
  author    = {Sanchez-Reillo, Raul and Alonso-Moreno, Raul and Czajka, Adam and Young-Bin Kwon},
  booktitle = {2008 10th International Conference on Control, Automation, Robotics and Vision},
  title     = {Automatic remote evaluation system for biometric testing},
  year      = {2008},
  pages     = {1137-1143},
  doi       = {10.1109/ICARCV.2008.4795681},
}

@InBook{Daugman_EB_2009,
  author    = {John Daugman},
  editor    = {Stan Z. Li and Anil Jain},
  pages     = {819--825},
  publisher = {Springer},
  title     = {Iris Recognition at Airports and Border-Crossings},
  year      = {2009},
  address   = {Boston, MA},
  isbn      = {978-0-387-73003-5},
  booktitle = {Encyclopedia of Biometrics},
  doi       = {10.1007/978-0-387-73003-5_179},
  url       = {http://dx.doi.org/10.1007/978-0-387-73003-5_179},
}

@Article{McLaren_IOVS_1992,
  author   = {J W McLaren and J C Erie and R F Brubaker},
  journal  = J_IOVS,
  title    = {Computerized analysis of pupillograms in studies of alertness},
  year     = {1992},
  number   = {3},
  pages    = {671-677},
  volume   = {33},
  abstract = {When alert subjects sit in a quiet, darkened room for 10-15 min, their pupils remain dilated. If the subjects become sleepy or fatigued, their pupils become miotic and oscillate. Pupillometry, the recording of pupil diameter, has been used to study alertness. Pupillograms are typically graded by subjective assessment on the basis of oscillations of pupil diameter (hippus) and miosis. In this study, numeric parameters were calculated from pupillograms and were compared to subjective ratings by clinicians. Pupil diameters were recorded for 15 min at five samples per second with a custom built video pupillometer. Subjects sat in the dark and fixated a small red light. Digital filtering techniques and Fourier analysis were used to calculate several parameters designed to report hippus and miosis. The same set of pupillograms was graded by three physicians familiar with pupillometry. Grades were assigned on a scale of 1 to 4, 1 being most alert and 4 least alert. A linear combination of three of the numeric parameters had the highest correlation with the average subjective grade (r = 0.92). These techniques provide a quantitative way to evaluate pupillograms that will be used in the assessment of alertness.},
}

@Article{Fotiou_ECN_2007,
  author   = {D.F. Fotiou and C.G. Brozou and D.J. Tsiptsios and A. Fotiou and A. Kabitsi and M. Nakou and C. Giantselidis and A. Goula},
  journal  = J_ECN,
  title    = {Effect of age on pupillary light reflex: evaluation of pupil mobility for clinical practice and research},
  year     = {2007},
  number   = {1},
  pages    = {11-22},
  volume   = {47},
  abstract = {The aim of this study was to provide a data base for the measurement of various parameters of the pupil light reflex in normal subjects using a fast video pupillometry device (262 frames/sec). One hundred healthy subjects took part in the study aged 44.31+/-18.11 years. Subjects were divided in two (2) groups according to age: 18-50 years of age (group 1) and 51-81 years of age (group 2). All subjects were examined between 09.00 and 15.00 and the re-test examination was repeated exactly 24 hours later. All variables showed satisfactory test-retest reliability (Pearson test-retest showed values over 0. 70 for all parameters besides Time for Maximum Constriction (T3. 0.62) and Recovery (R%: 0.57)). The Latency of pupil reaction to light (Ti) was not affected by Age. Baseline Pupil Radius (RI) after 2 min of dark adaptation was statistically smaller in the elderly group p < 0.001; Maximum Constriction Velocity (VCmax), Maximum Constriction Acceleration (ACmax) and Amplitude (AMP) were significantly smaller in the elderly group (p < 0.001) when compared to group 1. When all parameters were studied their correlation showed a statistical significant difference for R1, R2, VCmax, ACmax and AMP when related to Age. However when Age was taken into account through the use of partial correlation, the relation between R1, R2 and AMP remained unaltered, but the relations between R1 and VCmax and ACmax were dramatically reduced from -0.39 to -0.21 and from -0.45 to -0.09 respectively indicating that the relation observed between Ri and VCmax and ACmax was due mainly to the Age of the subjects. The results suggest that age influences Baseline Pupil Size, Maximum Constriction Velocity (VCmax) and Acceleration (ACmax), while the Latency of the light reflex remains unaltered.},
}

@InProceedings{Ortiz_BTAS_2013,
  author    = {E. Ortiz and K. W. Bowyer and P. J. Flynn},
  booktitle = C_BTAS,
  title     = {A linear regression analysis of the effects of age related pupil dilation change in iris biometrics},
  year      = {2013},
  month     = {September},
  pages     = {1-6},
  abstract  = {Medical studies have shown that average pupil size decreases linearly throughout adult life. Therefore, on average, the longer the time between acquisition of two images of the same iris, the larger the difference in dilation between the two images. Several studies have shown that increased difference in dilation causes an increase in the false nonmatch rate for iris recognition. Thus, increased difference in pupil dilation is one natural mechanism contributing to an iris template aging effect. We present an experimental analysis of the change in genuine match scores in the context of dilation differences due to aging.},
  doi       = {10.1109/BTAS.2013.6712687},
  keywords  = {image matching;iris recognition;regression analysis;age related pupil dilation change;average pupil size;false nonmatch rate;genuine match scores;iris biometrics;iris recognition;iris template aging effect;linear regression analysis;pupil dilation differences;Aging;Biomedical imaging;Data models;Hamming distance;Iris recognition;Linear regression},
}

@InProceedings{Hollingsworth_BTAS_2008,
  author    = {K. P. Hollingsworth and K. W. Bowyer and P. J. Flynn},
  booktitle = C_BTAS,
  title     = {The Importance of Small Pupils: A Study of How Pupil Dilation Affects Iris Biometrics},
  year      = {2008},
  month     = {September},
  pages     = {1-6},
  abstract  = {This work studies the effect of pupil dilation on the accuracy of iris biometrics. We find that when matching enrollment and recognition images of the same person, larger differences in pupil dilation yield higher template dissimilarities, and so a greater chance of a false non-match. Another experimental result is that even when the degree of dilation is similar at enrollment and recognition, comparisons involving highly dilated pupils result in worse recognition performance than comparisons involving constricted pupils. We find that when the matched images have similarly highly dilated pupils, the mean Hamming distance of the match distribution increases and the mean Hamming distance of the non-match distribution decreases, bringing the distributions closer together from both directions. We recommend that a measure of pupil dilation be kept as meta-data for every iris code. Also, the absolute dilation of the two images, and the dilation difference between them, should factor into a confidence measure for an iris match.},
  doi       = {10.1109/BTAS.2008.4699341},
  keywords  = {biometrics (access control);eye;image matching;image recognition;iris biometrics;iris code;mean Hamming distance;meta-data;pupil dilation;recognition;template dissimilarities;Biometrics;Control systems;Hamming distance;Image recognition;Iris;Lighting control;Muscles;Robustness;Rubber;Size control},
}

@Article{Ortiz_IETb_2016,
  author   = {E. Ortiz and K. W. Bowyer and P. J. Flynn},
  journal  = J_IETB,
  title    = {Dilation-aware enrolment for iris recognition},
  year     = {2016},
  issn     = {2047-4938},
  number   = {2},
  pages    = {92-99},
  volume   = {5},
  abstract = {Iris recognition systems typically enrol a person based on a single `best' eye image. Research has shown that the probability of a false non-match result increases with increased difference in pupil dilation between the enrolment image and the probe image. Therefore, dilation-aware methods of enrolment should improve the accuracy of iris recognition. The authors examine a strategy to improve accuracy through a dilation-aware enrolment step that selects one or more enrolment images based on the observed distribution of dilation ratios for that eye. Additionally, they demonstrate that an image with median dilation is the optimal single eye image dilation-aware enrolment choice. Their results confirm that this dilation-aware enrolment strategy does improve matching accuracy compared with traditional single-image enrolment, and also compared with multi-image enrolment that does not take dilation into account.},
  doi      = {10.1049/iet-bmt.2015.0005},
  keywords = {eye;image matching;iris recognition;probability;best-eye image;dilation ratios;enrolment image;false nonmatch result probability;iris recognition accuracy improvement;matching accuracy improvement;median dilation;optimal single-eye image dilation-aware enrolment;probe image;pupil dilation},
}

@Misc{NEXUSjoint,
  author = {{NEXUS: Joint USA and Canada Trusted Traveler Program}},
  title  = {{US official site:} https://www.cbp.gov/travel/trusted-traveler-programs/nexus; {Canada} official site: http://www.nexus.gc.ca},
  year   = {(access: September 3, 2017)},
}

@Misc{IriShield,
  author       = {{IriTech Inc.}},
  howpublished = {on-line},
  title        = {{IriShield M2120U, http://www.iritech.com/products/hardware}},
  year         = {(access: September 10, 2017)},
}

@Article{Monro_TPAMI_2007,
  author   = {D. M. Monro and S. Rakshit and D. Zhang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {DCT-Based Iris Recognition},
  year     = {2007},
  issn     = {0162-8828},
  month    = {April},
  number   = {4},
  pages    = {586-595},
  volume   = {29},
  abstract = {This paper presents a novel iris coding method based on differences of discrete cosine transform (DCT) coefficients of overlapped angular patches from normalized iris images. The feature extraction capabilities of the DCT are optimized on the two largest publicly available iris image data sets, 2,156 images of 308 eyes from the CASIA database and 2,955 images of 150 eyes from the Bath database. On this data, we achieve 100 percent correct recognition rate (CRR) and perfect receiver-operating characteristic (ROC) curves with no registered false accepts or rejects. Individual feature bit and patch position parameters are optimized for matching through a product-of-sum approach to Hamming distance calculation. For verification, a variable threshold is applied to the distance metric and the false acceptance rate (FAR) and false rejection rate (FRR) are recorded. A new worst-case metric is proposed for predicting practical system performance in the absence of matching failures, and the worst case theoretical equal error rate (EER) is predicted to be as low as 2.59 times 10-1 available data sets},
  doi      = {10.1109/TPAMI.2007.1002},
  keywords = {discrete cosine transforms;error statistics;feature extraction;image coding;image matching;sensitivity analysis;Bath database;CASIA database;DCT-based iris recognition;Hamming distance calculation;correct recognition rate;discrete cosine transform coefficients;equal error rate;false acceptance rate;false rejection rate;feature extraction;iris coding method;normalized iris images;overlapped angular patches;patch position parameters;product-of-sum approach;receiver-operating characteristic curves;worst-case metric;Character recognition;Discrete cosine transforms;Eyes;Feature extraction;Hamming distance;Image coding;Image databases;Iris recognition;Spatial databases;System performance;Biometrics;discrete cosine transform;image preprocessing;iris recognition;statistical analysis.;Algorithms;Artificial Intelligence;Biometry;Fourier Analysis;Humans;Image Interpretation, Computer-Assisted;Iris;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted},
}

@Misc{IRICORE_SDK,
  author = {{IriTech Ltd.}},
  title  = {{IriCore SDK}, ver. 3.6, http://www.iritech.com/products/software/iricore-eye-recognition-software (access: September 10, 2017)},
  year   = {2015},
}

@InProceedings{Banerjee_IWBF_2012,
  author    = {S. Banerjee and A. Ross},
  booktitle = W_IWBF,
  title     = {From image to sensor: Comparative evaluation of multiple PRNU estimation schemes for identifying sensors from NIR iris images},
  year      = {2017},
  month     = {April},
  pages     = {1-6},
  abstract  = {The field of digital image forensics concerns itself with the task of validating the authenticity of an image or determining the device that produced the image. Device or sensor identification can be accomplished by estimating sensor-specific pixel artifacts, such as Photo Response Non Uniformity (PRNU), that leave an imprint in the resulting image. Research in this field has predominantly focused on images obtained using sensors operating in the visible spectrum. Iris recognition systems, on the other hand, utilize sensors operating in the near-infrared (NIR) spectrum. In this work, we evaluate the applicability of different PRNU estimation schemes in accurately deducing sensor information from NIR iris images. We also analyze the impact of a photometric transformation on the estimation process. Experiments involving 12 sensors and 9511 images convey that the Basic and Enhanced Sensor Pattern Noise (SPN) schemes outperform the Maximum Likelihood and Phase-based SPN methods. Experiments also convey the need to explore alternate methods for performing digital image forensics on NIR iris images.},
  doi       = {10.1109/IWBF.2017.7935081},
  keywords  = {digital forensics;infrared imaging;iris recognition;maximum likelihood estimation;NIR iris images;PRNU estimation schemes;SPN schemes;digital image forensics;maximum likelihood method;near infrared iris images;near-infrared spectrum;phase-based SPN methods;photo response nonuniformity;sensor pattern noise;sensor-specific pixel artifacts;Cameras;Forensics;Iris;Iris recognition;Maximum likelihood estimation;Training},
}

@InProceedings{Uhl_ICB_2012,
  author    = {A. Uhl and Y. Höller},
  booktitle = C_ICB,
  title     = {Iris-sensor authentication using camera PRNU fingerprints},
  year      = {2012},
  month     = {March},
  pages     = {230-237},
  abstract  = {Integrity and authenticity are important issues in biometrics security. Digital image forensics have shown that the integrity and authenticity of an image can be evidenced by a specific sensor fingerprint. If an image contains the sensor fingerprint it is assumed to be truly taken by that sensor. In such a way, an image can be authenticated by identifying it's sensor. However, this fingerprint may be forged. We calculated the photo response non uniformity (PRNU) as sensor fingerprint and estimated how well it differentiates iris scans of the CASIA V4 iris-sensors. The distinction rate varies largely between the sensors, with equal error rates between 0.21 and 23.26%. By inserting a fingerprint of one sensor into iris-scans of another sensor we successfully forged several iris scans. This forgery required estimating the best embedding-strength for the fingerprint, and we present a useful heuristic to do so. The triangle test is proposed as a countermeasure against this attack. However, it was shown not to be very effective. This could be explained by the fact that the quality of a sensor fingerprint determines if a forgery may be detectable or not by the triangle test. It is possible that the special content of the images limits the applicability of the PRNU, its forgery and the detection of the forgery in iris databases.},
  doi       = {10.1109/ICB.2012.6199813},
  issn      = {2376-4201},
  keywords  = {biometrics (access control);fingerprint identification;image coding;image sensors;iris recognition;security of data;visual databases;CASIA V4 iris-sensors;PRNU;biometrics security;camera PRNU fingerprints;digital image forensics;equal error rates;fingerprint sensor;iris databases;iris sensor authentication;photo response non uniformity;Correlation;Databases;Feature extraction;Forgery;Iris recognition;Silicon;Watermarking},
}

@Article{Czajka_PAN_2016,
  author  = {Czajka, A. and Kasprzak, W. and Wilkowski, A.},
  journal = {Bulletin of The Polish Academy of Sciences},
  title   = {Verification of iris image authenticity using fragile watermarking},
  year    = {2016},
  number  = {4},
  pages   = {807-819},
  volume  = {64},
  doi     = {10.1515/bpasts-2016-0090},
  url     = {http://journals.pan.pl/Content/105757/PDF/10.1515bpasts-2016-0090.pdf},
}


@InProceedings{Kauba_IJCB_2017,
  author    = {Christof Kauba and Luca Debiasi and Andreas Uhl},
  booktitle = C_IJCB,
  title     = {Identifying the Origin of Iris Images Based on Fusion of Local Image Descriptors and PRNU Based Techniques},
  year      = {2017},
  month     = {October},
  pages     = {294-301},
  abstract  = {Being aware of the origin (source sensor) of an iris im- ages offers several advantages. Identifying the specific sen- sor unit supports ensuring the integrity and authenticity of iris images and thus detecting insertion attacks at a biomet- ric system. Moreover, by knowing the sensor model selec- tive processing, such as image enhancements, becomes fea- sible. In order to determine the origin (i.e. dataset) of near- infrared (NIR) and visible spectrum iris/ocular images, we evaluate the performance of three different approaches, a photo response non-uniformity (PRNU) based and an image texture feature based one, and the fusion of both. Our first set of experiments includes 19 different datasets comprising different sensors and image resolutions. The second set in- cludes 6 different camera models with 5 instances each. We evaluate the applicability of the three approaches in these test scenarios from a forensic and non-forensic perspective.},
}

@Article{Daugman_TIFS_2017,
  author   = {J. Daugman and C. Downing},
  journal  = {IEEE Transactions on Information Forensics and Security},
  title    = {Effect of Severe Image Compression on Iris Recognition Performance},
  year     = {2008},
  issn     = {1556-6013},
  month    = {March},
  number   = {1},
  pages    = {52-61},
  volume   = {3},
  abstract = {We investigate three schemes for severe compression of iris images in order to assess what their impact would be on recognition performance of the algorithms deployed today for identifying people by this biometric feature. Currently, standard iris images are 600 times larger than the IrisCode templates computed from them for database storage and search; but it is administratively desired that iris data should be stored, transmitted, and embedded in media in the form of images rather than as templates computed with proprietary algorithms. To reconcile that goal with its implications for bandwidth and storage, we present schemes that combine region-of-interest isolation with JPEG and JPEG2000 compression at severe levels, and we test them using a publicly available database of iris images. We show that it is possible to compress iris images to as little as 2000 bytes with minimal impact on recognition performance. Only some 2% to 3% of the bits in the IrisCode templates are changed by such severe image compression, and we calculate the entropy per code bit introduced by each compression scheme. Error tradeoff curve metrics document very good recognition performance despite this reduction in data size by a net factor of 150, approaching a convergence of image data size and template size.},
  doi      = {10.1109/TIFS.2007.916009},
  keywords = {biometrics (access control);data compression;image coding;image recognition;visual databases;JPEG2000 compression;biometric feature;database storage;error tradeoff curve metrics;image compression;image data size;image template size;iris image database;iris recognition performance;region-of-interest isolation;Bandwidth;Biometrics;Embedded computing;Image coding;Image databases;Image recognition;Image storage;Iris recognition;Spatial databases;Transform coding;Biometrics;JPEG2000;image compression;image segmentation;iris recognition;receiver operating characteristic (ROC) curves;region of interest},
}

@InProceedings{Daugman_IMAIP_2000,
  author    = {J. Daugman},
  booktitle = {Institute of Mathematics and its Applications, Proc. 2nd IMA-IP},
  title     = {Wavelet Demodulation Codes, Statistical Independence, and Pattern Recognition},
  year      = {2000},
  pages     = {244--260},
  abstract  = {Samples from stochastic signals with sufficient complexity need reveal only a little unexpected agreement, in order to reject the hypothesis that they are independent. The mere failure of a test of statistical independence can thereby serve as a basis for recognizing patterns confidently, provided they possess enough degrees-of-freedom. This paper discusses exploitation of this statistical principle in combination with wavelet image coding to extract phase descriptions of patterns. Demodulation and coarse quantization of the phase information creates decision environments characterized by well separated binomial-class distributions, and this lends itself to rapid and reliable pattern recognition. 1 Introduction The central issue in pattern recognition is the relationship between within-class variability and between-class variability. These are determined by the forms of variation (degrees-of-freedom) spanned by the pattern classes. Ideally the withinclass variability should be small.},
}



@Article{Beck_IOVS_2008,
  author  = {H. C. Beck and I. Ezon and L. Flom and C. Pitchford and L. Park},
  journal = {Investigative Ophthalmology \& Visual Science},
  title   = {Iris Recognition Technology in Newborns},
  year    = {2008},
  month   = {May},
  number  = {13},
  pages   = {2265},
  volume  = {49},
}




@Article{Tiwari_IJACE_2012,
  author  = {Shrikant Tiwari and Aruni Singh and Sanjay Singh},
  journal = {International Journal of Advanced Computer Engineering \& Architecture},
  title   = {Cosmetic Contact Lenses and Iris Recognition Spoofing},
  year    = {2012},
  number  = {2},
  pages   = {201--209},
  volume  = {2},
}

  
@Misc{ArtForgery,
  author       = {Andrew Dickson},
  howpublished = {\url{https://www.theguardian.com/us-news/2018/aug/06/the-new-tool-in-the-art-of-spotting-forgeries-artificial-intelligence}},
  note         = {Accessed: July 16, 2020},
  title        = {{The new tool in the art of spotting forgeries: artificial intelligence}},
}

@Online{OptoSwim_URL,
  author       = {OptoSwim},
  lastaccessed = {July 28, 2020},
  title        = {{Underwater Swimmer + Drowning Detection Using Computer Vision}},
  url          = {https://www.optoswim.com},
  year         = {2020},
}

@Online{KenyaHRC_URL,
  author       = {Kenya Human Rights Commission},
  lastaccessed = {July 28, 2020},
  month        = {January},
  title        = {{Report of the Digital Identification Document (ID) \& Citizenship Consultative Meeting}},
  url          = {http://citizenshiprightsafrica.org/kenya-report-of-the-digital-identification-document-id-citizenship-consultative-meeting},
  year         = {2019},
}


@Online{IdeaCenter_URL,
  author       = {{Notre Dame IDEA Center}},
  lastaccessed = {July 28, 2020},
  title        = {{Resource for commercialization and entrepreneurial activities at the University of Notre Dame}},
  url          = {https://ideacenter.nd.edu},
  year         = {2020},
}

@Online{IEG_URL,
  author       = {{NIST}},
  lastaccessed = {August 3, 2020},
  title        = {{Iris Expert Group II}},
  url          = {https://www.nist.gov/programs-projects/iris-experts-group-ii-homepage},
  year         = {2020},
}

@Online{MaryGalvin,
  lastaccessed = {July 14, 2021},
  title        = {{Mary E. Galvin Science and Engineering Scholars Program}},
  url          = {https://galvinscholars.nd.edu/mentors/},
  year         = {2021},
}

@InProceedings{Chen_WACV_2019,
  author    = {Chen, Cunjian and Ross, Arun},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision Workshops (WACVW)},
  title     = {An Explainable Attention-Guided Iris Presentation Attack Detector},
  year      = {2021},
  pages     = {97-106},
  doi       = {10.1109/WACVW52041.2021.00015},
}

@Online{NIJ_URL,
  author       = {{NIJ}},
  lastaccessed = {August 3, 2020},
  title        = {{Software tool and methodology for enhancement of unidentified decedent systems with postmortem automatic iris recognition}},
  url          = {https://nij.ojp.gov/funding/awards/2018-du-bx-0215},
  year         = {2020},
}

@Online{WorldBankGroup_URL,
  author       = {{World Bank Group and Center for Global Development}},
  lastaccessed = {August 3, 2020},
  title        = {{Principles on Identification for Sustainable Development: Toward the Digital Age}},
  url          = {http://documents1.worldbank.org/curated/en/213581486378184357/pdf/Principles-on-identification-for-sustainable-development-toward-the-digital-age.pdf},
  year         = {2020},
}

@Online{AadhaarNewborn_URL,
  author       = {{Richa Taneja}},
  lastaccessed = {August 3, 2020},
  title        = {{Baby Girl Enrolled For Aadhar Within 2 Minutes Of Being Born}},
  url          = {https://www.ndtv.com/india-news/maharashtra-newborn-gets-enrolled-for-aadhaar-within-2-minutes-of-her-birth-1843639},
  year         = {2018},
}

@Misc{ISO_29794-6_2016,
  author  = {{ISO/IEC 29794-6:2015}},
  title   = {{Information technology -- Biometric sample quality -- Part 6: Iris image data}},
  year    = {2015},
  comment = {standards:PAD:ISO},
}

@Online{FingerprintNewborn_URL,
  author       = {{NEC Press Release}},
  lastaccessed = {August 3, 2020},
  month        = {June},
  title        = {{Gavi, NEC, and Simprints to deploy world's first scalable child fingerprint identification solution to boost immunization in developing countries}},
  url          = {https://www.nec.com/en/press/201906/global_20190606_01.html},
  year         = {2019},
}






@Comment{NEW 11.24.2022 -- to sort out}

@InProceedings{Szegedy_ICLR_2014,
  author    = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Intriguing properties of neural networks},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6199},
}

@InProceedings{Goodfellow_ICLR_2015,
  author    = {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Explaining and Harnessing Adversarial Examples},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
}


@Article{Kurakin_2016,
  author        = {Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
  journal       = {CoRR},
  title         = {Adversarial examples in the physical world},
  year          = {2016},
  volume        = {abs/1607.02533},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/KurakinGB16.bib},
  eprint        = {1607.02533},
  timestamp     = {Mon, 13 Aug 2018 16:48:46 +0200},
  url           = {http://arxiv.org/abs/1607.02533},
}

@Article{Eykholt_CVPR_2018,
  author  = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title   = {{Robust Physical-World Attacks on Deep Learning Visual Classification}},
  year    = {2018},
  issn    = {10636919},
  pages   = {1625--1634},
  doi     = {10.1109/CVPR.2018.00175},
  isbn    = {9781538664209},
}

@InProceedings{Dong_CVPR_2018,
  author    = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Boosting Adversarial Attacks with Momentum},
  year      = {2018},
  month     = {06},
  pages     = {9185-9193},
  doi       = {10.1109/CVPR.2018.00957},
}

@Article{Su_TEC_2019,
  author  = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal = {IEEE Transactions on Evolutionary Computation},
  title   = {One Pixel Attack for Fooling Deep Neural Networks},
  year    = {2019},
  number  = {5},
  pages   = {828-841},
  volume  = {23},
  doi     = {10.1109/TEVC.2019.2890858},
}

  
  
%%%%%%%%%%%%%%% FACE PAD %%%%%%%%%%%%%%%%%
@Article{Li_ICPR_2016,
  author   = {Li, Xiaobai and Komulainen, Jukka and Zhao, Guoying and Yuen, Pong Chi and Pietikainen, Matti},
  journal  = {Proceedings - International Conference on Pattern Recognition},
  title    = {{Generalized face anti-spoofing by detecting pulse from face videos}},
  year     = {2016},
  issn     = {10514651},
  pages    = {4244--4249},
  volume   = {0},
  doi      = {10.1109/ICPR.2016.7900300},
  isbn     = {9781509048472},
  keywords = {Anti-spoofing, Cross-database, Face liveness, Mask, Pulse},
}

@Article{Liu_ECCV_2016,
  author   = {Liu, Siqi and Yuen, Pong C. and Zhang, Shengping and Zhao, Guoying},
  journal  = {European Conference on Computer Vision (ECCV)},
  title    = {{3D Mask Face Anti-spoofing with Remote Photoplethysmography}},
  year     = {2016},
  issn     = {03796566},
  pages    = {85--100},
  doi      = {10.1007/978-3-319-46478-7},
  isbn     = {9783319464787},
  keywords = {3d mask attack, Face anti-spoofing, Remote photoplethysmography},
}

@Article{Nowara_FG_2017,
  author  = {Nowara, Ewa Magdalena and Sabharwal, Ashutosh and Veeraraghavan, Ashok},
  journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
  title   = {{PPGSecure: Biometric Presentation Attack Detection Using Photopletysmograms}},
  year    = {2017},
  pages   = {56--62},
  volume  = {77005},
  doi     = {10.1109/FG.2017.16},
  isbn    = {9781509040230},
}

@Article{Heusch_BTAS_2018,
  author  = {Heusch, Guillaume and Marcel, Sebastien},
  journal = {IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS)},
  title   = {{Pulse-based features for face presentation attack detection}},
  year    = {2018},
  doi     = {10.1109/BTAS.2018.8698579},
  isbn    = {9781538671795},
}

@Article{Liu_ECCV_2018,
  author   = {Liu, Si Qi and Lan, Xiangyuan and Yuen, Pong C.},
  journal  = {European Conference on Computer Vision (ECCV)},
  title    = {{Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection}},
  year     = {2018},
  issn     = {16113349},
  pages    = {577--594},
  volume   = {11220 LNCS},
  doi      = {10.1007/978-3-030-01270-0_34},
  isbn     = {9783030012694},
  keywords = {3D mask attack, Face presentation attack detection, Remote photoplethysmography},
}

@Article{Hernandez-Ortega_CVPRW_2018,
  author    = {Hernandez-Ortega, Javier and Fierrez, Julian and Morales, Aythami and Tome, Pedro},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {{Time analysis of pulse-based face anti-spoofing in visible and NIR}},
  year      = {2018},
  issn      = {21607516},
  pages     = {657--665},
  volume    = {2018-June},
  doi       = {10.1109/CVPRW.2018.00096},
  isbn      = {9781538661000},
  publisher = {IEEE},
}

@Article{Lin2019,
  author   = {Lin, Bofan and Li, Xiaobai and Yu, Zitong and Zhao, Guoying},
  title    = {{Face Liveness Detection by rPPG Features and Contextual Patch-Based CNN}},
  year     = {2019},
  pages    = {61--68},
  doi      = {10.1145/3345336.3345345},
  isbn     = {9781450363051},
  keywords = {authentication, biometrics, contextual patch-based cnn, face anti-spoofing, mask, rppg, security and privacy},
}



%%% REMOTE PHOTOPLETHYSMOGRAPHY %%%

@InProceedings{Garbey_CVPR_2004,
  author    = {Garbey, M. and Merla, A. and Pavlidis, I.},
  booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  title     = {Estimation of blood flow speed and vessel location from thermal video},
  year      = {2004},
  pages     = {I-I},
  volume    = {1},
  doi       = {10.1109/CVPR.2004.1315054},
}


@Article{Garbey_TBME_2007,
  author  = {Garbey, Marc and Sun, Nanfei and Merla, Arcangelo and Pavlidis, Ioannis},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Contact-Free Measurement of Cardiac Pulse Based on the Analysis of Thermal Imagery},
  year    = {2007},
  number  = {8},
  pages   = {1418-1426},
  volume  = {54},
  doi     = {10.1109/TBME.2007.891930},
}


@Article{Speth_CVIU_2021,
  author  = {Speth, Jeremy and Vance, Nathan and Czajka, Adam and Bowyer, Kevin and Flynn, Patrick},
  journal = {Computer Vision and Image Understanding (CVIU)},
  title   = {Unifying frame rate and temporal dilations for improved remote pulse detection},
  year    = {2021},
  pages   = {1056--1062},
  doi     = {10.1109/ROMAN.2014.6926392},
  isbn    = {9781479967636},
}

@InProceedings{Speth_IJCB_2021,
  author    = {Speth, Jeremy and Vance, Nathan and Czajka, Adam and Bowyer, Kevin and Wright, Diane and Flynn, Patrick},
  booktitle = {International Joint Conference on Biometrics (IJCB)},
  title     = {Deception Detection and Remote Physiological Monitoring: A Dataset and Baseline Experimental Results},
  year      = {2021},
  pages     = {4264-4271},
  doi       = {10.1109/CVPR.2014.543},
}


@Article{Tarvainen2002,
  author    = {Tarvainen, Mika P. and Ranta-aho, Perttu O. and Karjalainen, Pasi A.},
  journal   = {IEEE Transactions on Biomedical Engineering},
  title     = {{An advanced detrending method with application to HRV analysis}},
  year      = {2002},
  number    = {2},
  pages     = {172--175},
  volume    = {49},
  abstract  = {An advanced, simple to use, detrending method to be used before heart rate variability analysis (HRV) is presented. The method is based on smoothness priors approach and operates like a time-varying finite-impulse response high-pass filter. The effect of the detrending on time- and frequency-domain analysis of HRV is studied.},
  doi       = {10.1109/10.979357},
  keywords  = {Heart rate variability, Signal detrending, Smoothness priors, Spectral analysis},
  publisher = {IEEE},
}



@Article{Pavlidis2002,
  author    = {Ioannis Pavlidis and Eberhardt, {Norman L.} and Levine, {James A.}},
  journal   = {Nature},
  title     = {Seeing through the face of deception: Thermal imaging offers a promising hands-off approach to mass security screening},
  year      = {2002},
  issn      = {0028-0836},
  number    = {6867},
  pages     = {35},
  volume    = {415},
  doi       = {10.1038/415035a},
  language  = {English (US)},
  publisher = {Nature Publishing Group},
}


@Article{Wieringa2005,
  author   = {Wieringa, F. P. and Mastik, F. and {Van Der Steen}, A. F.W.},
  journal  = {Annals of Biomedical Engineering},
  title    = {{Contactless multiple wavelength photoplethysmographic imaging: A first step toward "{SpO2} camera" technology}},
  year     = {2005},
  issn     = {00906964},
  number   = {8},
  pages    = {1034--1041},
  volume   = {33},
  doi      = {10.1007/s10439-005-5763-2},
  keywords = {Contactless measurement of respiration and heart a, Multispectral imaging, Pulse oximetry},
}


@Article{Zheng2007,
  author  = {Zheng, Jia and Hu, Sijung},
  journal = {Journal of Physics: Conference Series},
  title   = {{The preliminary investigation of imaging photoplethysmographic system}},
  year    = {2007},
  issn    = {17426596},
  number  = {1},
  volume  = {85},
  doi     = {10.1088/1742-6596/85/1/012031},
}


@Article{Verkruysse2008,
  author    = {Wim Verkruysse and Lars O Svaasand and J Stuart Nelson},
  journal   = {Opt. Express},
  title     = {Remote plethysmographic imaging using ambient light.},
  year      = {2008},
  month     = {Dec},
  number    = {26},
  pages     = {21434--21445},
  volume    = {16},
  doi       = {10.1364/OE.16.021434},
  keywords  = {Medical and biological imaging; Passive remote sensing ; Infrared radiation; Laser imaging; Light sources; Remote sensing; Visible light; Visible sources},
  publisher = {OSA},
  url       = {http://www.opticsexpress.org/abstract.cfm?URI=oe-16-26-21434},
}


@Article{Poh2010,
  author    = {Ming-Zher Poh and Daniel J. McDuff and Rosalind W. Picard},
  journal   = {Opt. Express},
  title     = {Non-contact, automated cardiac pulse measurements using video imaging and blind source separation.},
  year      = {2010},
  month     = {May},
  number    = {10},
  pages     = {10762--10774},
  volume    = {18},
  doi       = {10.1364/OE.18.010762},
  keywords  = {Medical optics and biotechnology; Remote sensing and sensors ; Fast Fourier transforms; Image analysis; Light sources; Linear filtering; Magnetic resonance imaging; Power spectra},
  publisher = {OSA},
  url       = {http://www.opticsexpress.org/abstract.cfm?URI=oe-18-10-10762},
}


@Article{Poh2011,
  author  = {M. {Poh} and D. J. {McDuff} and R. W. {Picard}},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Advancements in Noncontact, Multiparameter Physiological Measurements Using a Webcam},
  year    = {2011},
  number  = {1},
  pages   = {7-11},
  volume  = {58},
}


@Article{scikit2011,
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  title   = {Scikit-learn: Machine Learning in {P}ython},
  year    = {2011},
  pages   = {2825--2830},
  volume  = {12},
}


@InProceedings{Owayjan2012,
  author    = {M. {Owayjan} and A. {Kashour} and N. {Al Haddad} and M. {Fadel} and G. {Al Souki}},
  booktitle = {2012 2nd International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)},
  title     = {The design and development of a lie detection system using facial micro-expressions},
  year      = {2012},
  pages     = {33-38},
}


@Article{Soleymani2012,
  author    = {Soleymani, Mohammad and Lichtenauer, Jeroen and Pun, Thierry and Pantic, Maja},
  journal   = {IEEE Transactions on Affective Computing},
  title     = {{A multimodal database for affect recognition and implicit tagging}},
  year      = {2012},
  issn      = {19493045},
  number    = {1},
  pages     = {42--55},
  volume    = {3},
  doi       = {10.1109/T-AFFC.2011.25},
  keywords  = {EEG, Emotion recognition, affective computing, eye gaze, facial expressions, implicit tagging, pattern classification, physiological signals},
  publisher = {IEEE},
}


@Article{DeHaan2013,
  author  = {G. {De Haan} and V. {Jeanne}},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Robust Pulse Rate From Chrominance-Based rPPG},
  year    = {2013},
  number  = {10},
  pages   = {2878-2886},
  volume  = {60},
}


@Article{DeHaan2014,
  author   = {{De Haan}, Gerard and {Van Leest}, Arno},
  journal  = {Physiological Measurement},
  title    = {{Improved motion robustness of remote-PPG by using the blood volume pulse signature}},
  year     = {2014},
  issn     = {1361-6579},
  number   = {9},
  pages    = {1913--1926},
  volume   = {35},
  doi      = {10.1088/0967-3334/35/9/1913},
  keywords = {biomedical monitoring, image analysis, photoplethysmography, remote sensing},
}


@InProceedings{Li2014,
  author    = {Li, Xiaobai and Chen, Jie and Zhao, Guoying and Pietikainen, Matti},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Remote Heart Rate Measurement from Face Videos under Realistic Situations},
  year      = {2014},
  month     = {06},
  pages     = {4264-4271},
  doi       = {10.1109/CVPR.2014.543},
}

@Article{Stricker2014,
  author  = {Stricker, Ronny and Muller, Steffen and Gross, Horst Michael},
  journal = {IEEE International Symposium on Robot and Human Interactive Communication},
  title   = {{Non-contact video-based pulse rate measurement on a mobile service robot}},
  year    = {2014},
  pages   = {1056--1062},
  doi     = {10.1109/ROMAN.2014.6926392},
  isbn    = {9781479967636},
}


@InProceedings{Perez-Rosas2014,
  author    = {P{\'e}rez-Rosas, Ver{\'o}nica and Mihalcea, Rada and Narvaez, Alexis and Burzo, Mihai},
  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
  title     = {A Multimodal Dataset for Deception Detection},
  year      = {2014},
  address   = {Reykjavik, Iceland},
  month     = may,
  pages     = {3118--3122},
  publisher = {European Language Resources Association (ELRA)},
  url       = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/869_Paper.pdf},
}


@InProceedings{Hsu2014,
  author    = {Hsu, Yungchien and Lin, Yen Liang and Hsu, Winston},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Learning-based heart rate detection from remote photoplethysmography features},
  year      = {2014},
  pages     = {4433-4437},
  publisher = {IEEE},
  doi       = {10.1109/ICASSP.2014.6854440},
  isbn      = {9781479928927},
  issn      = {15206149},
  keywords  = {heart rate, photoplethysmography (PPG), regression learning},
}


@InProceedings{Kazemi2014,
  author    = {V. {Kazemi} and J. {Sullivan}},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {One millisecond face alignment with an ensemble of regression trees},
  year      = {2014},
  pages     = {1867-1874},
}


@Article{Rajoub2014,
  author  = {B. A. {Rajoub} and R. {Zwiggelaar}},
  journal = {IEEE Transactions on Information Forensics and Security},
  title   = {Thermal Facial Analysis for Deception Detection},
  year    = {2014},
  number  = {6},
  pages   = {1015-1023},
  volume  = {9},
}


@InProceedings{Simonyan2014,
  author    = {Simonyan, Karen and Zisserman, Andrew},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
  title     = {Two-Stream Convolutional Networks for Action Recognition in Videos},
  year      = {2014},
  address   = {Cambridge, MA, USA},
  pages     = {568–576},
  publisher = {MIT Press},
  series    = {NIPS'14},
  location  = {Montreal, Canada},
  numpages  = {9},
}


@Article{Blackford2015,
  author  = {Blackford, Ethan B. and Estepp, Justin R.},
  journal = {Medical Imaging: Biomedical Applications in Molecular, Structural, and Functional Imaging},
  title   = {Effects of frame rate and image resolution on pulse rate measured using multiple camera imaging photoplethysmography},
  year    = {2015},
  issn    = {16057422},
  number  = {March 2015},
  pages   = {94172D},
  volume  = {9417},
  doi     = {10.1117/12.2083940},
  isbn    = {9781628415070},
}


@Article{Guazzi2015,
  author  = {Guazzi, Alessandro and Villarroel, Mauricio and Jorge, Joao and Daly, Jonathan and Frise, Matthew and Robbins, Peter and Tarassenko, L.},
  journal = {Biomedical optics express},
  title   = {Non-contact measurement of oxygen saturation with an {RGB} camera},
  year    = {2015},
  month   = {09},
  pages   = {3320-38},
  volume  = {6},
  doi     = {10.1364/BOE.6.003320},
}

@InProceedings{Perez-Rosas2015,
  author    = {P\'{e}rez-Rosas, Ver\'{o}nica and Abouelenien, Mohamed and Mihalcea, Rada and Burzo, Mihai},
  booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
  title     = {Deception Detection Using Real-Life Trial Data},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {59–66},
  publisher = {Association for Computing Machinery},
  series    = {ICMI ’15},
  doi       = {10.1145/2818346.2820758},
  isbn      = {9781450339124},
  keywords  = {verbal, real-life trial, multimodal, non-verbal, deception detection},
  location  = {Seattle, Washington, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2818346.2820758},
}


@InProceedings{Radlak2015,
  author    = {Radlak, Krystian and Bozek, Maciej and Smolka, Bogdan},
  booktitle = {Proceedings of the 2015 ACM on Workshop on Multimodal Deception Detection},
  title     = {Silesian Deception Database: Presentation and Analysis},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {29–35},
  publisher = {Association for Computing Machinery},
  series    = {WMDD ’15},
  doi       = {10.1145/2823465.2823469},
  isbn      = {9781450339872},
  keywords  = {cognitive load, silesian deception database, face analysis, deception detection},
  location  = {Seattle, Washington, USA},
  numpages  = {7},
  url       = {https://doi.org/10.1145/2823465.2823469},
}

@Article{Nasri2016,
  author    = {Nasri, Hanen and Ouarda, Wael and Alimi, Adel M.},
  journal   = {Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA},
  title     = {{ReLiDSS: Novel lie detection system from speech signal}},
  year      = {2016},
  issn      = {21615330},
  volume    = {0},
  doi       = {10.1109/AICCSA.2016.7945789},
  isbn      = {9781509043200},
  keywords  = {Lie detection, MFCC, Person recognition, Pitch, SVM, Speech signal, Stress},
  publisher = {IEEE},
}


@Article{Sun2016,
  author   = {Sun, Yu and Thakor, Nitish},
  journal  = {IEEE Transactions on Biomedical Engineering},
  title    = {{Photoplethysmography Revisited: From Contact to Noncontact, from Point to Imaging}},
  year     = {2016},
  issn     = {15582531},
  doi      = {10.1109/TBME.2015.2476337},
  keywords = {IPPG], camera, cellphone, imaging photoplethysmography [(PPG), motion artifact, noncontact},
}


@InProceedings{ZhangMMSE2016,
  author    = {Z. {Zhang} and J. M. {Girard} and Y. {Wu} and X. {Zhang} and P. {Liu} and U. {Ciftci} and S. {Canavan} and M. {Reale} and A. {Horowitz} and H. {Yang} and J. F. {Cohn} and Q. {Ji} and L. {Yin}},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis},
  year      = {2016},
  pages     = {3438-3446},
}


@Article{ZhangMTCNN2016,
  author   = {K. Zhang and Z. Zhang and Z. Li and Y. Qiao},
  journal  = {IEEE Signal Processing Letters},
  title    = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
  year     = {2016},
  issn     = {1070-9908},
  month    = {Oct},
  number   = {10},
  pages    = {1499-1503},
  volume   = {23},
  doi      = {10.1109/LSP.2016.2603342},
  keywords = {Benchmark testing;Computer architecture;Convolution;Detectors;Face;Face detection;Training;Cascaded convolutional neural network (CNN);face alignment;face detection},
}


@InProceedings{Tulyakov2016,
  author    = {S. {Tulyakov} and X. {Alameda-Pineda} and E. {Ricci} and L. {Yin} and J. F. {Cohn} and N. {Sebe}},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Self-Adaptive Matrix Completion for Heart Rate Estimation from Face Videos under Realistic Conditions},
  year      = {2016},
  pages     = {2396-2404},
}


@Article{Wang2016,
  author    = {Wang, Wenjin and Stuijk, Sander and {De Haan}, Gerard},
  journal   = {IEEE Transactions on Biomedical Engineering},
  title     = {{A Novel Algorithm for Remote Photoplethysmography: Spatial Subspace Rotation}},
  year      = {2016},
  issn      = {15582531},
  number    = {9},
  pages     = {1974--1984},
  volume    = {63},
  doi       = {10.1109/TBME.2015.2508602},
  keywords  = {Biomedical monitoring, colors, photoplethysmography, remote sensing},
  publisher = {IEEE},
}


@Article{Heusch2017,
  author        = {Guillaume Heusch and Andr{\'{e}} Anjos and S{\'{e}}bastien Marcel},
  journal       = {CoRR},
  title         = {A Reproducible Study on Remote Heart Rate Measurement},
  year          = {2017},
  volume        = {abs/1709.00962},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1709-00962.bib},
  timestamp     = {Mon, 13 Aug 2018 16:48:50 +0200},
  url           = {http://arxiv.org/abs/1709.00962},
}


@InProceedings{Hsu2017,
  author    = {G. {Hsu} and A. {Ambikapathi} and M. {Chen}},
  booktitle = {IEEE International Joint Conference on Biometrics (IJCB)},
  title     = {Deep learning with time-frequency representation for pulse estimation from facial videos},
  year      = {2017},
  pages     = {383-389},
}

@InProceedings{HernandezOrtega_AAAIW_2021,
  author    = {Javier Hernandez-Ortega and Ruben Tolosana and Julian Fierrez and Aythami Morales},
  booktitle = {{AAAI Workshop on Artificial Intelligence Safety (SafeAI)}},
  title     = {{DeepFakesON-Phys: DeepFakes Detection based on Heart Rate Estimation}},
  year      = {2021},
  pages     = {1-6},
}

@Article{Ciftci_IJCB_2020,
  author  = {U. A. Ciftci and Ilke Demir and L. Yin},
  journal = {2020 IEEE International Joint Conference on Biometrics (IJCB)},
  title   = {How Do the Hearts of Deep Fakes Beat? Deep Fake Source Detection via Interpreting Residuals with Biological Signals},
  year    = {2020},
  pages   = {1-10},
}

@Article{Qi_ACM_2020,
  author  = {Hua Qi and Q. Guo and Felix Juefei-Xu and Xiaofei Xie and L. Ma and Wei Feng and Y. Liu and Jianjun Zhao},
  journal = {Proceedings of the 28th ACM International Conference on Multimedia},
  title   = {DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms},
  year    = {2020},
}

@Article{McDuff2017,
  author    = {McDuff, Daniel J. and Blackford, Ethan B. and Estepp, Justin R.},
  journal   = {IEEE International Conference on Automatic Face and Gesture Recognition Workshops (FG)},
  title     = {{The Impact of Video Compression on Remote Cardiac Pulse Measurement Using Imaging Photoplethysmography}},
  year      = {2017},
  pages     = {63--70},
  doi       = {10.1109/FG.2017.17},
  isbn      = {9781509040230},
  publisher = {IEEE},
}


@InProceedings{Turnip2017,
  author    = {Turnip, Arjon and Amri, M Faizal and Fakrurroja, Hanif and Simbolon, Artha Ivonita and Suhendra, M. Agung and Kusumandari, Dwi Esti},
  booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
  title     = {Deception Detection of EEG-P300 Component Classified by SVM Method},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {299–303},
  publisher = {Association for Computing Machinery},
  series    = {ICSCA ’17},
  doi       = {10.1145/3056662.3056709},
  isbn      = {9781450348577},
  keywords  = {EEG-P300, extraction, SVM, BCI, deception, classification},
  location  = {Bangkok, Thailand},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3056662.3056709},
}

  


@Article{Wang2017,
  author  = {W. {Wang} and A. C. {den Brinker} and S. {Stuijk} and G. {de Haan}},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Algorithmic Principles of Remote {PPG}},
  year    = {2017},
  number  = {7},
  pages   = {1479-1491},
  volume  = {64},
}


@InProceedings{Baltrusaitis2018,
  author    = {T. {Baltrusaitis} and A. {Zadeh} and Y. C. {Lim} and L. {Morency}},
  booktitle = {IEEE International Conference on Automatic Face Gesture Recognition (FG)},
  title     = {OpenFace 2.0: Facial Behavior Analysis Toolkit},
  year      = {2018},
  pages     = {59-66},
}


@InProceedings{Chen2018,
  author    = {Chen, Weixuan and McDuff, Daniel},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title     = {{DeepPhys}: Video-Based Physiological Measurement Using Convolutional Attention Networks},
  year      = {2018},
  pages     = {356--373},
  isbn      = {978-3-030-01216-8},
}


@Article{Duran2018,
  author   = {Duran, Geoffrey and Tapiero, Isabelle and Michael, George A},
  journal  = {Physiology {\&} Behavior},
  title    = {{Resting heart rate: A physiological predicator of lie detection ability}},
  year     = {2018},
  issn     = {0031-9384},
  pages    = {10--15},
  volume   = {186},
  doi      = {https://doi.org/10.1016/j.physbeh.2018.01.002},
  keywords = {Arousal, Cue utilization theory, Resting heart rate, Detection of deception},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031938418300027},
}


@Article{Hara2018,
  author  = {Kensho Hara and Hirokatsu Kataoka and Yutaka Satoh},
  journal = {2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title   = {Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?},
  year    = {2018},
  pages   = {6546-6555},
}


@Article{Martinez2018,
  author  = {Mart{\'{i}}nez, Gloria and Howard, Newton and Abbott, Derek and Lim, Kenneth and Ward, Rabab and Elgendi, Mohamed},
  journal = {Journal of Clinical Medicine},
  title   = {Can Photoplethysmography Replace Arterial Blood Pressure in the Assessment of Blood Pressure?},
  year    = {2018},
  issn    = {2077-0383},
  number  = {10},
  pages   = {316},
  volume  = {7},
  doi     = {10.3390/jcm7100316},
  pmid    = {30274376},
}


@InProceedings{Niu2018,
  author    = {Xuesong Niu and Hu Han and Shiguang Shan and Xilin Chen},
  booktitle = {Asian Conference on Computer Vision (ACCV)},
  title     = {{VIPL-HR}: A Multi-modal Database for Pulse Estimation from Less-constrained Face Video},
  year      = {2018},
  pages     = {562-576},
}


@Article{Bobbia2019,
  author        = {Bobbia, Serge and Macwan, Richard and Benezeth, Yannick and Mansouri, Alamin and Dubois, Julien},
  journal       = {Pattern Recognition Letters},
  title         = {{Unsupervised skin tissue segmentation for remote photoplethysmography}},
  year          = {2019},
  issn          = {01678655},
  pages         = {82--90},
  volume        = {124},
  doi           = {10.1016/j.patrec.2017.10.017},
  keywords      = {Image processing, Living skin tissue segmentation, Remote photoplethysmography, UBFC-RPPG Database, Unsupervised},
  mendeley-tags = {UBFC-RPPG Database},
  publisher     = {Elsevier B.V.},
  url           = {https://doi.org/10.1016/j.patrec.2017.10.017},
}


@InProceedings{Gupta2019,
  author    = {V. {Gupta} and M. {Agarwal} and M. {Arora} and T. {Chakraborty} and R. {Singh} and M. {Vatsa}},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Bag-of-Lies: A Multimodal Dataset for Deception Detection},
  year      = {2019},
  pages     = {83-90},
}


@InProceedings{Soldner2019,
  author    = {Soldner, Felix and P{\'e}rez-Rosas, Ver{\'o}nica and Mihalcea, Rada},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  title     = {Box of Lies: Multimodal Deception Detection in Dialogues},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  month     = jun,
  pages     = {1768--1777},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N19-1175},
  url       = {https://www.aclweb.org/anthology/N19-1175},
}

@Article{Villarroel2019,
  author  = {M. Villarroel and Sitthichok Chaichulee and Jo{\~a}o Jorge and Sara Davis and Gabrielle Green and Carlos Arteta and Andrew Zisserman and K. McCormick and P. Watkinson and L. Tarassenko},
  journal = {NPJ Digital Medicine},
  title   = {Non-contact physiological monitoring of preterm infants in the Neonatal Intensive Care Unit},
  year    = {2019},
  pages   = {1-18},
  volume  = {2},
}


@InProceedings{Yu2019,
  author    = {Yu, Zitong and Li, Xiaobai and Zhao, Guoying},
  booktitle = {Proceedings of the British Machine Vision Conference (BMVC)},
  title     = {Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks},
  year      = {2019},
  pages     = {1-12},
}

@InProceedings{Yu_ICCV_2019,
  author    = {Yu*, Zitong and Peng*, Wei and Li, Xiaobai and Hong, Xiaopeng and Zhao, Guoying},
  booktitle = {International Conference on Computer Vision (ICCV)},
  title     = {Remote Heart Rate Measurement from Highly Compressed Facial Videos: an End-to-end Deep Learning Solution with Video Enhancement},
  year      = {2019},
}

@InProceedings{Lee_ECCV_2020,
  author    = {Lee, Eugene and Chen, Evan and Lee, Chen-Yi},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title     = {Meta-rPPG: Remote Heart Rate Estimation Using a Transductive Meta-Learner},
  year      = {2020},
}


@InProceedings{Mironenko2020,
  author    = {Mironenko, Yuriy and Kalinin, Konstantin and Kopeliovich, Mikhail and Petrushan, Mikhail},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Remote Photoplethysmography: Rarely Considered Factors},
  year      = {2020},
  month     = {June},
  pages     = {1197-1206},
  doi       = {10.1109/CVPRW50498.2020.00156},
}


@InProceedings{Nagasawa_2020,
  author    = {Nagasawa, Takumi and Takahashi, Ryo and Koopipat, Chawan and Tsumura, Norimichi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {Stress Estimation Using Multimodal Biosignal Information From RGB Facial Video},
  year      = {2020},
  month     = {June},
}

@InProceedings{Niu_ECCV_2020,
  author    = {Niu, Xuesong and Yu, Zitong and Han, Hu and Li, Xiaobai and Shan, Shiguang and Zhao, Guoying},
  booktitle = {European Conference on Computer Vision (ECCV)},
  title     = {Video-based Remote Physiological Measurement via Cross-verified Feature Disentangling.},
  year      = {2020},
}

@Article{Niu2020,
  author    = {Niu, Xuesong and Shan, Shiguang and Han, Hu and Chen, Xilin},
  journal   = {IEEE Transactions on Image Processing},
  title     = {{RhythmNet}: End-to-End Heart Rate Estimation from Face via Spatial-Temporal Representation},
  year      = {2020},
  issn      = {19410042},
  pages     = {2409--2423},
  volume    = {29},
  doi       = {10.1109/TIP.2019.2947204},
  keywords  = {Remote heart rate estimation, end-to-end learning, rPPG, spatial-temporal representation},
  publisher = {IEEE},
}


@Article{Wang2020,
  author    = {Wenjin Wang and Caifeng Shan},
  journal   = {Biomedical Physics {\&} Engineering Express},
  title     = {Impact of makeup on remote-{PPG} monitoring},
  year      = {2020},
  month     = mar,
  number    = {3},
  pages     = {035004},
  volume    = {6},
  doi       = {10.1088/2057-1976/ab51ba},
  publisher = {{IOP} Publishing},
}


@Article{Zhan2020,
  author        = {Zhan, Qi and Wang, Wenjin and {de Haan}, Gerard},
  journal       = {Biomedical Optics Express},
  title         = {{Analysis of CNN-based remote-PPG to understand limitations and sensitivities}},
  year          = {2020},
  issn          = {2156-7085},
  number        = {3},
  pages         = {1268--1283},
  volume        = {11},
  archiveprefix = {arXiv},
  arxivid       = {1911.02736},
  doi           = {10.1364/boe.382637},
  eprint        = {1911.02736},
}


@Article{Bai2018,
  author        = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
  journal       = {CoRR},
  title         = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
  year          = {2018},
  volume        = {abs/1803.01271},
  archiveprefix = {arXiv},
  url           = {http://arxiv.org/abs/1803.01271},
}


@Article{McDuff2021,
  author     = {Daniel McDuff},
  journal    = {CoRR},
  title      = {Camera Measurement of Physiological Vital Signs},
  year       = {2021},
  volume     = {abs/2111.11547},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2111-11547.bib},
  eprint     = {2111.11547},
  eprinttype = {arXiv},
  timestamp  = {Fri, 26 Nov 2021 13:48:43 +0100},
  url        = {https://arxiv.org/abs/2111.11547},
}


@InProceedings{Speth_2022_WACV,
  author    = {Speth, Jeremy and Vance, Nathan and Flynn, Patrick and Bowyer, Kevin W. and Czajka, Adam},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Digital and Physical-World Attacks on Remote Pulse Detection},
  year      = {2022},
  month     = {January},
  pages     = {2407-2416},
}


@InProceedings{Torralba2011,
  author    = {Torralba, Antonio and Efros, Alexei A.},
  booktitle = {CVPR 2011},
  title     = {Unbiased look at dataset bias},
  year      = {2011},
  pages     = {1521-1528},
  doi       = {10.1109/CVPR.2011.5995347},
}

@Article{Dubnov2004,
  author  = {Dubnov, S.},
  journal = {IEEE Signal Processing Letters},
  title   = {Generalization of spectral flatness measure for non-Gaussian linear processes},
  year    = {2004},
  number  = {8},
  pages   = {698-701},
  volume  = {11},
  doi     = {10.1109/LSP.2004.831663},
}

@Article{Dolhansky_2019_dfdc,
  author  = {Dolhansky, Brian and Howes, Russ and Pflaum, Ben and Baram, Nicole and Ferrer, Cristian Canton},
  journal = {arXiv preprint arXiv:1910.08854},
  title   = {The deepfake detection challenge (dfdc) preview dataset},
  year    = {2019},
}

@Misc{Dolhansky2020,
  author        = {Brian Dolhansky and Joanna Bitton and Ben Pflaum and Jikuo Lu and Russ Howes and Menglin Wang and Cristian Canton Ferrer},
  title         = {The DeepFake Detection Challenge Dataset},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.07397},
  primaryclass  = {cs.CV},
}

@InProceedings{Liu_2016_CVPRW,
  author    = {Liu, Siqi and Yang, Baoyao and Yuen, Pong C. and Zhao, Guoying},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {A 3D Mask Face Anti-Spoofing Database With Real World Variations},
  year      = {2016},
  month     = {June},
}

@InProceedings{Nowara_ICCVW_2019,
  author    = {Nowara, Ewa and McDuff, Daniel},
  booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
  title     = {Combating the {Impact} of {Video} {Compression} on {Non}-{Contact} {Vital} {Sign} {Measurement} {Using} {Supervised} {Learning}},
  year      = {2019},
  month     = oct,
  note      = {ISSN: 2473-9944},
  pages     = {1706--1712},
  doi       = {10.1109/ICCVW.2019.00211},
  keywords  = {deep learning, Image coding, imaging photoplethysmography, Signal to noise ratio, Skin, Streaming media, supervised learning, temporal video compression, Training, Video compression, Video recording, vital signs},
}

@InProceedings{Zhao_CVPRW_2018,
  author    = {Zhao, Changchen and Lin, Chun-Liang and Chen, Weihai and Li, Zhengguo},
  booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
  title     = {A {Novel} {Framework} for {Remote} {Photoplethysmography} {Pulse} {Extraction} on {Compressed} {Videos}},
  year      = {2018},
  month     = jun,
  note      = {ISSN: 2160-7516},
  pages     = {1380--138009},
  doi       = {10.1109/CVPRW.2018.00177},
  keywords  = {Bit rate, Color, Colored noise, Heart rate, Skin, Video compression, Videos},
}

@Article{Nowara_BOE_2021,
  author  = {Nowara, Ewa M. and McDuff, Daniel and Veeraraghavan, Ashok},
  journal = {Biomedical Optics Express},
  title   = {{Systematic analysis of video-based pulse measurement from compressed videos}},
  year    = {2021},
  issn    = {2156-7085},
  number  = {1},
  pages   = {494},
  volume  = {12},
  doi     = {10.1364/boe.408471},
}

@Article{Liu_WACV_2020,
  author  = {Liu, Si Qi and Lan, Xiangyuan and Yuen, Pong C.},
  journal = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title   = {{Temporal similarity analysis of remote photoplethysmography for fast 3D mask face presentation attack detection}},
  year    = {2020},
  pages   = {2597--2605},
  doi     = {10.1109/WACV45572.2020.9093337},
  isbn    = {9781728165530},
}

@Article{Geiger2013IJRR,
  author  = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
  journal = {International Journal of Robotics Research (IJRR)},
  title   = {Vision meets Robotics: The KITTI Dataset},
  year    = {2013},
}

@Article{Wu2022,
  author    = {Wu, Bing-fei and Wu, Bing-jhang and Cheng, Shao-en and Sun, Yu},
  title     = {{Motion-Robust Atrial Fibrillation Detection Based on Remote-Photoplethysmography}},
  year      = {2022},
  number    = {XX},
  pages     = {1--12},
  volume    = {XX},
  doi       = {10.1109/JBHI.2022.3172705},
  publisher = {IEEE Journal of Biomedical and Health Informatics},
}


@InProceedings{Kossack_2022_CVPR,
  author    = {Kossack, Benjamin and Wisotzky, Eric and Eisert, Peter and Schraven, Sebastian P. and Globke, Brigitta and Hilsmann, Anna},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {Perfusion Assessment via Local Remote Photoplethysmography (rPPG)},
  year      = {2022},
  month     = {June},
  pages     = {2192-2201},
}

@Article{hendrycks2019oe,
  author  = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  journal = {Proceedings of the International Conference on Learning Representations},
  title   = {Deep Anomaly Detection with Outlier Exposure},
  year    = {2019},
}

@InProceedings{Lee2018,
  author    = {Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
  booktitle = {International Conference on Learning Representations},
  title     = {Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples},
  year      = {2018},
  url       = {https://openreview.net/forum?id=ryiAv2xAZ},
}

@InProceedings{Zhao_2021,
  author    = {Zhao, Yu and Zou, Bochao and Yang, Fan and Lu, Lin and Belkacem, Abdelkader Nasreddine and Chen, Chao},
  booktitle = {2021 IEEE International Joint Conference on Biometrics (IJCB)},
  title     = {Video-Based Physiological Measurement Using 3D Central Difference Convolution Attention Network},
  year      = {2021},
  pages     = {1-6},
  doi       = {10.1109/IJCB52358.2021.9484405},
}

  
  
@InProceedings{Liu_MTTS_2020,
  author    = {Liu, Xin and Fromm, Josh and Patel, Shwetak and McDuff, Daniel},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {19400--19411},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  url       = {https://proceedings.neurips.cc/paper/2020/file/e1228be46de6a0234ac22ded31417bc7-Paper.pdf},
}

@InProceedings{Yu_2022_CVPR,
  author    = {Yu, Zitong and Shen, Yuming and Shi, Jingang and Zhao, Hengshuang and Torr, Philip H.S. and Zhao, Guoying},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {PhysFormer: Facial Video-Based Physiological Measurement With Temporal Difference Transformer},
  year      = {2022},
  month     = {June},
  pages     = {4186-4196},
}

@InProceedings{Lu2021,
  author    = {Lu, Hao and Han, Hu and Zhou, S Kevin},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{Dual-GAN : Joint BVP and Noise Modeling for Remote Physiological Measurement}},
  year      = {2021},
  pages     = {12404--12413},
}

@Article{Wang2019,
  author    = {Wang, Wenjin and {Den Brinker}, Albertus C. and {De Haan}, Gerard},
  journal   = {IEEE Transactions on Biomedical Engineering},
  title     = {{Single-Element Remote-PPG}},
  year      = {2019},
  issn      = {15582531},
  number    = {7},
  pages     = {2032--2043},
  volume    = {66},
  doi       = {10.1109/TBME.2018.2882396},
  publisher = {IEEE},
}

@Article{Liu_JBHI_2022,
  author  = {Liu, Xuenan and Yang, Xuezhi and Wang, Dingliang and Wong, Alexander and Ma, Likun and Li, Longwei},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  title   = {VidAF: A Motion-Robust Model for Atrial Fibrillation Screening From Facial Videos},
  year    = {2022},
  number  = {4},
  pages   = {1672-1683},
  volume  = {26},
  doi     = {10.1109/JBHI.2021.3124967},
}

@Article{Wang2017LivingSkin,
  author  = {Wang, Wenjin and Stuijk, Sander and de Haan, Gerard},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Living-Skin Classification via Remote-PPG},
  year    = {2017},
  number  = {12},
  pages   = {2781-2792},
  volume  = {64},
  doi     = {10.1109/TBME.2017.2676160},
}


@InProceedings{Iuchi_2022_CVPR,
  author    = {Iuchi, Kaito and Miyazaki, Ryogo and Cardoso, George C. and Ogawa-Ochiai, Keiko and Tsumura, Norimichi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  title     = {Remote Estimation of Continuous Blood Pressure by a Convolutional Neural Network Trained on Spatial Patterns of Facial Pulse Waves},
  year      = {2022},
  month     = {June},
  pages     = {2139-2145},
}

@Article{Wang2015,
  author  = {Wang, Wenjin and Stuijk, Sander and de Haan, Gerard},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {Unsupervised Subject Detection via Remote PPG},
  year    = {2015},
  number  = {11},
  pages   = {2629-2637},
  volume  = {62},
  doi     = {10.1109/TBME.2015.2438321},
}


@Article{Shaffer2017,
  author  = {Shaffer, Fred and Ginsberg, J. P.},
  journal = {Frontiers in Public Health},
  title   = {An Overview of Heart Rate Variability Metrics and Norms},
  year    = {2017},
  issn    = {2296-2565},
  volume  = {5},
  doi     = {10.3389/fpubh.2017.00258},
  url     = {https://www.frontiersin.org/articles/10.3389/fpubh.2017.00258},
}


@Article{ScholkmannAMPD2012,
  author  = {Scholkmann, Felix and Boss, Jens and Wolf, Martin},
  journal = {Algorithms},
  title   = {An Efficient Algorithm for Automatic Peak Detection in Noisy Periodic and Quasi-Periodic Signals},
  year    = {2012},
  issn    = {1999-4893},
  number  = {4},
  pages   = {588--603},
  volume  = {5},
  doi     = {10.3390/a5040588},
  url     = {https://www.mdpi.com/1999-4893/5/4/588},
}


@InProceedings{ColakPyAMPD2016,
  author    = {Colak, Alperen Mustafa and Shibata, Yuichiro and Kurokawa, Fujio},
  booktitle = {2016 IEEE International Conference on Renewable Energy Research and Applications (ICRERA)},
  title     = {FPGA implementation of the automatic multiscale based peak detection for real-time signal analysis on renewable energy systems},
  year      = {2016},
  pages     = {379-384},
  doi       = {10.1109/ICRERA.2016.7884365},
}

@InProceedings{Kohli_ICB_2013,
  author    = {Kohli, Naman and Yadav, Daksha and Vatsa, Mayank and Singh, Richa},
  booktitle = C_ICB,
  title     = {Revisiting iris recognition with color cosmetic contact lenses},
  year      = {2013},
  address   = {Madrid, Spain},
  month     = {June},
  pages     = {1-7},
  publisher = {IEEE},
  abstract  = {Over the years, iris recognition has gained importance in the biometrics applications and is being used in several large scale nationwide projects. Though iris patterns are unique, they may be affected by external factors such as illumination, camera-eye angle, and sensor interoperability. The presence of contact lens, particularly color cosmetic lens, may also pose a challenge to iris biometrics as it obfuscates the iris patterns and changes the inter and intra-class distributions. This paper presents an in-depth analysis of the effect of contact lens on iris recognition performance. We also present the IIIT-D Contact Lens Iris database with over 6500 images pertaining to 101 subjects. For each subject, images are captured without lens, transparent (prescription) lens, and color cosmetic lens (textured) using two different iris sensors. The results computed using VeriEye suggest that color cosmetic lens significantly increases the false rejection at a fixed false acceptance rate. Also, the experiments on four existing lens detection algorithms suggest that incorporating lens detection helps in maintaining the iris recognition performance. However further research is required to build sophisticated lens detection algorithm that can improve iris recognition.},
  comment   = {iris:PAD},
  doi       = {10.1109/ICB.2013.6613021},
  issn      = {2376-4201},
  keywords  = {contact lenses;image colour analysis;image sensors;iris recognition;object detection;open systems;IIIT-D Contact Lens Iris database;VeriEye;afixed false acceptance rate;biometrics applications;camera-eye angle;color cosmetic contact lenses;interclass distributions;intraclass distributions;iris patterns;iris recognition performance;sensor interoperability;Accuracy;Databases;Image color analysis;Iris recognition;Lenses;Probes;Support vector machines},
}

@Article{Larpkrajang_IJLM_2016,
  author   = {Sadudee Larpkrajang and Wisarn Worasuwannarak and Vichan Peonim and Jitta Udnoon and Smith Srisont},
  journal  = J_IJLM,
  title    = {The use of pilocarpine eye drops for estimating the time since death},
  year     = {2016},
  issn     = {1752-928X},
  pages    = {100 - 103},
  volume   = {39},
  abstract = {Objective
The objective of this study was to estimate the time since death using pilocarpine eye drops.
Methods
In this study, 100 postmortem cases with known time of death were included. In each case, the left pupil was measured in millimeter units using a vernier caliper, and pilocarpine eye drops were applied. The pupil was measured again 10 min later, and statistical analysis was used to analyze the correlation between the time since death and the change in the pupil.
Results
The longest duration since death that the pupils showed reaction to pilocarpine was 15 h. The correlation between the change in the pupil and the postmortem interval was found (Spearman's rho, r = −0.304, p = 0.002), and the change in the pupil may be used to predict the postmortem interval by the following regression equation: postmortem interval (PMI) = 8.310−3.702 (Diff) ± 0.735 (PMI was postmortem interval in hours and Diff was the difference in the size of the pupil after administering pilocarpine in millimeter units).
Conclusion
The present study showed that pilocarpine eye drops can be used to estimate the time since death.},
  doi      = {10.1016/j.jflm.2016.01.008},
  keywords = {Forensic medicine, Supravital reaction, Pilocarpine, Time since death, Postmortem interval},
  url      = {https://doi.org/10.1016/j.jflm.2016.01.008},
}

@InProceedings{Ahmad_BTAS_2019,
  author    = {Ahmad, Sohaib and Fuller, Benjamin},
  booktitle = C_BTAS,
  title     = {ThirdEye: Triplet Based Iris Recognition without Normalization},
  year      = {2019},
  pages     = {1-9},
  doi       = {10.1109/BTAS46853.2019.9185998},
  keywords  = {Iris recognition;Feature extraction;Training;Image segmentation;Machine learning;Pipelines;Neurons},
}

@Article{Ren_TPAMI_2023,
  author   = {Ren, Min and Wang, Yunlong and Zhu, Yuhao and Zhang, Kunbo and Sun, Zhenan},
  journal  = J_TPAMI,
  title    = {Multiscale Dynamic Graph Representation for Biometric Recognition With Occlusions},
  year     = {2023},
  number   = {12},
  pages    = {15120-15136},
  volume   = {45},
  doi      = {10.1109/TPAMI.2023.3298836},
  keywords = {Iris recognition;Face recognition;Pattern recognition;Feature extraction;Sun;Machine intelligence;Image edge detection;Biometrics;deep learning;face recognition;graph neural networks;iris recognition},
}

@InProceedings{Ren_AAAI_2020,
  author    = {Ren, Min and Wang, Yunlong and Sun, Zhenan and Tan, Tieniu},
  booktitle = {{Annual AAAI Conference on Artificial Intelligence}},
  title     = {Dynamic Graph Representation for Occlusion Handling in Biometrics.},
  year      = {2020},
  pages     = {11940--11947},
}

@InProceedings{Boyd_WACVW_2023,
  author    = {Boyd, Aidan and Moreira, Daniel and Kuehlkamp, Andrey and Bowyer, Kevin and Czajka, Adam},
  booktitle = C_WACVW,
  title     = {Human Saliency-Driven Patch-based Matching for Interpretable Post-mortem Iris Recognition},
  year      = {2023},
  pages     = {701-710},
  doi       = {10.1109/WACVW58289.2023.00077},
  keywords  = {Training;Visualization;Forensics;Biological system modeling;Source coding;Machine learning;Feature extraction},
}

@MastersThesis{Masek_MSc_2003,
  author   = {Libor Masek},
  school   = {The University of Western Australia},
  title    = {Recognition of Human Iris Patternsfor Biometric Identification},
  year     = {2003},
  abstract = {A biometric system provides automatic identification of an individual based on a
unique feature or characteristic possessed by the individual. Iris recognition is
regarded as the most reliable and accurate biometric identification system available.
Most commercial iris recognition systems use patented algorithms developed by
Daugman, and these algorithms are able to produce perfect recognition rates.
However, published results have usually been produced under favourable conditions,
and there have been no independent trials of the technology. The work presented in this thesis involved developing an ‘open-source’ iris
recognition system in order to verify both the uniqueness of the human iris and also
its performance as a biometric. For determining the recognition performance of the
system two databases of digitised greyscale eye images were used. The iris recognition system consists of an automatic segmentation system that is based
on the Hough transform, and is able to localise the circular iris and pupil region,
occluding eyelids and eyelashes, and reflections. The extracted iris region was then
normalised into a rectangular block with constant dimensions to account for imaging
inconsistencies. Finally, the phase data from 1D Log-Gabor filters was extracted and
quantised to four levels to encode the unique pattern of the iris into a bit-wise
biometric template. The Hamming distance was employed for classification of iris templates, and two
templates were found to match if a test of statistical independence was failed. The
system performed with perfect recognition on a set of 75 eye images; however, tests
on another set of 624 images resulted in false accept and false reject rates of 0.005%
and 0.238% respectively. Therefore, iris recognition is shown to be a reliable and
accurate biometric technology.},
  url      = {https://peterkovesi.com/studentprojects/libor/index.html},
}

@InProceedings{Boyd_IJCB_2020,
  author    = {Boyd, Aidan and Czajka, Adam and Bowyer, Kevin},
  booktitle = C_IJCB,
  title     = {Are Gabor Kernels Optimal for Iris Recognition?},
  year      = {2020},
  pages     = {1-9},
  doi       = {10.1109/IJCB48548.2020.9304939},
  keywords  = {Kernel;Iris recognition;Databases;Training;Sensors;Open source software;Tools},
}

@TechReport{NIST_TN_2058,
  author      = {Matey, JR and Quinn, GW and Grother, PJ},
  institution = {{National Institute of Standards and Technology}},
  title       = {{IREX Validation Dataset} 2019},
  year        = {2019},
  address     = {Gaithersburg, MD},
  number      = {2058},
  type        = {NIST Technical Note (TN)},
  doi         = {10.6028/NIST.TN.2058},
}

@Book{Hyvrinen_NIS_2009,
  author    = {Hyv\"{a}rinen, Aapo and Hurri, Jarmo and Hoyer, Patrik O.},
  publisher = {Springer London},
  title     = {Natural Image Statistics},
  year      = {2009},
  isbn      = {9781848824911},
  doi       = {10.1007/978-1-84882-491-1},
  issn      = {1381-6446},
  journal   = {Computational Imaging and Vision},
  url       = {http://dx.doi.org/10.1007/978-1-84882-491-1},
}

@InProceedings{Ojansivu_ISP_2008,
  author    = {Ojansivu, Ville and Heikkil{\"a}, Janne},
  booktitle = {Image and Signal Processing},
  title     = {Blur Insensitive Texture Classification Using Local Phase Quantization},
  year      = {2008},
  address   = {Berlin, Heidelberg},
  editor    = {Elmoataz, Abderrahim and Lezoray, Olivier and Nouboud, Fathallah and Mammass, Driss},
  pages     = {236--243},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {In this paper, we propose a new descriptor for texture classification that is robust to image blurring. The descriptor utilizes phase information computed locally in a window for every image position. The phases of the four low-frequency coefficients are decorrelated and uniformly quantized in an eight-dimensional space. A histogram of the resulting code words is created and used as a feature in texture classification. Ideally, the low-frequency phase components are shown to be invariant to centrally symmetric blur. Although this ideal invariance is not completely achieved due to the finite window size, the method is still highly insensitive to blur. Because only phase information is used, the method is also invariant to uniform illumination changes. According to our experiments, the classification accuracy of blurred texture images is much higher with the new method than with the well-known LBP or Gabor filter bank methods. Interestingly, it is also slightly better for textures that are not blurred.},
  isbn      = {978-3-540-69905-7},
}

@Article{Fierrez_PR_2007,
  author   = {Julian Fierrez and Javier Ortega-Garcia and Doroteo Torre Toledano and Joaquin Gonzalez-Rodriguez},
  journal  = {Pattern Recognition},
  title    = {{Biosec baseline corpus: A multimodal biometric database}},
  year     = {2007},
  issn     = {0031-3203},
  number   = {4},
  pages    = {1389 - 1392},
  volume   = {40},
  doi      = {https://doi.org/10.1016/j.patcog.2006.10.014},
  keywords = {Multimodal, Biometrics, Authentication, Verification, Database, Performance, Fingerprint, Iris, Face, Voice},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320306004304},
}

@Misc{bath_dataset,
  author = {Monro, DM and Rakshit, S and Zhang, D},
  title  = {{University of Bath, UK Iris Image Database}},
  year   = {2009},
}

@Article{Phillips_TPAMI_2010,
  author    = {Phillips, P.J. and Scruggs, W.T. and O’Toole, A.J. and Flynn, P.J. and Bowyer, K.W. and Schott, C.L. and Sharpe, M.},
  journal   = J_TPAMI,
  title     = {FRVT 2006 and ICE 2006 Large-Scale Experimental Results},
  year      = {2010},
  issn      = {2160-9292},
  month     = may,
  number    = {5},
  pages     = {831–846},
  volume    = {32},
  doi       = {10.1109/tpami.2009.59},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  url       = {http://dx.doi.org/10.1109/TPAMI.2009.59},
}

@Article{Proenca_TPAMI_2010,
  author  = {Proenca, H. and Filipe, S. and Santos, R. and Oliveira, J. and Alexandre,{L.A.}},
  journal = J_TPAMI,
  title   = {The {UBIRIS.v2}: A Database of Visible Wavelength Images Captured On-The-Move and At-A-Distance},
  year    = {2010},
  number  = {8},
  pages   = {1529-1535},
  volume  = {32},
  doi     = {10.1109/TPAMI.2009.66},
}

@Online{CASIA_Iris_v4_URL,
  abstract     = {CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn. CASIA-IrisV4 contains a total of 54,607 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.},
  author       = {{Chinese Academy of Sciences}},
  lastaccessed = {March 31, 2024},
  organization = {{Chinese Academy of Sciences}},
  title        = {{CASIA-Iris-Syn v4}},
  url          = {http://www.cbsr.ia.ac.cn/china/Iris%20Databases%20CH.asp},
  year         = {2010},
}

@Article{AlonsoFernandez_IET_2015,
  author      = {Fernando Alonso-Fernandez and Josef Bigun},
  journal     = {IET Biometrics},
  title       = {Near-infrared and visible-light periocular recognition with Gabor features using frequency-adaptive automatic eye detection},
  year        = {2015},
  issn        = {2047-4938},
  month       = {June},
  pages       = {74-89(15)},
  volume      = {4},
  abstract    = {Periocular recognition has gained attention recently due to demands of increased robustness of face or iris in less controlled scenarios. We present a new system for eye detection based on complex symmetry filters, which has the advantage of not needing training. Also, separability of the filters allows faster detection via one-dimensional convolutions. This system is used as input to a periocular algorithm based on retinotopic sampling grids and Gabor spectrum decomposition. The evaluation framework is composed of six databases acquired both with near-infrared and visible sensors. The experimental setup is complemented with four iris matchers, used for fusion experiments. The eye detection system presented shows very high accuracy with near-infrared data, and a reasonable good accuracy with one visible database. Regarding the periocular system, it exhibits great robustness to small errors in locating the eye centre, as well as to scale changes of the input image. The density of the sampling grid can also be reduced without sacrificing accuracy. Lastly, despite the poorer performance of the iris matchers with visible data, fusion with the periocular system can provide an improvement of more than 20%. The six databases used have been manually annotated, with the annotation made publicly available.},
  affiliation = {School of Information Science, Computer and Electrical Engineering, Halmstad University, Box 823, Halmstad SE 301-18, Sweden},
  copyright   = {The Institution of Engineering and Technology},
  issue       = {2},
  keywords    = {VW range;visible range;sampling grid density reduction;Gabor features;visible databases;one-dimensional convolutions;frequency-adaptive automatic eye detection;scale changes robustness;retinotopic sampling grids;complex symmetry fllters;fusion experiments;iris matchers;Gabor analysis;near-infrared sensors;NIR sensors;detection fllter separability;eye centre;visible data;input image;error robustness;NIR data;visible-light periocular recognition;facial region;near-infrared periocular recognition;},
  language    = {English},
  publisher   = {Institution of Engineering and Technology},
  url         = {https://digital-library.theiet.org/content/journals/10.1049/iet-bmt.2014.0038},
}

@InProceedings{Hofbauer_ICPR_2014,
  author    = {Hofbauer, Heinz and Alonso-Fernandez, Fernando and Wild, Peter and Bigun, Josef and Uhl, Andreas},
  booktitle = C_ICPR,
  title     = {A Ground Truth for Iris Segmentation},
  year      = {2014},
  month     = aug,
  publisher = {IEEE},
  doi       = {10.1109/icpr.2014.101},
  url       = {http://dx.doi.org/10.1109/ICPR.2014.101},
}

@Book{Hyvrinen_NIS_2009,
  author    = {Hyv\"{a}rinen, Aapo and Hurri, Jarmo and Hoyer, Patrik O.},
  publisher = {Springer London},
  title     = {Natural Image Statistics},
  year      = {2009},
  isbn      = {9781848824911},
  doi       = {10.1007/978-1-84882-491-1},
  issn      = {1381-6446},
  journal   = {Computational Imaging and Vision},
  url       = {http://dx.doi.org/10.1007/978-1-84882-491-1},
}

@Book{Mulawka2014,
  author    = {Mulawka, Marzena},
  publisher = {Routledge},
  title     = {Postmortem Fingerprinting and Unidentified Human Remains},
  year      = {2014},
}

@TechReport{Hofbauer_Techreport_2014,
  author      = {Heinz Hofbauer and Fernando Alonso-Fernandez and Peter Wild and Josef Bigun and Andreas Uhl},
  institution = {University of Salzburg},
  title       = {{Evaluation of the IRISSEG datasets}},
  year        = {2014},
  month       = feb,
  url         = {https://www.wavelab.at/sources/Hofbauer14b/},
}

@InProceedings{Tinsley_IJCB_2023,
  author    = {Tinsley, Patrick and Purnapatra, Sandip and Mitcheff, Mahsa and Boyd, Aidan and Crum, Colton and Bowyer, Kevin and Flynn, Patrick and Schuckers, Stephanie and Czajka, Adam and Fang, Meiling and Damer, Naser and Liu, Xingyu and Wang, Caiyong and Sun, Xianyun and Chang, Zhaohua and Li, Xinyue and Zhao, Guangzhe and Tapia, Juan and Busch, Christoph and Aravena, Carlos and Schulz, Daniel},
  booktitle = {2023 IEEE International Joint Conference on Biometrics (IJCB)},
  title     = {{Iris Liveness Detection Competition (LivDet-Iris) – The 2023 Edition}},
  year      = {2023},
  pages     = {1-10},
  doi       = {10.1109/IJCB57857.2023.10448637},
  keywords  = {Training;Instruments;Benchmark testing;Classification algorithms;Civil engineering;Iris recognition;Lenses},
}

@Article{Boyd_TIFS_2023,
  author   = {Boyd, Aidan and Speth, Jeremy and Parzianello, Lucas and Bowyer, Kevin W. and Czajka, Adam},
  journal  = J_TIFS,
  title    = {{Comprehensive Study in Open-Set Iris Presentation Attack Detection}},
  year     = {2023},
  pages    = {3238-3250},
  volume   = {18},
  doi      = {10.1109/TIFS.2023.3274477},
  keywords = {Iris recognition;Lenses;Iris;Training;Benchmark testing;Protocols;Feature extraction;Presentation attack detection;iris recognition;open-set recognition},
}

@InProceedings{SohlDickstein_ICML_2015,
  author    = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = C_ICML,
  title     = {Deep unsupervised learning using nonequilibrium thermodynamics},
  year      = {2015},
  pages     = {2256–2265},
  publisher = {JMLR.org},
  series    = {ICML'15},
  volume    = {37},
  abstract  = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  location  = {Lille, France},
  numpages  = {10},
}

@InProceedings{Rombach_CVPR_2022,
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle = C_CVPR,
  title     = {High-Resolution Image Synthesis with Latent Diffusion Models},
  year      = {2022},
  pages     = {10674-10685},
  doi       = {10.1109/CVPR52688.2022.01042},
  keywords  = {Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control;Image and video synthesis and generation},
}

@Misc{IREX_X_URL,
  howpublished = {\url{https://pages.nist.gov/IREX10/}},
  note         = {Accessed: April 6, 2024},
  title        = {{IREX 10: Identification Track}},
}

@Comment{jabref-meta: databaseType:bibtex;}
